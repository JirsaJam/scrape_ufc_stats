{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.infer\n",
    "import pyro.optim\n",
    "from pyro.nn import PyroSample, PyroModule\n",
    "from pyro.infer.autoguide import AutoDiagonalNormal, AutoLowRankMultivariateNormal, AutoMultivariateNormal\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import os\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>EVENT</th>\n",
       "      <th>BOUT</th>\n",
       "      <th>OUTCOME</th>\n",
       "      <th>WEIGHTCLASS</th>\n",
       "      <th>METHOD</th>\n",
       "      <th>ROUND</th>\n",
       "      <th>TIME</th>\n",
       "      <th>TIME FORMAT</th>\n",
       "      <th>REFEREE</th>\n",
       "      <th>...</th>\n",
       "      <th>delta_ELO_FIGHTER</th>\n",
       "      <th>delta_AGE</th>\n",
       "      <th>delta_REACH</th>\n",
       "      <th>delta_HEIGHT</th>\n",
       "      <th>delta_SIG_STR_vs_avoid</th>\n",
       "      <th>delta_TD_vs_avoid</th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>opponent_PC1</th>\n",
       "      <th>opponent_PC2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>UFC25:UltimateJapan3</td>\n",
       "      <td>TitoOrtizvs.WanderleiSilva</td>\n",
       "      <td>W/L</td>\n",
       "      <td>UFC Light Heavyweight Title Bout</td>\n",
       "      <td>Decision - Unanimous</td>\n",
       "      <td>5</td>\n",
       "      <td>5:00</td>\n",
       "      <td>5 Rnd (5-5-5-5-5)</td>\n",
       "      <td>John McCarthy</td>\n",
       "      <td>...</td>\n",
       "      <td>-45.805984</td>\n",
       "      <td>-1.442847</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>5.852806</td>\n",
       "      <td>-0.025210</td>\n",
       "      <td>-2309.770770</td>\n",
       "      <td>111.777029</td>\n",
       "      <td>123.244200</td>\n",
       "      <td>65.610081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>UFC25:UltimateJapan3</td>\n",
       "      <td>TitoOrtizvs.WanderleiSilva</td>\n",
       "      <td>W/L</td>\n",
       "      <td>UFC Light Heavyweight Title Bout</td>\n",
       "      <td>Decision - Unanimous</td>\n",
       "      <td>5</td>\n",
       "      <td>5:00</td>\n",
       "      <td>5 Rnd (5-5-5-5-5)</td>\n",
       "      <td>John McCarthy</td>\n",
       "      <td>...</td>\n",
       "      <td>45.805984</td>\n",
       "      <td>1.442847</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.812415</td>\n",
       "      <td>-0.099214</td>\n",
       "      <td>123.244200</td>\n",
       "      <td>65.610081</td>\n",
       "      <td>-2309.770770</td>\n",
       "      <td>111.777029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>UFC27:UltimateBadBoyz</td>\n",
       "      <td>JeremyHornvs.EugeneJackson</td>\n",
       "      <td>W/L</td>\n",
       "      <td>Middleweight Bout</td>\n",
       "      <td>Submission</td>\n",
       "      <td>1</td>\n",
       "      <td>4:32</td>\n",
       "      <td>3 Rnd (5-5-5)</td>\n",
       "      <td>Mario Yamasaki</td>\n",
       "      <td>...</td>\n",
       "      <td>22.284177</td>\n",
       "      <td>-8.919918</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.581752</td>\n",
       "      <td>0.029975</td>\n",
       "      <td>-846.219711</td>\n",
       "      <td>-360.531587</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>UFC28:HighStakes</td>\n",
       "      <td>RandyCouturevs.KevinRandleman</td>\n",
       "      <td>W/L</td>\n",
       "      <td>UFC Heavyweight Title Bout</td>\n",
       "      <td>KO/TKO</td>\n",
       "      <td>3</td>\n",
       "      <td>4:13</td>\n",
       "      <td>5 Rnd (5-5-5-5-5)</td>\n",
       "      <td>John McCarthy</td>\n",
       "      <td>...</td>\n",
       "      <td>44.264592</td>\n",
       "      <td>8.134155</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.683676</td>\n",
       "      <td>0.236066</td>\n",
       "      <td>-1103.921141</td>\n",
       "      <td>-426.244419</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>UFC28:HighStakes</td>\n",
       "      <td>JensPulvervs.JohnLewis</td>\n",
       "      <td>W/L</td>\n",
       "      <td>Lightweight Bout</td>\n",
       "      <td>KO/TKO</td>\n",
       "      <td>1</td>\n",
       "      <td>0:15</td>\n",
       "      <td>3 Rnd (5-5-5)</td>\n",
       "      <td>Mario Yamasaki</td>\n",
       "      <td>...</td>\n",
       "      <td>27.873931</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>0.894140</td>\n",
       "      <td>-0.006089</td>\n",
       "      <td>-310.385372</td>\n",
       "      <td>125.252047</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10398</th>\n",
       "      <td>11693</td>\n",
       "      <td>UFCFightNight:Sandhagenvs.Nurmagomedov</td>\n",
       "      <td>ViktoriiaDudakovavs.SamHughes</td>\n",
       "      <td>L/W</td>\n",
       "      <td>Women's Strawweight Bout</td>\n",
       "      <td>Decision - Split</td>\n",
       "      <td>3</td>\n",
       "      <td>5:00</td>\n",
       "      <td>3 Rnd (5-5-5)</td>\n",
       "      <td>Daniel Movahedi</td>\n",
       "      <td>...</td>\n",
       "      <td>-53.379668</td>\n",
       "      <td>6.579055</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.026368</td>\n",
       "      <td>0.064299</td>\n",
       "      <td>4525.143067</td>\n",
       "      <td>-1018.395761</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10399</th>\n",
       "      <td>11694</td>\n",
       "      <td>UFCFightNight:Sandhagenvs.Nurmagomedov</td>\n",
       "      <td>SharaMagomedovvs.MichalOleksiejczuk</td>\n",
       "      <td>W/L</td>\n",
       "      <td>Middleweight Bout</td>\n",
       "      <td>Decision - Unanimous</td>\n",
       "      <td>3</td>\n",
       "      <td>5:00</td>\n",
       "      <td>3 Rnd (5-5-5)</td>\n",
       "      <td>Daniel Movahedi</td>\n",
       "      <td>...</td>\n",
       "      <td>-24.699481</td>\n",
       "      <td>-0.772074</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>3.331041</td>\n",
       "      <td>-0.523622</td>\n",
       "      <td>2107.863170</td>\n",
       "      <td>-1553.580021</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10400</th>\n",
       "      <td>11695</td>\n",
       "      <td>UFCFightNight:Sandhagenvs.Nurmagomedov</td>\n",
       "      <td>TonyFergusonvs.MichaelChiesa</td>\n",
       "      <td>L/W</td>\n",
       "      <td>Welterweight Bout</td>\n",
       "      <td>Submission</td>\n",
       "      <td>1</td>\n",
       "      <td>3:44</td>\n",
       "      <td>3 Rnd (5-5-5)</td>\n",
       "      <td>Marc Goddard</td>\n",
       "      <td>...</td>\n",
       "      <td>44.625080</td>\n",
       "      <td>-3.816564</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-3.546148</td>\n",
       "      <td>0.076678</td>\n",
       "      <td>8868.919814</td>\n",
       "      <td>-158.645274</td>\n",
       "      <td>13129.879716</td>\n",
       "      <td>-6580.190969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10401</th>\n",
       "      <td>11696</td>\n",
       "      <td>UFCFightNight:Sandhagenvs.Nurmagomedov</td>\n",
       "      <td>MarlonVeravs.DeivesonFigueiredo</td>\n",
       "      <td>L/W</td>\n",
       "      <td>Bantamweight Bout</td>\n",
       "      <td>Decision - Unanimous</td>\n",
       "      <td>3</td>\n",
       "      <td>5:00</td>\n",
       "      <td>3 Rnd (5-5-5)</td>\n",
       "      <td>Keith Peterson</td>\n",
       "      <td>...</td>\n",
       "      <td>-57.771347</td>\n",
       "      <td>-4.958248</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.794614</td>\n",
       "      <td>-0.137185</td>\n",
       "      <td>15885.407700</td>\n",
       "      <td>-6412.125198</td>\n",
       "      <td>9011.455859</td>\n",
       "      <td>-2906.188676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10402</th>\n",
       "      <td>11697</td>\n",
       "      <td>UFCFightNight:Sandhagenvs.Nurmagomedov</td>\n",
       "      <td>MackenzieDernvs.LoopyGodinez</td>\n",
       "      <td>W/L</td>\n",
       "      <td>Women's Strawweight Bout</td>\n",
       "      <td>Decision - Unanimous</td>\n",
       "      <td>3</td>\n",
       "      <td>5:00</td>\n",
       "      <td>3 Rnd (5-5-5)</td>\n",
       "      <td>Greg Kleynjans</td>\n",
       "      <td>...</td>\n",
       "      <td>26.507549</td>\n",
       "      <td>-0.454483</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-0.151119</td>\n",
       "      <td>0.191246</td>\n",
       "      <td>7081.267198</td>\n",
       "      <td>-1319.645664</td>\n",
       "      <td>8613.808940</td>\n",
       "      <td>-1492.417279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10403 rows × 177 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                   EVENT  \\\n",
       "0               0                    UFC25:UltimateJapan3   \n",
       "1               1                    UFC25:UltimateJapan3   \n",
       "2               4                   UFC27:UltimateBadBoyz   \n",
       "3               5                        UFC28:HighStakes   \n",
       "4               6                        UFC28:HighStakes   \n",
       "...           ...                                     ...   \n",
       "10398       11693  UFCFightNight:Sandhagenvs.Nurmagomedov   \n",
       "10399       11694  UFCFightNight:Sandhagenvs.Nurmagomedov   \n",
       "10400       11695  UFCFightNight:Sandhagenvs.Nurmagomedov   \n",
       "10401       11696  UFCFightNight:Sandhagenvs.Nurmagomedov   \n",
       "10402       11697  UFCFightNight:Sandhagenvs.Nurmagomedov   \n",
       "\n",
       "                                      BOUT OUTCOME  \\\n",
       "0               TitoOrtizvs.WanderleiSilva     W/L   \n",
       "1               TitoOrtizvs.WanderleiSilva     W/L   \n",
       "2               JeremyHornvs.EugeneJackson     W/L   \n",
       "3            RandyCouturevs.KevinRandleman     W/L   \n",
       "4                   JensPulvervs.JohnLewis     W/L   \n",
       "...                                    ...     ...   \n",
       "10398        ViktoriiaDudakovavs.SamHughes     L/W   \n",
       "10399  SharaMagomedovvs.MichalOleksiejczuk     W/L   \n",
       "10400         TonyFergusonvs.MichaelChiesa     L/W   \n",
       "10401      MarlonVeravs.DeivesonFigueiredo     L/W   \n",
       "10402         MackenzieDernvs.LoopyGodinez     W/L   \n",
       "\n",
       "                            WEIGHTCLASS                 METHOD  ROUND  TIME  \\\n",
       "0      UFC Light Heavyweight Title Bout  Decision - Unanimous       5  5:00   \n",
       "1      UFC Light Heavyweight Title Bout  Decision - Unanimous       5  5:00   \n",
       "2                     Middleweight Bout            Submission       1  4:32   \n",
       "3            UFC Heavyweight Title Bout                KO/TKO       3  4:13   \n",
       "4                      Lightweight Bout                KO/TKO       1  0:15   \n",
       "...                                 ...                    ...    ...   ...   \n",
       "10398          Women's Strawweight Bout      Decision - Split       3  5:00   \n",
       "10399                 Middleweight Bout  Decision - Unanimous       3  5:00   \n",
       "10400                 Welterweight Bout            Submission       1  3:44   \n",
       "10401                 Bantamweight Bout  Decision - Unanimous       3  5:00   \n",
       "10402          Women's Strawweight Bout  Decision - Unanimous       3  5:00   \n",
       "\n",
       "             TIME FORMAT          REFEREE  ... delta_ELO_FIGHTER delta_AGE  \\\n",
       "0      5 Rnd (5-5-5-5-5)    John McCarthy  ...        -45.805984 -1.442847   \n",
       "1      5 Rnd (5-5-5-5-5)    John McCarthy  ...         45.805984  1.442847   \n",
       "2          3 Rnd (5-5-5)   Mario Yamasaki  ...         22.284177 -8.919918   \n",
       "3      5 Rnd (5-5-5-5-5)    John McCarthy  ...         44.264592  8.134155   \n",
       "4          3 Rnd (5-5-5)   Mario Yamasaki  ...         27.873931       NaN   \n",
       "...                  ...              ...  ...               ...       ...   \n",
       "10398      3 Rnd (5-5-5)  Daniel Movahedi  ...        -53.379668  6.579055   \n",
       "10399      3 Rnd (5-5-5)  Daniel Movahedi  ...        -24.699481 -0.772074   \n",
       "10400      3 Rnd (5-5-5)     Marc Goddard  ...         44.625080 -3.816564   \n",
       "10401      3 Rnd (5-5-5)   Keith Peterson  ...        -57.771347 -4.958248   \n",
       "10402      3 Rnd (5-5-5)   Greg Kleynjans  ...         26.507549 -0.454483   \n",
       "\n",
       "      delta_REACH delta_HEIGHT  delta_SIG_STR_vs_avoid delta_TD_vs_avoid  \\\n",
       "0             0.0         -4.0                5.852806         -0.025210   \n",
       "1             0.0          4.0                0.812415         -0.099214   \n",
       "2             NaN          5.0               -0.581752          0.029975   \n",
       "3             NaN          3.0                1.683676          0.236066   \n",
       "4             NaN         -5.0                0.894140         -0.006089   \n",
       "...           ...          ...                     ...               ...   \n",
       "10398         NaN          NaN               -3.026368          0.064299   \n",
       "10399         NaN         -2.0                3.331041         -0.523622   \n",
       "10400        -1.0          2.0               -3.546148          0.076678   \n",
       "10401         2.0          3.0                0.794614         -0.137185   \n",
       "10402        -2.0         -2.0               -0.151119          0.191246   \n",
       "\n",
       "                PC1          PC2  opponent_PC1  opponent_PC2  \n",
       "0      -2309.770770   111.777029    123.244200     65.610081  \n",
       "1        123.244200    65.610081  -2309.770770    111.777029  \n",
       "2       -846.219711  -360.531587           NaN           NaN  \n",
       "3      -1103.921141  -426.244419           NaN           NaN  \n",
       "4       -310.385372   125.252047           NaN           NaN  \n",
       "...             ...          ...           ...           ...  \n",
       "10398   4525.143067 -1018.395761           NaN           NaN  \n",
       "10399   2107.863170 -1553.580021           NaN           NaN  \n",
       "10400   8868.919814  -158.645274  13129.879716  -6580.190969  \n",
       "10401  15885.407700 -6412.125198   9011.455859  -2906.188676  \n",
       "10402   7081.267198 -1319.645664   8613.808940  -1492.417279  \n",
       "\n",
       "[10403 rows x 177 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'/Users/jamesjirsa/Desktop/Data_Science/JJ_Projects/UFC/new_method/scrape_ufc_stats/final_fight_data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10403 entries, 0 to 10402\n",
      "Columns: 177 entries, Unnamed: 0 to opponent_PC2\n",
      "dtypes: float64(159), int64(4), object(14)\n",
      "memory usage: 14.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIGHTER</th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AJDobson</td>\n",
       "      <td>203.796394</td>\n",
       "      <td>-684.489701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AJFletcher</td>\n",
       "      <td>-186.817878</td>\n",
       "      <td>-240.184326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AaronPhillips</td>\n",
       "      <td>-776.704717</td>\n",
       "      <td>-475.177970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AaronRiley</td>\n",
       "      <td>2578.040225</td>\n",
       "      <td>-922.712749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AaronRosa</td>\n",
       "      <td>-912.749506</td>\n",
       "      <td>-419.983606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1539</th>\n",
       "      <td>ZhalgasZhumagulov</td>\n",
       "      <td>572.936588</td>\n",
       "      <td>-812.903884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1540</th>\n",
       "      <td>ZhangLipeng</td>\n",
       "      <td>360.553828</td>\n",
       "      <td>-117.202185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541</th>\n",
       "      <td>ZhangTiequan</td>\n",
       "      <td>-2470.537479</td>\n",
       "      <td>181.515467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1542</th>\n",
       "      <td>ZhangWeili</td>\n",
       "      <td>5546.337955</td>\n",
       "      <td>-1268.547434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543</th>\n",
       "      <td>ZubairaTukhugov</td>\n",
       "      <td>2259.663774</td>\n",
       "      <td>-1662.710437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1544 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                FIGHTER          PC1          PC2\n",
       "0              AJDobson   203.796394  -684.489701\n",
       "1            AJFletcher  -186.817878  -240.184326\n",
       "2         AaronPhillips  -776.704717  -475.177970\n",
       "3            AaronRiley  2578.040225  -922.712749\n",
       "4             AaronRosa  -912.749506  -419.983606\n",
       "...                 ...          ...          ...\n",
       "1539  ZhalgasZhumagulov   572.936588  -812.903884\n",
       "1540        ZhangLipeng   360.553828  -117.202185\n",
       "1541       ZhangTiequan -2470.537479   181.515467\n",
       "1542         ZhangWeili  5546.337955 -1268.547434\n",
       "1543    ZubairaTukhugov  2259.663774 -1662.710437\n",
       "\n",
       "[1544 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_fight_idx = df.groupby('FIGHTER')['Fight_Number'].idxmax()\n",
    "max_fight_rows = df.loc[max_fight_idx]\n",
    "extra = max_fight_rows[['FIGHTER', 'PC1', 'PC2']]\n",
    "extra = extra.reset_index(drop=True)\n",
    "extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['PC1', 'PC2'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['RESULT'] = np.where(df['RESULT'] == 'Win', 1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.columns[df.columns.get_loc('ELO_FIGHTER'):].tolist()\n",
    "#features.remove('CUM_index')\n",
    "features.remove('DOB')\n",
    "df = df.dropna(subset = 'opponent_PC1')\n",
    "#df = df.drop(['CUM_index'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_features = [f for f in features if f.startswith('CUM_') and not (f.endswith('_PM') or f.endswith('_received'))]\n",
    "pm_features = [f for f in features if f.endswith('_PM') and not f.startswith('delta_')]\n",
    "delta_features = [f for f in features if f.startswith('delta_')]\n",
    "received_features = [f for f in features if f.endswith('_received')]\n",
    "\n",
    "# Fight-related features (non-CUM, non-delta, non-_received)\n",
    "fight_features = ['TOTAL STR._Landed', 'TOTAL STR._Attempted', 'TOTAL STR._Attempted_avoided', \n",
    "                  'TD_Landed', 'TD_Attempted', 'TD_Attempted_avoided', \n",
    "                  'HEAD_Landed', 'HEAD_Attempted', 'HEAD_Attempted_avoided', \n",
    "                  'BODY_Landed', 'BODY_Attempted', 'BODY_Attempted_avoided', \n",
    "                  'LEG_Landed', 'LEG_Attempted', 'LEG_Attempted_avoided', \n",
    "                  'DISTANCE_Landed', 'DISTANCE_Attempted', 'DISTANCE_Attempted_avoided', \n",
    "                  'CLINCH_Landed', 'CLINCH_Attempted', 'CLINCH_Attempted_avoided',\n",
    "                   'KD',\n",
    "                    'SIG.STR._Landed',\n",
    "                    'SIG.STR._Attempted',\n",
    "                    'SIG.STR._Attempted_avoided',\n",
    "                    'GROUND_Landed',\n",
    "                    'GROUND_Attempted',\n",
    "                    'GROUND_Attempted_avoided',\n",
    "                    'CTRL_TIME_IN_SECONDS',\n",
    "                    'SUB.ATT',\n",
    "                    'REV.',]\n",
    "\n",
    "# Remaining features not in other categories\n",
    "remaining_features = [f for f in features if f not in cum_features + pm_features + delta_features + received_features + fight_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CUM_SIG.STR._Landed_received',\n",
       " 'CUM_TOTAL STR._Landed_received',\n",
       " 'CUM_TD_Landed_received',\n",
       " 'CUM_HEAD_Landed_received',\n",
       " 'CUM_BODY_Landed_received',\n",
       " 'CUM_LEG_Landed_received',\n",
       " 'CUM_DISTANCE_Landed_received',\n",
       " 'CUM_CLINCH_Landed_received',\n",
       " 'CUM_GROUND_Landed_received',\n",
       " 'SIG.STR._Landed_received',\n",
       " 'TOTAL STR._Landed_received',\n",
       " 'TD_Landed_received',\n",
       " 'HEAD_Landed_received',\n",
       " 'BODY_Landed_received',\n",
       " 'LEG_Landed_received',\n",
       " 'DISTANCE_Landed_received',\n",
       " 'CLINCH_Landed_received',\n",
       " 'GROUND_Landed_received']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "received_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cum_features) + len(pm_features) + len(delta_features) + len(received_features) + len(fight_features) + len(remaining_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-17\n"
     ]
    }
   ],
   "source": [
    "abc = df[(df['DATE'] < '2005')]\n",
    "abc = abc[features]\n",
    "\n",
    "\n",
    "X_train = df[(df['DATE'] >= '2005') & (df['DATE'] < '2023')]\n",
    "y_train = X_train['RESULT']\n",
    "print(X_train.DATE.max())\n",
    "X_train = X_train[features]\n",
    "\n",
    "X_test = df[(df['DATE'] >= '2023') & (df['DATE'] < '2024')]\n",
    "y_test = X_test['RESULT']\n",
    "X_test = X_test[features]\n",
    "\n",
    "X_val = df[(df['DATE'] >= '2024')]\n",
    "y_val = X_val['RESULT']\n",
    "X_val = X_val[features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "abc_scaled = scaler.transform(abc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns = X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns = X_test.columns)\n",
    "X_val_scaled = pd.DataFrame(X_val_scaled, columns = X_val.columns)\n",
    "abc_scaled  = pd.DataFrame(abc_scaled, columns = abc.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9020, 158)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ELO_FIGHTER</th>\n",
       "      <th>CUM_KD</th>\n",
       "      <th>CUM_SIG.STR._Landed</th>\n",
       "      <th>CUM_SIG.STR._Attempted</th>\n",
       "      <th>CUM_SIG.STR._Attempted_avoided</th>\n",
       "      <th>CUM_TOTAL STR._Landed</th>\n",
       "      <th>CUM_TOTAL STR._Attempted</th>\n",
       "      <th>CUM_TOTAL STR._Attempted_avoided</th>\n",
       "      <th>CUM_TD_Landed</th>\n",
       "      <th>CUM_TD_Attempted</th>\n",
       "      <th>...</th>\n",
       "      <th>delta_CUM_GROUND_Landed_PM</th>\n",
       "      <th>delta_CUM_SUB.ATT_PM</th>\n",
       "      <th>delta_ELO_FIGHTER</th>\n",
       "      <th>delta_AGE</th>\n",
       "      <th>delta_REACH</th>\n",
       "      <th>delta_HEIGHT</th>\n",
       "      <th>delta_SIG_STR_vs_avoid</th>\n",
       "      <th>delta_TD_vs_avoid</th>\n",
       "      <th>opponent_PC1</th>\n",
       "      <th>opponent_PC2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.248139</td>\n",
       "      <td>0.769543</td>\n",
       "      <td>-0.504221</td>\n",
       "      <td>-0.546830</td>\n",
       "      <td>-0.565189</td>\n",
       "      <td>-0.638842</td>\n",
       "      <td>-0.631079</td>\n",
       "      <td>-0.591191</td>\n",
       "      <td>-0.685737</td>\n",
       "      <td>-0.692386</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.414060</td>\n",
       "      <td>0.446222</td>\n",
       "      <td>0.144171</td>\n",
       "      <td>-0.563464</td>\n",
       "      <td>-0.913979</td>\n",
       "      <td>-1.995668</td>\n",
       "      <td>-0.159784</td>\n",
       "      <td>-0.729390</td>\n",
       "      <td>-0.901345</td>\n",
       "      <td>0.638326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.719262</td>\n",
       "      <td>-0.369616</td>\n",
       "      <td>-0.659646</td>\n",
       "      <td>-0.629953</td>\n",
       "      <td>-0.713998</td>\n",
       "      <td>-0.552265</td>\n",
       "      <td>-0.586917</td>\n",
       "      <td>-0.724418</td>\n",
       "      <td>-0.504929</td>\n",
       "      <td>-0.580706</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.155445</td>\n",
       "      <td>-0.125488</td>\n",
       "      <td>-0.462714</td>\n",
       "      <td>1.549790</td>\n",
       "      <td>-1.218638</td>\n",
       "      <td>0.399134</td>\n",
       "      <td>-0.131731</td>\n",
       "      <td>0.105043</td>\n",
       "      <td>-0.591187</td>\n",
       "      <td>0.557743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.318515</td>\n",
       "      <td>-0.749336</td>\n",
       "      <td>-0.830613</td>\n",
       "      <td>-0.794502</td>\n",
       "      <td>-0.616817</td>\n",
       "      <td>-0.798468</td>\n",
       "      <td>-0.794907</td>\n",
       "      <td>-0.617837</td>\n",
       "      <td>0.308707</td>\n",
       "      <td>0.014924</td>\n",
       "      <td>...</td>\n",
       "      <td>0.155445</td>\n",
       "      <td>0.125488</td>\n",
       "      <td>0.462714</td>\n",
       "      <td>-1.549790</td>\n",
       "      <td>1.218638</td>\n",
       "      <td>-0.399134</td>\n",
       "      <td>-0.063439</td>\n",
       "      <td>1.923466</td>\n",
       "      <td>-0.554601</td>\n",
       "      <td>0.266547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.123275</td>\n",
       "      <td>0.010103</td>\n",
       "      <td>-0.360453</td>\n",
       "      <td>-0.506117</td>\n",
       "      <td>-0.762589</td>\n",
       "      <td>-0.549559</td>\n",
       "      <td>-0.604012</td>\n",
       "      <td>-0.783630</td>\n",
       "      <td>-0.685737</td>\n",
       "      <td>-0.729613</td>\n",
       "      <td>...</td>\n",
       "      <td>0.414060</td>\n",
       "      <td>-0.446222</td>\n",
       "      <td>-0.144171</td>\n",
       "      <td>0.563464</td>\n",
       "      <td>0.913979</td>\n",
       "      <td>1.995668</td>\n",
       "      <td>3.597300</td>\n",
       "      <td>-1.220989</td>\n",
       "      <td>-0.638712</td>\n",
       "      <td>0.156029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.303808</td>\n",
       "      <td>-0.369616</td>\n",
       "      <td>0.560441</td>\n",
       "      <td>0.231811</td>\n",
       "      <td>-0.197720</td>\n",
       "      <td>1.955767</td>\n",
       "      <td>1.045662</td>\n",
       "      <td>-0.173746</td>\n",
       "      <td>2.659211</td>\n",
       "      <td>1.392317</td>\n",
       "      <td>...</td>\n",
       "      <td>0.543198</td>\n",
       "      <td>-0.639070</td>\n",
       "      <td>0.630973</td>\n",
       "      <td>0.422334</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.197401</td>\n",
       "      <td>0.534409</td>\n",
       "      <td>1.193470</td>\n",
       "      <td>-0.452746</td>\n",
       "      <td>-0.066829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9015</th>\n",
       "      <td>0.846508</td>\n",
       "      <td>-0.369616</td>\n",
       "      <td>-0.197256</td>\n",
       "      <td>-0.226213</td>\n",
       "      <td>-0.422453</td>\n",
       "      <td>-0.279006</td>\n",
       "      <td>-0.253563</td>\n",
       "      <td>-0.431319</td>\n",
       "      <td>-0.595333</td>\n",
       "      <td>-0.059530</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.195186</td>\n",
       "      <td>-0.858231</td>\n",
       "      <td>0.873302</td>\n",
       "      <td>0.696665</td>\n",
       "      <td>0.304660</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.297544</td>\n",
       "      <td>-0.832062</td>\n",
       "      <td>-0.619763</td>\n",
       "      <td>0.822012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9016</th>\n",
       "      <td>0.090158</td>\n",
       "      <td>-0.749336</td>\n",
       "      <td>-0.725701</td>\n",
       "      <td>-0.767360</td>\n",
       "      <td>-0.832439</td>\n",
       "      <td>-0.425104</td>\n",
       "      <td>-0.575520</td>\n",
       "      <td>-0.783630</td>\n",
       "      <td>0.308707</td>\n",
       "      <td>0.089377</td>\n",
       "      <td>...</td>\n",
       "      <td>0.195186</td>\n",
       "      <td>0.858231</td>\n",
       "      <td>-0.873302</td>\n",
       "      <td>-0.696665</td>\n",
       "      <td>-0.304660</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.222487</td>\n",
       "      <td>1.597109</td>\n",
       "      <td>-0.412399</td>\n",
       "      <td>-0.114677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9017</th>\n",
       "      <td>-1.059828</td>\n",
       "      <td>-0.369616</td>\n",
       "      <td>-0.593590</td>\n",
       "      <td>-0.501028</td>\n",
       "      <td>-0.486229</td>\n",
       "      <td>-0.738947</td>\n",
       "      <td>-0.609710</td>\n",
       "      <td>-0.511255</td>\n",
       "      <td>-0.595333</td>\n",
       "      <td>-0.617933</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.090685</td>\n",
       "      <td>0.209981</td>\n",
       "      <td>-0.428331</td>\n",
       "      <td>0.052329</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.399134</td>\n",
       "      <td>-0.384122</td>\n",
       "      <td>-0.578205</td>\n",
       "      <td>-0.366346</td>\n",
       "      <td>0.513558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9018</th>\n",
       "      <td>1.824815</td>\n",
       "      <td>0.769543</td>\n",
       "      <td>3.525174</td>\n",
       "      <td>3.838325</td>\n",
       "      <td>3.889988</td>\n",
       "      <td>2.502285</td>\n",
       "      <td>3.265172</td>\n",
       "      <td>3.814184</td>\n",
       "      <td>0.670323</td>\n",
       "      <td>0.201058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028789</td>\n",
       "      <td>0.142765</td>\n",
       "      <td>0.777682</td>\n",
       "      <td>-1.342059</td>\n",
       "      <td>-0.304660</td>\n",
       "      <td>0.798267</td>\n",
       "      <td>0.226601</td>\n",
       "      <td>-0.526152</td>\n",
       "      <td>1.079822</td>\n",
       "      <td>-2.159894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9019</th>\n",
       "      <td>-1.026650</td>\n",
       "      <td>-0.749336</td>\n",
       "      <td>-0.104001</td>\n",
       "      <td>0.024852</td>\n",
       "      <td>0.203154</td>\n",
       "      <td>-0.081501</td>\n",
       "      <td>0.022807</td>\n",
       "      <td>0.196329</td>\n",
       "      <td>0.851131</td>\n",
       "      <td>0.796688</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119546</td>\n",
       "      <td>0.214591</td>\n",
       "      <td>-0.738637</td>\n",
       "      <td>0.856824</td>\n",
       "      <td>-0.304660</td>\n",
       "      <td>-1.995668</td>\n",
       "      <td>-2.162968</td>\n",
       "      <td>1.575356</td>\n",
       "      <td>-1.082498</td>\n",
       "      <td>0.810741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9020 rows × 158 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ELO_FIGHTER    CUM_KD  CUM_SIG.STR._Landed  CUM_SIG.STR._Attempted  \\\n",
       "0        0.248139  0.769543            -0.504221               -0.546830   \n",
       "1       -0.719262 -0.369616            -0.659646               -0.629953   \n",
       "2       -0.318515 -0.749336            -0.830613               -0.794502   \n",
       "3        0.123275  0.010103            -0.360453               -0.506117   \n",
       "4        1.303808 -0.369616             0.560441                0.231811   \n",
       "...           ...       ...                  ...                     ...   \n",
       "9015     0.846508 -0.369616            -0.197256               -0.226213   \n",
       "9016     0.090158 -0.749336            -0.725701               -0.767360   \n",
       "9017    -1.059828 -0.369616            -0.593590               -0.501028   \n",
       "9018     1.824815  0.769543             3.525174                3.838325   \n",
       "9019    -1.026650 -0.749336            -0.104001                0.024852   \n",
       "\n",
       "      CUM_SIG.STR._Attempted_avoided  CUM_TOTAL STR._Landed  \\\n",
       "0                          -0.565189              -0.638842   \n",
       "1                          -0.713998              -0.552265   \n",
       "2                          -0.616817              -0.798468   \n",
       "3                          -0.762589              -0.549559   \n",
       "4                          -0.197720               1.955767   \n",
       "...                              ...                    ...   \n",
       "9015                       -0.422453              -0.279006   \n",
       "9016                       -0.832439              -0.425104   \n",
       "9017                       -0.486229              -0.738947   \n",
       "9018                        3.889988               2.502285   \n",
       "9019                        0.203154              -0.081501   \n",
       "\n",
       "      CUM_TOTAL STR._Attempted  CUM_TOTAL STR._Attempted_avoided  \\\n",
       "0                    -0.631079                         -0.591191   \n",
       "1                    -0.586917                         -0.724418   \n",
       "2                    -0.794907                         -0.617837   \n",
       "3                    -0.604012                         -0.783630   \n",
       "4                     1.045662                         -0.173746   \n",
       "...                        ...                               ...   \n",
       "9015                 -0.253563                         -0.431319   \n",
       "9016                 -0.575520                         -0.783630   \n",
       "9017                 -0.609710                         -0.511255   \n",
       "9018                  3.265172                          3.814184   \n",
       "9019                  0.022807                          0.196329   \n",
       "\n",
       "      CUM_TD_Landed  CUM_TD_Attempted  ...  delta_CUM_GROUND_Landed_PM  \\\n",
       "0         -0.685737         -0.692386  ...                   -0.414060   \n",
       "1         -0.504929         -0.580706  ...                   -0.155445   \n",
       "2          0.308707          0.014924  ...                    0.155445   \n",
       "3         -0.685737         -0.729613  ...                    0.414060   \n",
       "4          2.659211          1.392317  ...                    0.543198   \n",
       "...             ...               ...  ...                         ...   \n",
       "9015      -0.595333         -0.059530  ...                   -0.195186   \n",
       "9016       0.308707          0.089377  ...                    0.195186   \n",
       "9017      -0.595333         -0.617933  ...                   -0.090685   \n",
       "9018       0.670323          0.201058  ...                    0.028789   \n",
       "9019       0.851131          0.796688  ...                    0.119546   \n",
       "\n",
       "      delta_CUM_SUB.ATT_PM  delta_ELO_FIGHTER  delta_AGE  delta_REACH  \\\n",
       "0                 0.446222           0.144171  -0.563464    -0.913979   \n",
       "1                -0.125488          -0.462714   1.549790    -1.218638   \n",
       "2                 0.125488           0.462714  -1.549790     1.218638   \n",
       "3                -0.446222          -0.144171   0.563464     0.913979   \n",
       "4                -0.639070           0.630973   0.422334     0.000000   \n",
       "...                    ...                ...        ...          ...   \n",
       "9015             -0.858231           0.873302   0.696665     0.304660   \n",
       "9016              0.858231          -0.873302  -0.696665    -0.304660   \n",
       "9017              0.209981          -0.428331   0.052329     0.000000   \n",
       "9018              0.142765           0.777682  -1.342059    -0.304660   \n",
       "9019              0.214591          -0.738637   0.856824    -0.304660   \n",
       "\n",
       "      delta_HEIGHT  delta_SIG_STR_vs_avoid  delta_TD_vs_avoid  opponent_PC1  \\\n",
       "0        -1.995668               -0.159784          -0.729390     -0.901345   \n",
       "1         0.399134               -0.131731           0.105043     -0.591187   \n",
       "2        -0.399134               -0.063439           1.923466     -0.554601   \n",
       "3         1.995668                3.597300          -1.220989     -0.638712   \n",
       "4         1.197401                0.534409           1.193470     -0.452746   \n",
       "...            ...                     ...                ...           ...   \n",
       "9015      0.000000                1.297544          -0.832062     -0.619763   \n",
       "9016      0.000000               -0.222487           1.597109     -0.412399   \n",
       "9017     -0.399134               -0.384122          -0.578205     -0.366346   \n",
       "9018      0.798267                0.226601          -0.526152      1.079822   \n",
       "9019     -1.995668               -2.162968           1.575356     -1.082498   \n",
       "\n",
       "      opponent_PC2  \n",
       "0         0.638326  \n",
       "1         0.557743  \n",
       "2         0.266547  \n",
       "3         0.156029  \n",
       "4        -0.066829  \n",
       "...            ...  \n",
       "9015      0.822012  \n",
       "9016     -0.114677  \n",
       "9017      0.513558  \n",
       "9018     -2.159894  \n",
       "9019      0.810741  \n",
       "\n",
       "[9020 rows x 158 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        patience: How many epochs to wait after last time validation loss improved.\n",
    "        min_delta: Minimum change in the monitored quantity to qualify as an improvement.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2000], Loss: 1.0121, Val Loss: 1.9554\n",
      "Epoch [2/2000], Loss: 0.9975, Val Loss: 1.9372\n",
      "Epoch [3/2000], Loss: 0.9848, Val Loss: 1.9195\n",
      "Epoch [4/2000], Loss: 0.9734, Val Loss: 1.9011\n",
      "Epoch [5/2000], Loss: 0.9626, Val Loss: 1.8814\n",
      "Epoch [6/2000], Loss: 0.9518, Val Loss: 1.8601\n",
      "Epoch [7/2000], Loss: 0.9406, Val Loss: 1.8367\n",
      "Epoch [8/2000], Loss: 0.9287, Val Loss: 1.8105\n",
      "Epoch [9/2000], Loss: 0.9156, Val Loss: 1.7811\n",
      "Epoch [10/2000], Loss: 0.9012, Val Loss: 1.7486\n",
      "Epoch [11/2000], Loss: 0.8855, Val Loss: 1.7130\n",
      "Epoch [12/2000], Loss: 0.8683, Val Loss: 1.6747\n",
      "Epoch [13/2000], Loss: 0.8496, Val Loss: 1.6342\n",
      "Epoch [14/2000], Loss: 0.8296, Val Loss: 1.5917\n",
      "Epoch [15/2000], Loss: 0.8083, Val Loss: 1.5477\n",
      "Epoch [16/2000], Loss: 0.7859, Val Loss: 1.5029\n",
      "Epoch [17/2000], Loss: 0.7628, Val Loss: 1.4579\n",
      "Epoch [18/2000], Loss: 0.7392, Val Loss: 1.4137\n",
      "Epoch [19/2000], Loss: 0.7156, Val Loss: 1.3711\n",
      "Epoch [20/2000], Loss: 0.6924, Val Loss: 1.3313\n",
      "Epoch [21/2000], Loss: 0.6701, Val Loss: 1.2954\n",
      "Epoch [22/2000], Loss: 0.6495, Val Loss: 1.2643\n",
      "Epoch [23/2000], Loss: 0.6309, Val Loss: 1.2387\n",
      "Epoch [24/2000], Loss: 0.6150, Val Loss: 1.2185\n",
      "Epoch [25/2000], Loss: 0.6017, Val Loss: 1.2031\n",
      "Epoch [26/2000], Loss: 0.5911, Val Loss: 1.1914\n",
      "Epoch [27/2000], Loss: 0.5826, Val Loss: 1.1816\n",
      "Epoch [28/2000], Loss: 0.5755, Val Loss: 1.1723\n",
      "Epoch [29/2000], Loss: 0.5692, Val Loss: 1.1625\n",
      "Epoch [30/2000], Loss: 0.5630, Val Loss: 1.1519\n",
      "Epoch [31/2000], Loss: 0.5568, Val Loss: 1.1409\n",
      "Epoch [32/2000], Loss: 0.5508, Val Loss: 1.1299\n",
      "Epoch [33/2000], Loss: 0.5449, Val Loss: 1.1195\n",
      "Epoch [34/2000], Loss: 0.5394, Val Loss: 1.1101\n",
      "Epoch [35/2000], Loss: 0.5344, Val Loss: 1.1015\n",
      "Epoch [36/2000], Loss: 0.5298, Val Loss: 1.0937\n",
      "Epoch [37/2000], Loss: 0.5254, Val Loss: 1.0865\n",
      "Epoch [38/2000], Loss: 0.5212, Val Loss: 1.0798\n",
      "Epoch [39/2000], Loss: 0.5171, Val Loss: 1.0733\n",
      "Epoch [40/2000], Loss: 0.5129, Val Loss: 1.0670\n",
      "Epoch [41/2000], Loss: 0.5086, Val Loss: 1.0608\n",
      "Epoch [42/2000], Loss: 0.5042, Val Loss: 1.0545\n",
      "Epoch [43/2000], Loss: 0.4998, Val Loss: 1.0484\n",
      "Epoch [44/2000], Loss: 0.4954, Val Loss: 1.0422\n",
      "Epoch [45/2000], Loss: 0.4911, Val Loss: 1.0360\n",
      "Epoch [46/2000], Loss: 0.4868, Val Loss: 1.0297\n",
      "Epoch [47/2000], Loss: 0.4826, Val Loss: 1.0236\n",
      "Epoch [48/2000], Loss: 0.4785, Val Loss: 1.0175\n",
      "Epoch [49/2000], Loss: 0.4745, Val Loss: 1.0117\n",
      "Epoch [50/2000], Loss: 0.4706, Val Loss: 1.0062\n",
      "Epoch [51/2000], Loss: 0.4668, Val Loss: 1.0013\n",
      "Epoch [52/2000], Loss: 0.4631, Val Loss: 0.9968\n",
      "Epoch [53/2000], Loss: 0.4596, Val Loss: 0.9928\n",
      "Epoch [54/2000], Loss: 0.4561, Val Loss: 0.9896\n",
      "Epoch [55/2000], Loss: 0.4529, Val Loss: 0.9871\n",
      "Epoch [56/2000], Loss: 0.4497, Val Loss: 0.9853\n",
      "Epoch [57/2000], Loss: 0.4468, Val Loss: 0.9841\n",
      "Epoch [58/2000], Loss: 0.4441, Val Loss: 0.9833\n",
      "Epoch [59/2000], Loss: 0.4415, Val Loss: 0.9829\n",
      "Epoch [60/2000], Loss: 0.4391, Val Loss: 0.9827\n",
      "Epoch [61/2000], Loss: 0.4369, Val Loss: 0.9823\n",
      "Epoch [62/2000], Loss: 0.4348, Val Loss: 0.9816\n",
      "Epoch [63/2000], Loss: 0.4329, Val Loss: 0.9805\n",
      "Epoch [64/2000], Loss: 0.4310, Val Loss: 0.9788\n",
      "Epoch [65/2000], Loss: 0.4292, Val Loss: 0.9766\n",
      "Epoch [66/2000], Loss: 0.4275, Val Loss: 0.9739\n",
      "Epoch [67/2000], Loss: 0.4258, Val Loss: 0.9707\n",
      "Epoch [68/2000], Loss: 0.4242, Val Loss: 0.9671\n",
      "Epoch [69/2000], Loss: 0.4225, Val Loss: 0.9633\n",
      "Epoch [70/2000], Loss: 0.4210, Val Loss: 0.9593\n",
      "Epoch [71/2000], Loss: 0.4195, Val Loss: 0.9553\n",
      "Epoch [72/2000], Loss: 0.4180, Val Loss: 0.9513\n",
      "Epoch [73/2000], Loss: 0.4166, Val Loss: 0.9475\n",
      "Epoch [74/2000], Loss: 0.4153, Val Loss: 0.9440\n",
      "Epoch [75/2000], Loss: 0.4141, Val Loss: 0.9408\n",
      "Epoch [76/2000], Loss: 0.4129, Val Loss: 0.9378\n",
      "Epoch [77/2000], Loss: 0.4118, Val Loss: 0.9349\n",
      "Epoch [78/2000], Loss: 0.4108, Val Loss: 0.9323\n",
      "Epoch [79/2000], Loss: 0.4098, Val Loss: 0.9298\n",
      "Epoch [80/2000], Loss: 0.4089, Val Loss: 0.9276\n",
      "Epoch [81/2000], Loss: 0.4080, Val Loss: 0.9254\n",
      "Epoch [82/2000], Loss: 0.4071, Val Loss: 0.9233\n",
      "Epoch [83/2000], Loss: 0.4063, Val Loss: 0.9212\n",
      "Epoch [84/2000], Loss: 0.4055, Val Loss: 0.9191\n",
      "Epoch [85/2000], Loss: 0.4047, Val Loss: 0.9170\n",
      "Epoch [86/2000], Loss: 0.4039, Val Loss: 0.9148\n",
      "Epoch [87/2000], Loss: 0.4031, Val Loss: 0.9126\n",
      "Epoch [88/2000], Loss: 0.4024, Val Loss: 0.9103\n",
      "Epoch [89/2000], Loss: 0.4017, Val Loss: 0.9080\n",
      "Epoch [90/2000], Loss: 0.4010, Val Loss: 0.9057\n",
      "Epoch [91/2000], Loss: 0.4003, Val Loss: 0.9034\n",
      "Epoch [92/2000], Loss: 0.3997, Val Loss: 0.9011\n",
      "Epoch [93/2000], Loss: 0.3990, Val Loss: 0.8989\n",
      "Epoch [94/2000], Loss: 0.3984, Val Loss: 0.8967\n",
      "Epoch [95/2000], Loss: 0.3978, Val Loss: 0.8947\n",
      "Epoch [96/2000], Loss: 0.3972, Val Loss: 0.8928\n",
      "Epoch [97/2000], Loss: 0.3966, Val Loss: 0.8910\n",
      "Epoch [98/2000], Loss: 0.3960, Val Loss: 0.8893\n",
      "Epoch [99/2000], Loss: 0.3955, Val Loss: 0.8878\n",
      "Epoch [100/2000], Loss: 0.3949, Val Loss: 0.8863\n",
      "Epoch [101/2000], Loss: 0.3944, Val Loss: 0.8849\n",
      "Epoch [102/2000], Loss: 0.3938, Val Loss: 0.8836\n",
      "Epoch [103/2000], Loss: 0.3933, Val Loss: 0.8823\n",
      "Epoch [104/2000], Loss: 0.3928, Val Loss: 0.8810\n",
      "Epoch [105/2000], Loss: 0.3923, Val Loss: 0.8797\n",
      "Epoch [106/2000], Loss: 0.3918, Val Loss: 0.8785\n",
      "Epoch [107/2000], Loss: 0.3914, Val Loss: 0.8771\n",
      "Epoch [108/2000], Loss: 0.3909, Val Loss: 0.8758\n",
      "Epoch [109/2000], Loss: 0.3904, Val Loss: 0.8745\n",
      "Epoch [110/2000], Loss: 0.3900, Val Loss: 0.8732\n",
      "Epoch [111/2000], Loss: 0.3895, Val Loss: 0.8718\n",
      "Epoch [112/2000], Loss: 0.3891, Val Loss: 0.8705\n",
      "Epoch [113/2000], Loss: 0.3887, Val Loss: 0.8692\n",
      "Epoch [114/2000], Loss: 0.3882, Val Loss: 0.8678\n",
      "Epoch [115/2000], Loss: 0.3878, Val Loss: 0.8665\n",
      "Epoch [116/2000], Loss: 0.3874, Val Loss: 0.8652\n",
      "Epoch [117/2000], Loss: 0.3870, Val Loss: 0.8638\n",
      "Epoch [118/2000], Loss: 0.3866, Val Loss: 0.8624\n",
      "Epoch [119/2000], Loss: 0.3862, Val Loss: 0.8610\n",
      "Epoch [120/2000], Loss: 0.3858, Val Loss: 0.8596\n",
      "Epoch [121/2000], Loss: 0.3854, Val Loss: 0.8582\n",
      "Epoch [122/2000], Loss: 0.3851, Val Loss: 0.8568\n",
      "Epoch [123/2000], Loss: 0.3847, Val Loss: 0.8553\n",
      "Epoch [124/2000], Loss: 0.3843, Val Loss: 0.8539\n",
      "Epoch [125/2000], Loss: 0.3840, Val Loss: 0.8525\n",
      "Epoch [126/2000], Loss: 0.3836, Val Loss: 0.8511\n",
      "Epoch [127/2000], Loss: 0.3833, Val Loss: 0.8497\n",
      "Epoch [128/2000], Loss: 0.3829, Val Loss: 0.8484\n",
      "Epoch [129/2000], Loss: 0.3826, Val Loss: 0.8472\n",
      "Epoch [130/2000], Loss: 0.3823, Val Loss: 0.8459\n",
      "Epoch [131/2000], Loss: 0.3819, Val Loss: 0.8447\n",
      "Epoch [132/2000], Loss: 0.3816, Val Loss: 0.8436\n",
      "Epoch [133/2000], Loss: 0.3813, Val Loss: 0.8424\n",
      "Epoch [134/2000], Loss: 0.3810, Val Loss: 0.8413\n",
      "Epoch [135/2000], Loss: 0.3807, Val Loss: 0.8402\n",
      "Epoch [136/2000], Loss: 0.3804, Val Loss: 0.8390\n",
      "Epoch [137/2000], Loss: 0.3801, Val Loss: 0.8379\n",
      "Epoch [138/2000], Loss: 0.3798, Val Loss: 0.8367\n",
      "Epoch [139/2000], Loss: 0.3795, Val Loss: 0.8355\n",
      "Epoch [140/2000], Loss: 0.3792, Val Loss: 0.8343\n",
      "Epoch [141/2000], Loss: 0.3789, Val Loss: 0.8331\n",
      "Epoch [142/2000], Loss: 0.3787, Val Loss: 0.8320\n",
      "Epoch [143/2000], Loss: 0.3784, Val Loss: 0.8308\n",
      "Epoch [144/2000], Loss: 0.3781, Val Loss: 0.8297\n",
      "Epoch [145/2000], Loss: 0.3779, Val Loss: 0.8285\n",
      "Epoch [146/2000], Loss: 0.3776, Val Loss: 0.8274\n",
      "Epoch [147/2000], Loss: 0.3774, Val Loss: 0.8264\n",
      "Epoch [148/2000], Loss: 0.3771, Val Loss: 0.8253\n",
      "Epoch [149/2000], Loss: 0.3769, Val Loss: 0.8242\n",
      "Epoch [150/2000], Loss: 0.3766, Val Loss: 0.8232\n",
      "Epoch [151/2000], Loss: 0.3764, Val Loss: 0.8221\n",
      "Epoch [152/2000], Loss: 0.3761, Val Loss: 0.8211\n",
      "Epoch [153/2000], Loss: 0.3759, Val Loss: 0.8200\n",
      "Epoch [154/2000], Loss: 0.3757, Val Loss: 0.8190\n",
      "Epoch [155/2000], Loss: 0.3754, Val Loss: 0.8180\n",
      "Epoch [156/2000], Loss: 0.3752, Val Loss: 0.8169\n",
      "Epoch [157/2000], Loss: 0.3750, Val Loss: 0.8159\n",
      "Epoch [158/2000], Loss: 0.3748, Val Loss: 0.8149\n",
      "Epoch [159/2000], Loss: 0.3746, Val Loss: 0.8140\n",
      "Epoch [160/2000], Loss: 0.3743, Val Loss: 0.8130\n",
      "Epoch [161/2000], Loss: 0.3741, Val Loss: 0.8120\n",
      "Epoch [162/2000], Loss: 0.3739, Val Loss: 0.8111\n",
      "Epoch [163/2000], Loss: 0.3737, Val Loss: 0.8102\n",
      "Epoch [164/2000], Loss: 0.3735, Val Loss: 0.8092\n",
      "Epoch [165/2000], Loss: 0.3733, Val Loss: 0.8083\n",
      "Epoch [166/2000], Loss: 0.3731, Val Loss: 0.8074\n",
      "Epoch [167/2000], Loss: 0.3729, Val Loss: 0.8064\n",
      "Epoch [168/2000], Loss: 0.3727, Val Loss: 0.8054\n",
      "Epoch [169/2000], Loss: 0.3725, Val Loss: 0.8044\n",
      "Epoch [170/2000], Loss: 0.3723, Val Loss: 0.8034\n",
      "Epoch [171/2000], Loss: 0.3721, Val Loss: 0.8024\n",
      "Epoch [172/2000], Loss: 0.3719, Val Loss: 0.8014\n",
      "Epoch [173/2000], Loss: 0.3717, Val Loss: 0.8004\n",
      "Epoch [174/2000], Loss: 0.3716, Val Loss: 0.7994\n",
      "Epoch [175/2000], Loss: 0.3714, Val Loss: 0.7983\n",
      "Epoch [176/2000], Loss: 0.3712, Val Loss: 0.7973\n",
      "Epoch [177/2000], Loss: 0.3710, Val Loss: 0.7963\n",
      "Epoch [178/2000], Loss: 0.3708, Val Loss: 0.7952\n",
      "Epoch [179/2000], Loss: 0.3706, Val Loss: 0.7942\n",
      "Epoch [180/2000], Loss: 0.3705, Val Loss: 0.7931\n",
      "Epoch [181/2000], Loss: 0.3703, Val Loss: 0.7920\n",
      "Epoch [182/2000], Loss: 0.3701, Val Loss: 0.7910\n",
      "Epoch [183/2000], Loss: 0.3699, Val Loss: 0.7899\n",
      "Epoch [184/2000], Loss: 0.3698, Val Loss: 0.7888\n",
      "Epoch [185/2000], Loss: 0.3696, Val Loss: 0.7877\n",
      "Epoch [186/2000], Loss: 0.3694, Val Loss: 0.7865\n",
      "Epoch [187/2000], Loss: 0.3693, Val Loss: 0.7854\n",
      "Epoch [188/2000], Loss: 0.3691, Val Loss: 0.7842\n",
      "Epoch [189/2000], Loss: 0.3689, Val Loss: 0.7831\n",
      "Epoch [190/2000], Loss: 0.3687, Val Loss: 0.7819\n",
      "Epoch [191/2000], Loss: 0.3686, Val Loss: 0.7808\n",
      "Epoch [192/2000], Loss: 0.3684, Val Loss: 0.7796\n",
      "Epoch [193/2000], Loss: 0.3683, Val Loss: 0.7784\n",
      "Epoch [194/2000], Loss: 0.3681, Val Loss: 0.7773\n",
      "Epoch [195/2000], Loss: 0.3679, Val Loss: 0.7761\n",
      "Epoch [196/2000], Loss: 0.3678, Val Loss: 0.7749\n",
      "Epoch [197/2000], Loss: 0.3676, Val Loss: 0.7738\n",
      "Epoch [198/2000], Loss: 0.3674, Val Loss: 0.7726\n",
      "Epoch [199/2000], Loss: 0.3673, Val Loss: 0.7714\n",
      "Epoch [200/2000], Loss: 0.3671, Val Loss: 0.7702\n",
      "Epoch [201/2000], Loss: 0.3670, Val Loss: 0.7690\n",
      "Epoch [202/2000], Loss: 0.3668, Val Loss: 0.7678\n",
      "Epoch [203/2000], Loss: 0.3666, Val Loss: 0.7666\n",
      "Epoch [204/2000], Loss: 0.3665, Val Loss: 0.7654\n",
      "Epoch [205/2000], Loss: 0.3663, Val Loss: 0.7641\n",
      "Epoch [206/2000], Loss: 0.3662, Val Loss: 0.7629\n",
      "Epoch [207/2000], Loss: 0.3660, Val Loss: 0.7617\n",
      "Epoch [208/2000], Loss: 0.3659, Val Loss: 0.7605\n",
      "Epoch [209/2000], Loss: 0.3657, Val Loss: 0.7593\n",
      "Epoch [210/2000], Loss: 0.3656, Val Loss: 0.7581\n",
      "Epoch [211/2000], Loss: 0.3654, Val Loss: 0.7569\n",
      "Epoch [212/2000], Loss: 0.3653, Val Loss: 0.7557\n",
      "Epoch [213/2000], Loss: 0.3651, Val Loss: 0.7545\n",
      "Epoch [214/2000], Loss: 0.3650, Val Loss: 0.7532\n",
      "Epoch [215/2000], Loss: 0.3648, Val Loss: 0.7520\n",
      "Epoch [216/2000], Loss: 0.3646, Val Loss: 0.7508\n",
      "Epoch [217/2000], Loss: 0.3645, Val Loss: 0.7495\n",
      "Epoch [218/2000], Loss: 0.3644, Val Loss: 0.7483\n",
      "Epoch [219/2000], Loss: 0.3642, Val Loss: 0.7470\n",
      "Epoch [220/2000], Loss: 0.3641, Val Loss: 0.7458\n",
      "Epoch [221/2000], Loss: 0.3639, Val Loss: 0.7445\n",
      "Epoch [222/2000], Loss: 0.3638, Val Loss: 0.7433\n",
      "Epoch [223/2000], Loss: 0.3636, Val Loss: 0.7420\n",
      "Epoch [224/2000], Loss: 0.3635, Val Loss: 0.7407\n",
      "Epoch [225/2000], Loss: 0.3633, Val Loss: 0.7395\n",
      "Epoch [226/2000], Loss: 0.3632, Val Loss: 0.7382\n",
      "Epoch [227/2000], Loss: 0.3630, Val Loss: 0.7369\n",
      "Epoch [228/2000], Loss: 0.3629, Val Loss: 0.7356\n",
      "Epoch [229/2000], Loss: 0.3627, Val Loss: 0.7343\n",
      "Epoch [230/2000], Loss: 0.3626, Val Loss: 0.7330\n",
      "Epoch [231/2000], Loss: 0.3624, Val Loss: 0.7318\n",
      "Epoch [232/2000], Loss: 0.3623, Val Loss: 0.7306\n",
      "Epoch [233/2000], Loss: 0.3621, Val Loss: 0.7294\n",
      "Epoch [234/2000], Loss: 0.3620, Val Loss: 0.7281\n",
      "Epoch [235/2000], Loss: 0.3619, Val Loss: 0.7269\n",
      "Epoch [236/2000], Loss: 0.3617, Val Loss: 0.7256\n",
      "Epoch [237/2000], Loss: 0.3616, Val Loss: 0.7243\n",
      "Epoch [238/2000], Loss: 0.3614, Val Loss: 0.7231\n",
      "Epoch [239/2000], Loss: 0.3613, Val Loss: 0.7219\n",
      "Epoch [240/2000], Loss: 0.3611, Val Loss: 0.7208\n",
      "Epoch [241/2000], Loss: 0.3610, Val Loss: 0.7197\n",
      "Epoch [242/2000], Loss: 0.3609, Val Loss: 0.7185\n",
      "Epoch [243/2000], Loss: 0.3607, Val Loss: 0.7174\n",
      "Epoch [244/2000], Loss: 0.3606, Val Loss: 0.7162\n",
      "Epoch [245/2000], Loss: 0.3604, Val Loss: 0.7151\n",
      "Epoch [246/2000], Loss: 0.3603, Val Loss: 0.7140\n",
      "Epoch [247/2000], Loss: 0.3602, Val Loss: 0.7128\n",
      "Epoch [248/2000], Loss: 0.3600, Val Loss: 0.7117\n",
      "Epoch [249/2000], Loss: 0.3599, Val Loss: 0.7106\n",
      "Epoch [250/2000], Loss: 0.3598, Val Loss: 0.7095\n",
      "Epoch [251/2000], Loss: 0.3596, Val Loss: 0.7084\n",
      "Epoch [252/2000], Loss: 0.3595, Val Loss: 0.7073\n",
      "Epoch [253/2000], Loss: 0.3593, Val Loss: 0.7062\n",
      "Epoch [254/2000], Loss: 0.3592, Val Loss: 0.7052\n",
      "Epoch [255/2000], Loss: 0.3591, Val Loss: 0.7042\n",
      "Epoch [256/2000], Loss: 0.3589, Val Loss: 0.7031\n",
      "Epoch [257/2000], Loss: 0.3588, Val Loss: 0.7021\n",
      "Epoch [258/2000], Loss: 0.3587, Val Loss: 0.7011\n",
      "Epoch [259/2000], Loss: 0.3585, Val Loss: 0.7002\n",
      "Epoch [260/2000], Loss: 0.3584, Val Loss: 0.6992\n",
      "Epoch [261/2000], Loss: 0.3583, Val Loss: 0.6983\n",
      "Epoch [262/2000], Loss: 0.3581, Val Loss: 0.6974\n",
      "Epoch [263/2000], Loss: 0.3580, Val Loss: 0.6965\n",
      "Epoch [264/2000], Loss: 0.3579, Val Loss: 0.6956\n",
      "Epoch [265/2000], Loss: 0.3577, Val Loss: 0.6947\n",
      "Epoch [266/2000], Loss: 0.3576, Val Loss: 0.6938\n",
      "Epoch [267/2000], Loss: 0.3575, Val Loss: 0.6929\n",
      "Epoch [268/2000], Loss: 0.3573, Val Loss: 0.6920\n",
      "Epoch [269/2000], Loss: 0.3572, Val Loss: 0.6912\n",
      "Epoch [270/2000], Loss: 0.3571, Val Loss: 0.6903\n",
      "Epoch [271/2000], Loss: 0.3570, Val Loss: 0.6895\n",
      "Epoch [272/2000], Loss: 0.3568, Val Loss: 0.6887\n",
      "Epoch [273/2000], Loss: 0.3567, Val Loss: 0.6880\n",
      "Epoch [274/2000], Loss: 0.3566, Val Loss: 0.6872\n",
      "Epoch [275/2000], Loss: 0.3565, Val Loss: 0.6865\n",
      "Epoch [276/2000], Loss: 0.3563, Val Loss: 0.6858\n",
      "Epoch [277/2000], Loss: 0.3562, Val Loss: 0.6851\n",
      "Epoch [278/2000], Loss: 0.3561, Val Loss: 0.6845\n",
      "Epoch [279/2000], Loss: 0.3560, Val Loss: 0.6838\n",
      "Epoch [280/2000], Loss: 0.3558, Val Loss: 0.6831\n",
      "Epoch [281/2000], Loss: 0.3557, Val Loss: 0.6824\n",
      "Epoch [282/2000], Loss: 0.3556, Val Loss: 0.6818\n",
      "Epoch [283/2000], Loss: 0.3555, Val Loss: 0.6811\n",
      "Epoch [284/2000], Loss: 0.3553, Val Loss: 0.6804\n",
      "Epoch [285/2000], Loss: 0.3552, Val Loss: 0.6798\n",
      "Epoch [286/2000], Loss: 0.3551, Val Loss: 0.6791\n",
      "Epoch [287/2000], Loss: 0.3550, Val Loss: 0.6785\n",
      "Epoch [288/2000], Loss: 0.3548, Val Loss: 0.6779\n",
      "Epoch [289/2000], Loss: 0.3547, Val Loss: 0.6773\n",
      "Epoch [290/2000], Loss: 0.3546, Val Loss: 0.6768\n",
      "Epoch [291/2000], Loss: 0.3545, Val Loss: 0.6762\n",
      "Epoch [292/2000], Loss: 0.3544, Val Loss: 0.6756\n",
      "Epoch [293/2000], Loss: 0.3542, Val Loss: 0.6750\n",
      "Epoch [294/2000], Loss: 0.3541, Val Loss: 0.6744\n",
      "Epoch [295/2000], Loss: 0.3540, Val Loss: 0.6739\n",
      "Epoch [296/2000], Loss: 0.3539, Val Loss: 0.6734\n",
      "Epoch [297/2000], Loss: 0.3538, Val Loss: 0.6728\n",
      "Epoch [298/2000], Loss: 0.3536, Val Loss: 0.6723\n",
      "Epoch [299/2000], Loss: 0.3535, Val Loss: 0.6717\n",
      "Epoch [300/2000], Loss: 0.3534, Val Loss: 0.6711\n",
      "Epoch [301/2000], Loss: 0.3533, Val Loss: 0.6706\n",
      "Epoch [302/2000], Loss: 0.3532, Val Loss: 0.6701\n",
      "Epoch [303/2000], Loss: 0.3531, Val Loss: 0.6696\n",
      "Epoch [304/2000], Loss: 0.3529, Val Loss: 0.6691\n",
      "Epoch [305/2000], Loss: 0.3528, Val Loss: 0.6687\n",
      "Epoch [306/2000], Loss: 0.3527, Val Loss: 0.6682\n",
      "Epoch [307/2000], Loss: 0.3526, Val Loss: 0.6677\n",
      "Epoch [308/2000], Loss: 0.3525, Val Loss: 0.6672\n",
      "Epoch [309/2000], Loss: 0.3524, Val Loss: 0.6668\n",
      "Epoch [310/2000], Loss: 0.3522, Val Loss: 0.6663\n",
      "Epoch [311/2000], Loss: 0.3521, Val Loss: 0.6658\n",
      "Epoch [312/2000], Loss: 0.3520, Val Loss: 0.6653\n",
      "Epoch [313/2000], Loss: 0.3519, Val Loss: 0.6649\n",
      "Epoch [314/2000], Loss: 0.3518, Val Loss: 0.6644\n",
      "Epoch [315/2000], Loss: 0.3517, Val Loss: 0.6640\n",
      "Epoch [316/2000], Loss: 0.3516, Val Loss: 0.6636\n",
      "Epoch [317/2000], Loss: 0.3515, Val Loss: 0.6633\n",
      "Epoch [318/2000], Loss: 0.3513, Val Loss: 0.6629\n",
      "Epoch [319/2000], Loss: 0.3512, Val Loss: 0.6625\n",
      "Epoch [320/2000], Loss: 0.3511, Val Loss: 0.6621\n",
      "Epoch [321/2000], Loss: 0.3510, Val Loss: 0.6617\n",
      "Epoch [322/2000], Loss: 0.3509, Val Loss: 0.6613\n",
      "Epoch [323/2000], Loss: 0.3508, Val Loss: 0.6609\n",
      "Epoch [324/2000], Loss: 0.3507, Val Loss: 0.6605\n",
      "Epoch [325/2000], Loss: 0.3506, Val Loss: 0.6602\n",
      "Epoch [326/2000], Loss: 0.3505, Val Loss: 0.6598\n",
      "Epoch [327/2000], Loss: 0.3504, Val Loss: 0.6595\n",
      "Epoch [328/2000], Loss: 0.3502, Val Loss: 0.6592\n",
      "Epoch [329/2000], Loss: 0.3501, Val Loss: 0.6589\n",
      "Epoch [330/2000], Loss: 0.3500, Val Loss: 0.6587\n",
      "Epoch [331/2000], Loss: 0.3499, Val Loss: 0.6584\n",
      "Epoch [332/2000], Loss: 0.3498, Val Loss: 0.6582\n",
      "Epoch [333/2000], Loss: 0.3497, Val Loss: 0.6579\n",
      "Epoch [334/2000], Loss: 0.3496, Val Loss: 0.6576\n",
      "Epoch [335/2000], Loss: 0.3495, Val Loss: 0.6573\n",
      "Epoch [336/2000], Loss: 0.3494, Val Loss: 0.6570\n",
      "Epoch [337/2000], Loss: 0.3493, Val Loss: 0.6567\n",
      "Epoch [338/2000], Loss: 0.3492, Val Loss: 0.6565\n",
      "Epoch [339/2000], Loss: 0.3491, Val Loss: 0.6562\n",
      "Epoch [340/2000], Loss: 0.3490, Val Loss: 0.6560\n",
      "Epoch [341/2000], Loss: 0.3489, Val Loss: 0.6558\n",
      "Epoch [342/2000], Loss: 0.3487, Val Loss: 0.6555\n",
      "Epoch [343/2000], Loss: 0.3486, Val Loss: 0.6553\n",
      "Epoch [344/2000], Loss: 0.3485, Val Loss: 0.6550\n",
      "Epoch [345/2000], Loss: 0.3484, Val Loss: 0.6547\n",
      "Epoch [346/2000], Loss: 0.3483, Val Loss: 0.6545\n",
      "Epoch [347/2000], Loss: 0.3482, Val Loss: 0.6543\n",
      "Epoch [348/2000], Loss: 0.3481, Val Loss: 0.6540\n",
      "Epoch [349/2000], Loss: 0.3480, Val Loss: 0.6538\n",
      "Epoch [350/2000], Loss: 0.3479, Val Loss: 0.6535\n",
      "Epoch [351/2000], Loss: 0.3478, Val Loss: 0.6532\n",
      "Epoch [352/2000], Loss: 0.3477, Val Loss: 0.6530\n",
      "Epoch [353/2000], Loss: 0.3476, Val Loss: 0.6527\n",
      "Epoch [354/2000], Loss: 0.3475, Val Loss: 0.6524\n",
      "Epoch [355/2000], Loss: 0.3474, Val Loss: 0.6521\n",
      "Epoch [356/2000], Loss: 0.3473, Val Loss: 0.6518\n",
      "Epoch [357/2000], Loss: 0.3472, Val Loss: 0.6515\n",
      "Epoch [358/2000], Loss: 0.3471, Val Loss: 0.6513\n",
      "Epoch [359/2000], Loss: 0.3470, Val Loss: 0.6511\n",
      "Epoch [360/2000], Loss: 0.3469, Val Loss: 0.6509\n",
      "Epoch [361/2000], Loss: 0.3468, Val Loss: 0.6507\n",
      "Epoch [362/2000], Loss: 0.3467, Val Loss: 0.6505\n",
      "Epoch [363/2000], Loss: 0.3466, Val Loss: 0.6503\n",
      "Epoch [364/2000], Loss: 0.3465, Val Loss: 0.6500\n",
      "Epoch [365/2000], Loss: 0.3464, Val Loss: 0.6498\n",
      "Epoch [366/2000], Loss: 0.3463, Val Loss: 0.6496\n",
      "Epoch [367/2000], Loss: 0.3462, Val Loss: 0.6494\n",
      "Epoch [368/2000], Loss: 0.3461, Val Loss: 0.6492\n",
      "Epoch [369/2000], Loss: 0.3460, Val Loss: 0.6490\n",
      "Epoch [370/2000], Loss: 0.3459, Val Loss: 0.6488\n",
      "Epoch [371/2000], Loss: 0.3458, Val Loss: 0.6486\n",
      "Epoch [372/2000], Loss: 0.3457, Val Loss: 0.6484\n",
      "Epoch [373/2000], Loss: 0.3456, Val Loss: 0.6482\n",
      "Epoch [374/2000], Loss: 0.3455, Val Loss: 0.6480\n",
      "Epoch [375/2000], Loss: 0.3454, Val Loss: 0.6478\n",
      "Epoch [376/2000], Loss: 0.3453, Val Loss: 0.6476\n",
      "Epoch [377/2000], Loss: 0.3452, Val Loss: 0.6473\n",
      "Epoch [378/2000], Loss: 0.3451, Val Loss: 0.6471\n",
      "Epoch [379/2000], Loss: 0.3450, Val Loss: 0.6469\n",
      "Epoch [380/2000], Loss: 0.3449, Val Loss: 0.6467\n",
      "Epoch [381/2000], Loss: 0.3448, Val Loss: 0.6465\n",
      "Epoch [382/2000], Loss: 0.3447, Val Loss: 0.6462\n",
      "Epoch [383/2000], Loss: 0.3446, Val Loss: 0.6460\n",
      "Epoch [384/2000], Loss: 0.3445, Val Loss: 0.6457\n",
      "Epoch [385/2000], Loss: 0.3444, Val Loss: 0.6455\n",
      "Epoch [386/2000], Loss: 0.3443, Val Loss: 0.6452\n",
      "Epoch [387/2000], Loss: 0.3442, Val Loss: 0.6450\n",
      "Epoch [388/2000], Loss: 0.3441, Val Loss: 0.6447\n",
      "Epoch [389/2000], Loss: 0.3440, Val Loss: 0.6444\n",
      "Epoch [390/2000], Loss: 0.3440, Val Loss: 0.6441\n",
      "Epoch [391/2000], Loss: 0.3439, Val Loss: 0.6439\n",
      "Epoch [392/2000], Loss: 0.3438, Val Loss: 0.6437\n",
      "Epoch [393/2000], Loss: 0.3437, Val Loss: 0.6435\n",
      "Epoch [394/2000], Loss: 0.3436, Val Loss: 0.6434\n",
      "Epoch [395/2000], Loss: 0.3435, Val Loss: 0.6433\n",
      "Epoch [396/2000], Loss: 0.3434, Val Loss: 0.6431\n",
      "Epoch [397/2000], Loss: 0.3433, Val Loss: 0.6429\n",
      "Epoch [398/2000], Loss: 0.3432, Val Loss: 0.6427\n",
      "Epoch [399/2000], Loss: 0.3431, Val Loss: 0.6424\n",
      "Epoch [400/2000], Loss: 0.3430, Val Loss: 0.6422\n",
      "Epoch [401/2000], Loss: 0.3429, Val Loss: 0.6421\n",
      "Epoch [402/2000], Loss: 0.3428, Val Loss: 0.6419\n",
      "Epoch [403/2000], Loss: 0.3427, Val Loss: 0.6417\n",
      "Epoch [404/2000], Loss: 0.3426, Val Loss: 0.6415\n",
      "Epoch [405/2000], Loss: 0.3426, Val Loss: 0.6412\n",
      "Epoch [406/2000], Loss: 0.3425, Val Loss: 0.6410\n",
      "Epoch [407/2000], Loss: 0.3424, Val Loss: 0.6408\n",
      "Epoch [408/2000], Loss: 0.3423, Val Loss: 0.6406\n",
      "Epoch [409/2000], Loss: 0.3422, Val Loss: 0.6404\n",
      "Epoch [410/2000], Loss: 0.3421, Val Loss: 0.6401\n",
      "Epoch [411/2000], Loss: 0.3420, Val Loss: 0.6399\n",
      "Epoch [412/2000], Loss: 0.3419, Val Loss: 0.6397\n",
      "Epoch [413/2000], Loss: 0.3418, Val Loss: 0.6394\n",
      "Epoch [414/2000], Loss: 0.3417, Val Loss: 0.6392\n",
      "Epoch [415/2000], Loss: 0.3416, Val Loss: 0.6389\n",
      "Epoch [416/2000], Loss: 0.3415, Val Loss: 0.6387\n",
      "Epoch [417/2000], Loss: 0.3415, Val Loss: 0.6385\n",
      "Epoch [418/2000], Loss: 0.3414, Val Loss: 0.6382\n",
      "Epoch [419/2000], Loss: 0.3413, Val Loss: 0.6380\n",
      "Epoch [420/2000], Loss: 0.3412, Val Loss: 0.6377\n",
      "Epoch [421/2000], Loss: 0.3411, Val Loss: 0.6375\n",
      "Epoch [422/2000], Loss: 0.3410, Val Loss: 0.6372\n",
      "Epoch [423/2000], Loss: 0.3409, Val Loss: 0.6370\n",
      "Epoch [424/2000], Loss: 0.3408, Val Loss: 0.6367\n",
      "Epoch [425/2000], Loss: 0.3407, Val Loss: 0.6364\n",
      "Epoch [426/2000], Loss: 0.3406, Val Loss: 0.6361\n",
      "Epoch [427/2000], Loss: 0.3405, Val Loss: 0.6358\n",
      "Epoch [428/2000], Loss: 0.3405, Val Loss: 0.6355\n",
      "Epoch [429/2000], Loss: 0.3404, Val Loss: 0.6352\n",
      "Epoch [430/2000], Loss: 0.3403, Val Loss: 0.6348\n",
      "Epoch [431/2000], Loss: 0.3402, Val Loss: 0.6345\n",
      "Epoch [432/2000], Loss: 0.3401, Val Loss: 0.6342\n",
      "Epoch [433/2000], Loss: 0.3400, Val Loss: 0.6339\n",
      "Epoch [434/2000], Loss: 0.3399, Val Loss: 0.6336\n",
      "Epoch [435/2000], Loss: 0.3398, Val Loss: 0.6333\n",
      "Epoch [436/2000], Loss: 0.3397, Val Loss: 0.6330\n",
      "Epoch [437/2000], Loss: 0.3396, Val Loss: 0.6327\n",
      "Epoch [438/2000], Loss: 0.3396, Val Loss: 0.6323\n",
      "Epoch [439/2000], Loss: 0.3395, Val Loss: 0.6320\n",
      "Epoch [440/2000], Loss: 0.3394, Val Loss: 0.6317\n",
      "Epoch [441/2000], Loss: 0.3393, Val Loss: 0.6314\n",
      "Epoch [442/2000], Loss: 0.3392, Val Loss: 0.6311\n",
      "Epoch [443/2000], Loss: 0.3391, Val Loss: 0.6308\n",
      "Epoch [444/2000], Loss: 0.3390, Val Loss: 0.6304\n",
      "Epoch [445/2000], Loss: 0.3389, Val Loss: 0.6300\n",
      "Epoch [446/2000], Loss: 0.3388, Val Loss: 0.6297\n",
      "Epoch [447/2000], Loss: 0.3388, Val Loss: 0.6293\n",
      "Epoch [448/2000], Loss: 0.3387, Val Loss: 0.6290\n",
      "Epoch [449/2000], Loss: 0.3386, Val Loss: 0.6286\n",
      "Epoch [450/2000], Loss: 0.3385, Val Loss: 0.6283\n",
      "Epoch [451/2000], Loss: 0.3384, Val Loss: 0.6280\n",
      "Epoch [452/2000], Loss: 0.3383, Val Loss: 0.6277\n",
      "Epoch [453/2000], Loss: 0.3382, Val Loss: 0.6274\n",
      "Epoch [454/2000], Loss: 0.3381, Val Loss: 0.6271\n",
      "Epoch [455/2000], Loss: 0.3380, Val Loss: 0.6267\n",
      "Epoch [456/2000], Loss: 0.3380, Val Loss: 0.6264\n",
      "Epoch [457/2000], Loss: 0.3379, Val Loss: 0.6260\n",
      "Epoch [458/2000], Loss: 0.3378, Val Loss: 0.6257\n",
      "Epoch [459/2000], Loss: 0.3377, Val Loss: 0.6254\n",
      "Epoch [460/2000], Loss: 0.3376, Val Loss: 0.6251\n",
      "Epoch [461/2000], Loss: 0.3375, Val Loss: 0.6248\n",
      "Epoch [462/2000], Loss: 0.3374, Val Loss: 0.6246\n",
      "Epoch [463/2000], Loss: 0.3374, Val Loss: 0.6243\n",
      "Epoch [464/2000], Loss: 0.3373, Val Loss: 0.6239\n",
      "Epoch [465/2000], Loss: 0.3372, Val Loss: 0.6236\n",
      "Epoch [466/2000], Loss: 0.3371, Val Loss: 0.6231\n",
      "Epoch [467/2000], Loss: 0.3370, Val Loss: 0.6227\n",
      "Epoch [468/2000], Loss: 0.3369, Val Loss: 0.6222\n",
      "Epoch [469/2000], Loss: 0.3368, Val Loss: 0.6218\n",
      "Epoch [470/2000], Loss: 0.3368, Val Loss: 0.6213\n",
      "Epoch [471/2000], Loss: 0.3367, Val Loss: 0.6209\n",
      "Epoch [472/2000], Loss: 0.3366, Val Loss: 0.6204\n",
      "Epoch [473/2000], Loss: 0.3365, Val Loss: 0.6200\n",
      "Epoch [474/2000], Loss: 0.3364, Val Loss: 0.6196\n",
      "Epoch [475/2000], Loss: 0.3363, Val Loss: 0.6192\n",
      "Epoch [476/2000], Loss: 0.3362, Val Loss: 0.6189\n",
      "Epoch [477/2000], Loss: 0.3362, Val Loss: 0.6185\n",
      "Epoch [478/2000], Loss: 0.3361, Val Loss: 0.6182\n",
      "Epoch [479/2000], Loss: 0.3360, Val Loss: 0.6179\n",
      "Epoch [480/2000], Loss: 0.3359, Val Loss: 0.6175\n",
      "Epoch [481/2000], Loss: 0.3358, Val Loss: 0.6172\n",
      "Epoch [482/2000], Loss: 0.3357, Val Loss: 0.6168\n",
      "Epoch [483/2000], Loss: 0.3357, Val Loss: 0.6165\n",
      "Epoch [484/2000], Loss: 0.3356, Val Loss: 0.6162\n",
      "Epoch [485/2000], Loss: 0.3355, Val Loss: 0.6158\n",
      "Epoch [486/2000], Loss: 0.3354, Val Loss: 0.6155\n",
      "Epoch [487/2000], Loss: 0.3353, Val Loss: 0.6151\n",
      "Epoch [488/2000], Loss: 0.3352, Val Loss: 0.6147\n",
      "Epoch [489/2000], Loss: 0.3351, Val Loss: 0.6143\n",
      "Epoch [490/2000], Loss: 0.3351, Val Loss: 0.6140\n",
      "Epoch [491/2000], Loss: 0.3350, Val Loss: 0.6136\n",
      "Epoch [492/2000], Loss: 0.3349, Val Loss: 0.6134\n",
      "Epoch [493/2000], Loss: 0.3348, Val Loss: 0.6131\n",
      "Epoch [494/2000], Loss: 0.3347, Val Loss: 0.6128\n",
      "Epoch [495/2000], Loss: 0.3346, Val Loss: 0.6125\n",
      "Epoch [496/2000], Loss: 0.3346, Val Loss: 0.6123\n",
      "Epoch [497/2000], Loss: 0.3345, Val Loss: 0.6121\n",
      "Epoch [498/2000], Loss: 0.3344, Val Loss: 0.6118\n",
      "Epoch [499/2000], Loss: 0.3343, Val Loss: 0.6115\n",
      "Epoch [500/2000], Loss: 0.3342, Val Loss: 0.6111\n",
      "Epoch [501/2000], Loss: 0.3341, Val Loss: 0.6107\n",
      "Epoch [502/2000], Loss: 0.3341, Val Loss: 0.6103\n",
      "Epoch [503/2000], Loss: 0.3340, Val Loss: 0.6100\n",
      "Epoch [504/2000], Loss: 0.3339, Val Loss: 0.6097\n",
      "Epoch [505/2000], Loss: 0.3338, Val Loss: 0.6094\n",
      "Epoch [506/2000], Loss: 0.3337, Val Loss: 0.6092\n",
      "Epoch [507/2000], Loss: 0.3336, Val Loss: 0.6089\n",
      "Epoch [508/2000], Loss: 0.3336, Val Loss: 0.6086\n",
      "Epoch [509/2000], Loss: 0.3335, Val Loss: 0.6082\n",
      "Epoch [510/2000], Loss: 0.3334, Val Loss: 0.6078\n",
      "Epoch [511/2000], Loss: 0.3333, Val Loss: 0.6075\n",
      "Epoch [512/2000], Loss: 0.3332, Val Loss: 0.6070\n",
      "Epoch [513/2000], Loss: 0.3331, Val Loss: 0.6066\n",
      "Epoch [514/2000], Loss: 0.3331, Val Loss: 0.6062\n",
      "Epoch [515/2000], Loss: 0.3330, Val Loss: 0.6058\n",
      "Epoch [516/2000], Loss: 0.3329, Val Loss: 0.6054\n",
      "Epoch [517/2000], Loss: 0.3328, Val Loss: 0.6050\n",
      "Epoch [518/2000], Loss: 0.3327, Val Loss: 0.6046\n",
      "Epoch [519/2000], Loss: 0.3326, Val Loss: 0.6043\n",
      "Epoch [520/2000], Loss: 0.3326, Val Loss: 0.6039\n",
      "Epoch [521/2000], Loss: 0.3325, Val Loss: 0.6035\n",
      "Epoch [522/2000], Loss: 0.3324, Val Loss: 0.6030\n",
      "Epoch [523/2000], Loss: 0.3323, Val Loss: 0.6026\n",
      "Epoch [524/2000], Loss: 0.3322, Val Loss: 0.6021\n",
      "Epoch [525/2000], Loss: 0.3321, Val Loss: 0.6016\n",
      "Epoch [526/2000], Loss: 0.3321, Val Loss: 0.6012\n",
      "Epoch [527/2000], Loss: 0.3320, Val Loss: 0.6008\n",
      "Epoch [528/2000], Loss: 0.3319, Val Loss: 0.6004\n",
      "Epoch [529/2000], Loss: 0.3318, Val Loss: 0.5999\n",
      "Epoch [530/2000], Loss: 0.3317, Val Loss: 0.5994\n",
      "Epoch [531/2000], Loss: 0.3316, Val Loss: 0.5989\n",
      "Epoch [532/2000], Loss: 0.3316, Val Loss: 0.5984\n",
      "Epoch [533/2000], Loss: 0.3315, Val Loss: 0.5979\n",
      "Epoch [534/2000], Loss: 0.3314, Val Loss: 0.5974\n",
      "Epoch [535/2000], Loss: 0.3313, Val Loss: 0.5969\n",
      "Epoch [536/2000], Loss: 0.3312, Val Loss: 0.5965\n",
      "Epoch [537/2000], Loss: 0.3311, Val Loss: 0.5962\n",
      "Epoch [538/2000], Loss: 0.3311, Val Loss: 0.5958\n",
      "Epoch [539/2000], Loss: 0.3310, Val Loss: 0.5954\n",
      "Epoch [540/2000], Loss: 0.3309, Val Loss: 0.5950\n",
      "Epoch [541/2000], Loss: 0.3308, Val Loss: 0.5945\n",
      "Epoch [542/2000], Loss: 0.3307, Val Loss: 0.5941\n",
      "Epoch [543/2000], Loss: 0.3306, Val Loss: 0.5936\n",
      "Epoch [544/2000], Loss: 0.3306, Val Loss: 0.5932\n",
      "Epoch [545/2000], Loss: 0.3305, Val Loss: 0.5926\n",
      "Epoch [546/2000], Loss: 0.3304, Val Loss: 0.5921\n",
      "Epoch [547/2000], Loss: 0.3303, Val Loss: 0.5916\n",
      "Epoch [548/2000], Loss: 0.3302, Val Loss: 0.5911\n",
      "Epoch [549/2000], Loss: 0.3301, Val Loss: 0.5907\n",
      "Epoch [550/2000], Loss: 0.3301, Val Loss: 0.5902\n",
      "Epoch [551/2000], Loss: 0.3300, Val Loss: 0.5897\n",
      "Epoch [552/2000], Loss: 0.3299, Val Loss: 0.5892\n",
      "Epoch [553/2000], Loss: 0.3298, Val Loss: 0.5887\n",
      "Epoch [554/2000], Loss: 0.3297, Val Loss: 0.5882\n",
      "Epoch [555/2000], Loss: 0.3296, Val Loss: 0.5876\n",
      "Epoch [556/2000], Loss: 0.3296, Val Loss: 0.5870\n",
      "Epoch [557/2000], Loss: 0.3295, Val Loss: 0.5865\n",
      "Epoch [558/2000], Loss: 0.3294, Val Loss: 0.5860\n",
      "Epoch [559/2000], Loss: 0.3293, Val Loss: 0.5855\n",
      "Epoch [560/2000], Loss: 0.3292, Val Loss: 0.5851\n",
      "Epoch [561/2000], Loss: 0.3291, Val Loss: 0.5848\n",
      "Epoch [562/2000], Loss: 0.3291, Val Loss: 0.5843\n",
      "Epoch [563/2000], Loss: 0.3290, Val Loss: 0.5838\n",
      "Epoch [564/2000], Loss: 0.3289, Val Loss: 0.5832\n",
      "Epoch [565/2000], Loss: 0.3288, Val Loss: 0.5826\n",
      "Epoch [566/2000], Loss: 0.3287, Val Loss: 0.5821\n",
      "Epoch [567/2000], Loss: 0.3287, Val Loss: 0.5816\n",
      "Epoch [568/2000], Loss: 0.3286, Val Loss: 0.5812\n",
      "Epoch [569/2000], Loss: 0.3285, Val Loss: 0.5809\n",
      "Epoch [570/2000], Loss: 0.3284, Val Loss: 0.5805\n",
      "Epoch [571/2000], Loss: 0.3283, Val Loss: 0.5802\n",
      "Epoch [572/2000], Loss: 0.3282, Val Loss: 0.5797\n",
      "Epoch [573/2000], Loss: 0.3282, Val Loss: 0.5793\n",
      "Epoch [574/2000], Loss: 0.3281, Val Loss: 0.5788\n",
      "Epoch [575/2000], Loss: 0.3280, Val Loss: 0.5782\n",
      "Epoch [576/2000], Loss: 0.3279, Val Loss: 0.5776\n",
      "Epoch [577/2000], Loss: 0.3278, Val Loss: 0.5771\n",
      "Epoch [578/2000], Loss: 0.3278, Val Loss: 0.5766\n",
      "Epoch [579/2000], Loss: 0.3277, Val Loss: 0.5761\n",
      "Epoch [580/2000], Loss: 0.3276, Val Loss: 0.5757\n",
      "Epoch [581/2000], Loss: 0.3275, Val Loss: 0.5752\n",
      "Epoch [582/2000], Loss: 0.3274, Val Loss: 0.5748\n",
      "Epoch [583/2000], Loss: 0.3274, Val Loss: 0.5743\n",
      "Epoch [584/2000], Loss: 0.3273, Val Loss: 0.5738\n",
      "Epoch [585/2000], Loss: 0.3272, Val Loss: 0.5733\n",
      "Epoch [586/2000], Loss: 0.3271, Val Loss: 0.5726\n",
      "Epoch [587/2000], Loss: 0.3270, Val Loss: 0.5720\n",
      "Epoch [588/2000], Loss: 0.3270, Val Loss: 0.5713\n",
      "Epoch [589/2000], Loss: 0.3269, Val Loss: 0.5708\n",
      "Epoch [590/2000], Loss: 0.3268, Val Loss: 0.5704\n",
      "Epoch [591/2000], Loss: 0.3267, Val Loss: 0.5699\n",
      "Epoch [592/2000], Loss: 0.3266, Val Loss: 0.5695\n",
      "Epoch [593/2000], Loss: 0.3266, Val Loss: 0.5689\n",
      "Epoch [594/2000], Loss: 0.3265, Val Loss: 0.5684\n",
      "Epoch [595/2000], Loss: 0.3264, Val Loss: 0.5679\n",
      "Epoch [596/2000], Loss: 0.3263, Val Loss: 0.5673\n",
      "Epoch [597/2000], Loss: 0.3262, Val Loss: 0.5667\n",
      "Epoch [598/2000], Loss: 0.3262, Val Loss: 0.5662\n",
      "Epoch [599/2000], Loss: 0.3261, Val Loss: 0.5657\n",
      "Epoch [600/2000], Loss: 0.3260, Val Loss: 0.5652\n",
      "Epoch [601/2000], Loss: 0.3259, Val Loss: 0.5649\n",
      "Epoch [602/2000], Loss: 0.3259, Val Loss: 0.5645\n",
      "Epoch [603/2000], Loss: 0.3258, Val Loss: 0.5640\n",
      "Epoch [604/2000], Loss: 0.3257, Val Loss: 0.5635\n",
      "Epoch [605/2000], Loss: 0.3256, Val Loss: 0.5630\n",
      "Epoch [606/2000], Loss: 0.3255, Val Loss: 0.5626\n",
      "Epoch [607/2000], Loss: 0.3255, Val Loss: 0.5623\n",
      "Epoch [608/2000], Loss: 0.3254, Val Loss: 0.5620\n",
      "Epoch [609/2000], Loss: 0.3253, Val Loss: 0.5617\n",
      "Epoch [610/2000], Loss: 0.3252, Val Loss: 0.5613\n",
      "Epoch [611/2000], Loss: 0.3252, Val Loss: 0.5607\n",
      "Epoch [612/2000], Loss: 0.3251, Val Loss: 0.5602\n",
      "Epoch [613/2000], Loss: 0.3250, Val Loss: 0.5597\n",
      "Epoch [614/2000], Loss: 0.3249, Val Loss: 0.5594\n",
      "Epoch [615/2000], Loss: 0.3248, Val Loss: 0.5590\n",
      "Epoch [616/2000], Loss: 0.3248, Val Loss: 0.5586\n",
      "Epoch [617/2000], Loss: 0.3247, Val Loss: 0.5581\n",
      "Epoch [618/2000], Loss: 0.3246, Val Loss: 0.5576\n",
      "Epoch [619/2000], Loss: 0.3245, Val Loss: 0.5573\n",
      "Epoch [620/2000], Loss: 0.3245, Val Loss: 0.5571\n",
      "Epoch [621/2000], Loss: 0.3244, Val Loss: 0.5567\n",
      "Epoch [622/2000], Loss: 0.3243, Val Loss: 0.5562\n",
      "Epoch [623/2000], Loss: 0.3242, Val Loss: 0.5558\n",
      "Epoch [624/2000], Loss: 0.3241, Val Loss: 0.5554\n",
      "Epoch [625/2000], Loss: 0.3241, Val Loss: 0.5551\n",
      "Epoch [626/2000], Loss: 0.3240, Val Loss: 0.5548\n",
      "Epoch [627/2000], Loss: 0.3239, Val Loss: 0.5543\n",
      "Epoch [628/2000], Loss: 0.3238, Val Loss: 0.5537\n",
      "Epoch [629/2000], Loss: 0.3238, Val Loss: 0.5532\n",
      "Epoch [630/2000], Loss: 0.3237, Val Loss: 0.5527\n",
      "Epoch [631/2000], Loss: 0.3236, Val Loss: 0.5522\n",
      "Epoch [632/2000], Loss: 0.3235, Val Loss: 0.5517\n",
      "Epoch [633/2000], Loss: 0.3234, Val Loss: 0.5510\n",
      "Epoch [634/2000], Loss: 0.3234, Val Loss: 0.5503\n",
      "Epoch [635/2000], Loss: 0.3233, Val Loss: 0.5497\n",
      "Epoch [636/2000], Loss: 0.3232, Val Loss: 0.5492\n",
      "Epoch [637/2000], Loss: 0.3231, Val Loss: 0.5488\n",
      "Epoch [638/2000], Loss: 0.3231, Val Loss: 0.5484\n",
      "Epoch [639/2000], Loss: 0.3230, Val Loss: 0.5479\n",
      "Epoch [640/2000], Loss: 0.3229, Val Loss: 0.5475\n",
      "Epoch [641/2000], Loss: 0.3228, Val Loss: 0.5471\n",
      "Epoch [642/2000], Loss: 0.3228, Val Loss: 0.5467\n",
      "Epoch [643/2000], Loss: 0.3227, Val Loss: 0.5462\n",
      "Epoch [644/2000], Loss: 0.3226, Val Loss: 0.5458\n",
      "Epoch [645/2000], Loss: 0.3225, Val Loss: 0.5454\n",
      "Epoch [646/2000], Loss: 0.3224, Val Loss: 0.5451\n",
      "Epoch [647/2000], Loss: 0.3224, Val Loss: 0.5448\n",
      "Epoch [648/2000], Loss: 0.3223, Val Loss: 0.5443\n",
      "Epoch [649/2000], Loss: 0.3222, Val Loss: 0.5439\n",
      "Epoch [650/2000], Loss: 0.3221, Val Loss: 0.5436\n",
      "Epoch [651/2000], Loss: 0.3221, Val Loss: 0.5432\n",
      "Epoch [652/2000], Loss: 0.3220, Val Loss: 0.5429\n",
      "Epoch [653/2000], Loss: 0.3219, Val Loss: 0.5426\n",
      "Epoch [654/2000], Loss: 0.3218, Val Loss: 0.5423\n",
      "Epoch [655/2000], Loss: 0.3218, Val Loss: 0.5419\n",
      "Epoch [656/2000], Loss: 0.3217, Val Loss: 0.5415\n",
      "Epoch [657/2000], Loss: 0.3216, Val Loss: 0.5413\n",
      "Epoch [658/2000], Loss: 0.3215, Val Loss: 0.5410\n",
      "Epoch [659/2000], Loss: 0.3215, Val Loss: 0.5408\n",
      "Epoch [660/2000], Loss: 0.3214, Val Loss: 0.5405\n",
      "Epoch [661/2000], Loss: 0.3213, Val Loss: 0.5402\n",
      "Epoch [662/2000], Loss: 0.3212, Val Loss: 0.5398\n",
      "Epoch [663/2000], Loss: 0.3212, Val Loss: 0.5394\n",
      "Epoch [664/2000], Loss: 0.3211, Val Loss: 0.5391\n",
      "Epoch [665/2000], Loss: 0.3210, Val Loss: 0.5388\n",
      "Epoch [666/2000], Loss: 0.3209, Val Loss: 0.5386\n",
      "Epoch [667/2000], Loss: 0.3209, Val Loss: 0.5382\n",
      "Epoch [668/2000], Loss: 0.3208, Val Loss: 0.5379\n",
      "Epoch [669/2000], Loss: 0.3207, Val Loss: 0.5376\n",
      "Epoch [670/2000], Loss: 0.3206, Val Loss: 0.5373\n",
      "Epoch [671/2000], Loss: 0.3206, Val Loss: 0.5370\n",
      "Epoch [672/2000], Loss: 0.3205, Val Loss: 0.5366\n",
      "Epoch [673/2000], Loss: 0.3204, Val Loss: 0.5362\n",
      "Epoch [674/2000], Loss: 0.3203, Val Loss: 0.5361\n",
      "Epoch [675/2000], Loss: 0.3203, Val Loss: 0.5360\n",
      "Epoch [676/2000], Loss: 0.3202, Val Loss: 0.5358\n",
      "Epoch [677/2000], Loss: 0.3201, Val Loss: 0.5356\n",
      "Epoch [678/2000], Loss: 0.3200, Val Loss: 0.5351\n",
      "Epoch [679/2000], Loss: 0.3200, Val Loss: 0.5345\n",
      "Epoch [680/2000], Loss: 0.3199, Val Loss: 0.5342\n",
      "Epoch [681/2000], Loss: 0.3198, Val Loss: 0.5341\n",
      "Epoch [682/2000], Loss: 0.3197, Val Loss: 0.5341\n",
      "Epoch [683/2000], Loss: 0.3197, Val Loss: 0.5338\n",
      "Epoch [684/2000], Loss: 0.3196, Val Loss: 0.5335\n",
      "Epoch [685/2000], Loss: 0.3195, Val Loss: 0.5333\n",
      "Epoch [686/2000], Loss: 0.3194, Val Loss: 0.5331\n",
      "Epoch [687/2000], Loss: 0.3194, Val Loss: 0.5328\n",
      "Epoch [688/2000], Loss: 0.3193, Val Loss: 0.5325\n",
      "Epoch [689/2000], Loss: 0.3192, Val Loss: 0.5322\n",
      "Epoch [690/2000], Loss: 0.3191, Val Loss: 0.5320\n",
      "Epoch [691/2000], Loss: 0.3191, Val Loss: 0.5319\n",
      "Epoch [692/2000], Loss: 0.3190, Val Loss: 0.5317\n",
      "Epoch [693/2000], Loss: 0.3189, Val Loss: 0.5315\n",
      "Epoch [694/2000], Loss: 0.3189, Val Loss: 0.5313\n",
      "Epoch [695/2000], Loss: 0.3188, Val Loss: 0.5311\n",
      "Epoch [696/2000], Loss: 0.3187, Val Loss: 0.5308\n",
      "Epoch [697/2000], Loss: 0.3186, Val Loss: 0.5306\n",
      "Epoch [698/2000], Loss: 0.3186, Val Loss: 0.5305\n",
      "Epoch [699/2000], Loss: 0.3185, Val Loss: 0.5305\n",
      "Epoch [700/2000], Loss: 0.3184, Val Loss: 0.5304\n",
      "Epoch [701/2000], Loss: 0.3183, Val Loss: 0.5301\n",
      "Epoch [702/2000], Loss: 0.3183, Val Loss: 0.5299\n",
      "Epoch [703/2000], Loss: 0.3182, Val Loss: 0.5297\n",
      "Epoch [704/2000], Loss: 0.3181, Val Loss: 0.5296\n",
      "Epoch [705/2000], Loss: 0.3181, Val Loss: 0.5294\n",
      "Epoch [706/2000], Loss: 0.3180, Val Loss: 0.5290\n",
      "Epoch [707/2000], Loss: 0.3179, Val Loss: 0.5286\n",
      "Epoch [708/2000], Loss: 0.3178, Val Loss: 0.5284\n",
      "Epoch [709/2000], Loss: 0.3178, Val Loss: 0.5283\n",
      "Epoch [710/2000], Loss: 0.3177, Val Loss: 0.5281\n",
      "Epoch [711/2000], Loss: 0.3176, Val Loss: 0.5278\n",
      "Epoch [712/2000], Loss: 0.3176, Val Loss: 0.5277\n",
      "Epoch [713/2000], Loss: 0.3175, Val Loss: 0.5275\n",
      "Epoch [714/2000], Loss: 0.3174, Val Loss: 0.5275\n",
      "Epoch [715/2000], Loss: 0.3174, Val Loss: 0.5274\n",
      "Epoch [716/2000], Loss: 0.3173, Val Loss: 0.5273\n",
      "Epoch [717/2000], Loss: 0.3172, Val Loss: 0.5273\n",
      "Epoch [718/2000], Loss: 0.3171, Val Loss: 0.5272\n",
      "Epoch [719/2000], Loss: 0.3171, Val Loss: 0.5267\n",
      "Epoch [720/2000], Loss: 0.3170, Val Loss: 0.5264\n",
      "Epoch [721/2000], Loss: 0.3169, Val Loss: 0.5263\n",
      "Epoch [722/2000], Loss: 0.3169, Val Loss: 0.5262\n",
      "Epoch [723/2000], Loss: 0.3168, Val Loss: 0.5260\n",
      "Epoch [724/2000], Loss: 0.3167, Val Loss: 0.5257\n",
      "Epoch [725/2000], Loss: 0.3167, Val Loss: 0.5256\n",
      "Epoch [726/2000], Loss: 0.3166, Val Loss: 0.5256\n",
      "Epoch [727/2000], Loss: 0.3165, Val Loss: 0.5255\n",
      "Epoch [728/2000], Loss: 0.3164, Val Loss: 0.5253\n",
      "Epoch [729/2000], Loss: 0.3164, Val Loss: 0.5250\n",
      "Epoch [730/2000], Loss: 0.3163, Val Loss: 0.5247\n",
      "Epoch [731/2000], Loss: 0.3162, Val Loss: 0.5246\n",
      "Epoch [732/2000], Loss: 0.3162, Val Loss: 0.5245\n",
      "Epoch [733/2000], Loss: 0.3161, Val Loss: 0.5245\n",
      "Epoch [734/2000], Loss: 0.3160, Val Loss: 0.5244\n",
      "Epoch [735/2000], Loss: 0.3160, Val Loss: 0.5242\n",
      "Epoch [736/2000], Loss: 0.3159, Val Loss: 0.5239\n",
      "Epoch [737/2000], Loss: 0.3158, Val Loss: 0.5236\n",
      "Epoch [738/2000], Loss: 0.3157, Val Loss: 0.5235\n",
      "Epoch [739/2000], Loss: 0.3157, Val Loss: 0.5233\n",
      "Epoch [740/2000], Loss: 0.3156, Val Loss: 0.5232\n",
      "Epoch [741/2000], Loss: 0.3155, Val Loss: 0.5231\n",
      "Epoch [742/2000], Loss: 0.3155, Val Loss: 0.5230\n",
      "Epoch [743/2000], Loss: 0.3154, Val Loss: 0.5229\n",
      "Epoch [744/2000], Loss: 0.3153, Val Loss: 0.5228\n",
      "Epoch [745/2000], Loss: 0.3153, Val Loss: 0.5226\n",
      "Epoch [746/2000], Loss: 0.3152, Val Loss: 0.5225\n",
      "Epoch [747/2000], Loss: 0.3151, Val Loss: 0.5225\n",
      "Epoch [748/2000], Loss: 0.3151, Val Loss: 0.5225\n",
      "Epoch [749/2000], Loss: 0.3150, Val Loss: 0.5224\n",
      "Epoch [750/2000], Loss: 0.3149, Val Loss: 0.5223\n",
      "Epoch [751/2000], Loss: 0.3148, Val Loss: 0.5221\n",
      "Epoch [752/2000], Loss: 0.3148, Val Loss: 0.5218\n",
      "Epoch [753/2000], Loss: 0.3147, Val Loss: 0.5215\n",
      "Epoch [754/2000], Loss: 0.3147, Val Loss: 0.5212\n",
      "Epoch [755/2000], Loss: 0.3146, Val Loss: 0.5212\n",
      "Epoch [756/2000], Loss: 0.3145, Val Loss: 0.5214\n",
      "Epoch [757/2000], Loss: 0.3144, Val Loss: 0.5216\n",
      "Epoch [758/2000], Loss: 0.3144, Val Loss: 0.5215\n",
      "Epoch [759/2000], Loss: 0.3143, Val Loss: 0.5212\n",
      "Epoch [760/2000], Loss: 0.3142, Val Loss: 0.5209\n",
      "Epoch [761/2000], Loss: 0.3142, Val Loss: 0.5207\n",
      "Epoch [762/2000], Loss: 0.3141, Val Loss: 0.5205\n",
      "Epoch [763/2000], Loss: 0.3140, Val Loss: 0.5207\n",
      "Epoch [764/2000], Loss: 0.3140, Val Loss: 0.5207\n",
      "Epoch [765/2000], Loss: 0.3139, Val Loss: 0.5207\n",
      "Epoch [766/2000], Loss: 0.3138, Val Loss: 0.5207\n",
      "Epoch [767/2000], Loss: 0.3138, Val Loss: 0.5208\n",
      "Epoch [768/2000], Loss: 0.3137, Val Loss: 0.5208\n",
      "Epoch [769/2000], Loss: 0.3136, Val Loss: 0.5205\n",
      "Epoch [770/2000], Loss: 0.3136, Val Loss: 0.5201\n",
      "Epoch [771/2000], Loss: 0.3135, Val Loss: 0.5201\n",
      "Epoch [772/2000], Loss: 0.3134, Val Loss: 0.5202\n",
      "Epoch [773/2000], Loss: 0.3134, Val Loss: 0.5201\n",
      "Epoch [774/2000], Loss: 0.3133, Val Loss: 0.5201\n",
      "Epoch [775/2000], Loss: 0.3132, Val Loss: 0.5201\n",
      "Epoch [776/2000], Loss: 0.3132, Val Loss: 0.5199\n",
      "Epoch [777/2000], Loss: 0.3131, Val Loss: 0.5194\n",
      "Epoch [778/2000], Loss: 0.3130, Val Loss: 0.5191\n",
      "Epoch [779/2000], Loss: 0.3130, Val Loss: 0.5191\n",
      "Epoch [780/2000], Loss: 0.3129, Val Loss: 0.5192\n",
      "Epoch [781/2000], Loss: 0.3128, Val Loss: 0.5190\n",
      "Epoch [782/2000], Loss: 0.3128, Val Loss: 0.5188\n",
      "Epoch [783/2000], Loss: 0.3127, Val Loss: 0.5186\n",
      "Epoch [784/2000], Loss: 0.3126, Val Loss: 0.5184\n",
      "Epoch [785/2000], Loss: 0.3126, Val Loss: 0.5183\n",
      "Epoch [786/2000], Loss: 0.3125, Val Loss: 0.5183\n",
      "Epoch [787/2000], Loss: 0.3124, Val Loss: 0.5181\n",
      "Epoch [788/2000], Loss: 0.3124, Val Loss: 0.5180\n",
      "Epoch [789/2000], Loss: 0.3123, Val Loss: 0.5179\n",
      "Epoch [790/2000], Loss: 0.3122, Val Loss: 0.5177\n",
      "Epoch [791/2000], Loss: 0.3122, Val Loss: 0.5175\n",
      "Epoch [792/2000], Loss: 0.3121, Val Loss: 0.5175\n",
      "Epoch [793/2000], Loss: 0.3121, Val Loss: 0.5173\n",
      "Epoch [794/2000], Loss: 0.3120, Val Loss: 0.5171\n",
      "Epoch [795/2000], Loss: 0.3119, Val Loss: 0.5171\n",
      "Epoch [796/2000], Loss: 0.3119, Val Loss: 0.5173\n",
      "Epoch [797/2000], Loss: 0.3118, Val Loss: 0.5172\n",
      "Epoch [798/2000], Loss: 0.3117, Val Loss: 0.5169\n",
      "Epoch [799/2000], Loss: 0.3117, Val Loss: 0.5169\n",
      "Epoch [800/2000], Loss: 0.3116, Val Loss: 0.5170\n",
      "Epoch [801/2000], Loss: 0.3115, Val Loss: 0.5169\n",
      "Epoch [802/2000], Loss: 0.3115, Val Loss: 0.5168\n",
      "Epoch [803/2000], Loss: 0.3114, Val Loss: 0.5167\n",
      "Epoch [804/2000], Loss: 0.3113, Val Loss: 0.5165\n",
      "Epoch [805/2000], Loss: 0.3113, Val Loss: 0.5163\n",
      "Epoch [806/2000], Loss: 0.3112, Val Loss: 0.5161\n",
      "Epoch [807/2000], Loss: 0.3111, Val Loss: 0.5162\n",
      "Epoch [808/2000], Loss: 0.3111, Val Loss: 0.5165\n",
      "Epoch [809/2000], Loss: 0.3110, Val Loss: 0.5166\n",
      "Epoch [810/2000], Loss: 0.3109, Val Loss: 0.5165\n",
      "Epoch [811/2000], Loss: 0.3109, Val Loss: 0.5166\n",
      "Epoch [812/2000], Loss: 0.3108, Val Loss: 0.5165\n",
      "Epoch [813/2000], Loss: 0.3107, Val Loss: 0.5162\n",
      "Epoch [814/2000], Loss: 0.3107, Val Loss: 0.5160\n",
      "Epoch [815/2000], Loss: 0.3106, Val Loss: 0.5159\n",
      "Epoch [816/2000], Loss: 0.3105, Val Loss: 0.5156\n",
      "Epoch [817/2000], Loss: 0.3105, Val Loss: 0.5154\n",
      "Epoch [818/2000], Loss: 0.3104, Val Loss: 0.5157\n",
      "Epoch [819/2000], Loss: 0.3104, Val Loss: 0.5160\n",
      "Epoch [820/2000], Loss: 0.3103, Val Loss: 0.5159\n",
      "Epoch [821/2000], Loss: 0.3102, Val Loss: 0.5159\n",
      "Epoch [822/2000], Loss: 0.3102, Val Loss: 0.5159\n",
      "Epoch [823/2000], Loss: 0.3101, Val Loss: 0.5156\n",
      "Epoch [824/2000], Loss: 0.3100, Val Loss: 0.5154\n",
      "Epoch [825/2000], Loss: 0.3100, Val Loss: 0.5155\n",
      "Epoch [826/2000], Loss: 0.3099, Val Loss: 0.5153\n",
      "Epoch [827/2000], Loss: 0.3099, Val Loss: 0.5152\n",
      "Epoch [828/2000], Loss: 0.3098, Val Loss: 0.5154\n",
      "Epoch [829/2000], Loss: 0.3097, Val Loss: 0.5154\n",
      "Epoch [830/2000], Loss: 0.3097, Val Loss: 0.5151\n",
      "Epoch [831/2000], Loss: 0.3096, Val Loss: 0.5151\n",
      "Epoch [832/2000], Loss: 0.3096, Val Loss: 0.5151\n",
      "Epoch [833/2000], Loss: 0.3095, Val Loss: 0.5151\n",
      "Epoch [834/2000], Loss: 0.3094, Val Loss: 0.5151\n",
      "Epoch [835/2000], Loss: 0.3094, Val Loss: 0.5149\n",
      "Epoch [836/2000], Loss: 0.3093, Val Loss: 0.5146\n",
      "Epoch [837/2000], Loss: 0.3093, Val Loss: 0.5144\n",
      "Epoch [838/2000], Loss: 0.3092, Val Loss: 0.5146\n",
      "Epoch [839/2000], Loss: 0.3091, Val Loss: 0.5146\n",
      "Epoch [840/2000], Loss: 0.3091, Val Loss: 0.5144\n",
      "Epoch [841/2000], Loss: 0.3090, Val Loss: 0.5142\n",
      "Epoch [842/2000], Loss: 0.3090, Val Loss: 0.5141\n",
      "Epoch [843/2000], Loss: 0.3089, Val Loss: 0.5143\n",
      "Epoch [844/2000], Loss: 0.3088, Val Loss: 0.5143\n",
      "Epoch [845/2000], Loss: 0.3088, Val Loss: 0.5142\n",
      "Epoch [846/2000], Loss: 0.3087, Val Loss: 0.5142\n",
      "Epoch [847/2000], Loss: 0.3087, Val Loss: 0.5144\n",
      "Epoch [848/2000], Loss: 0.3086, Val Loss: 0.5142\n",
      "Epoch [849/2000], Loss: 0.3085, Val Loss: 0.5139\n",
      "Epoch [850/2000], Loss: 0.3085, Val Loss: 0.5137\n",
      "Epoch [851/2000], Loss: 0.3084, Val Loss: 0.5136\n",
      "Epoch [852/2000], Loss: 0.3084, Val Loss: 0.5132\n",
      "Epoch [853/2000], Loss: 0.3083, Val Loss: 0.5130\n",
      "Epoch [854/2000], Loss: 0.3082, Val Loss: 0.5135\n",
      "Epoch [855/2000], Loss: 0.3082, Val Loss: 0.5138\n",
      "Epoch [856/2000], Loss: 0.3081, Val Loss: 0.5135\n",
      "Epoch [857/2000], Loss: 0.3081, Val Loss: 0.5132\n",
      "Epoch [858/2000], Loss: 0.3080, Val Loss: 0.5132\n",
      "Epoch [859/2000], Loss: 0.3080, Val Loss: 0.5131\n",
      "Epoch [860/2000], Loss: 0.3079, Val Loss: 0.5130\n",
      "Epoch [861/2000], Loss: 0.3078, Val Loss: 0.5131\n",
      "Epoch [862/2000], Loss: 0.3078, Val Loss: 0.5129\n",
      "Epoch [863/2000], Loss: 0.3077, Val Loss: 0.5126\n",
      "Epoch [864/2000], Loss: 0.3077, Val Loss: 0.5129\n",
      "Epoch [865/2000], Loss: 0.3076, Val Loss: 0.5131\n",
      "Epoch [866/2000], Loss: 0.3075, Val Loss: 0.5128\n",
      "Epoch [867/2000], Loss: 0.3075, Val Loss: 0.5129\n",
      "Epoch [868/2000], Loss: 0.3074, Val Loss: 0.5131\n",
      "Epoch [869/2000], Loss: 0.3074, Val Loss: 0.5129\n",
      "Epoch [870/2000], Loss: 0.3073, Val Loss: 0.5128\n",
      "Epoch [871/2000], Loss: 0.3073, Val Loss: 0.5130\n",
      "Epoch [872/2000], Loss: 0.3072, Val Loss: 0.5131\n",
      "Epoch [873/2000], Loss: 0.3071, Val Loss: 0.5132\n",
      "Epoch [874/2000], Loss: 0.3071, Val Loss: 0.5134\n",
      "Epoch [875/2000], Loss: 0.3070, Val Loss: 0.5133\n",
      "Epoch [876/2000], Loss: 0.3070, Val Loss: 0.5132\n",
      "Epoch [877/2000], Loss: 0.3069, Val Loss: 0.5133\n",
      "Epoch [878/2000], Loss: 0.3069, Val Loss: 0.5135\n",
      "Epoch [879/2000], Loss: 0.3068, Val Loss: 0.5130\n",
      "Epoch [880/2000], Loss: 0.3067, Val Loss: 0.5127\n",
      "Epoch [881/2000], Loss: 0.3067, Val Loss: 0.5129\n",
      "Epoch [882/2000], Loss: 0.3066, Val Loss: 0.5129\n",
      "Epoch [883/2000], Loss: 0.3066, Val Loss: 0.5129\n",
      "Epoch [884/2000], Loss: 0.3065, Val Loss: 0.5132\n",
      "Epoch [885/2000], Loss: 0.3065, Val Loss: 0.5135\n",
      "Epoch [886/2000], Loss: 0.3064, Val Loss: 0.5132\n",
      "Epoch [887/2000], Loss: 0.3064, Val Loss: 0.5131\n",
      "Epoch [888/2000], Loss: 0.3063, Val Loss: 0.5132\n",
      "Epoch [889/2000], Loss: 0.3062, Val Loss: 0.5132\n",
      "Epoch [890/2000], Loss: 0.3062, Val Loss: 0.5131\n",
      "Epoch [891/2000], Loss: 0.3061, Val Loss: 0.5134\n",
      "Epoch [892/2000], Loss: 0.3061, Val Loss: 0.5135\n",
      "Epoch [893/2000], Loss: 0.3060, Val Loss: 0.5132\n",
      "Epoch [894/2000], Loss: 0.3060, Val Loss: 0.5128\n",
      "Epoch [895/2000], Loss: 0.3059, Val Loss: 0.5125\n",
      "Epoch [896/2000], Loss: 0.3059, Val Loss: 0.5124\n",
      "Epoch [897/2000], Loss: 0.3058, Val Loss: 0.5126\n",
      "Epoch [898/2000], Loss: 0.3058, Val Loss: 0.5128\n",
      "Epoch [899/2000], Loss: 0.3057, Val Loss: 0.5129\n",
      "Epoch [900/2000], Loss: 0.3056, Val Loss: 0.5130\n",
      "Epoch [901/2000], Loss: 0.3056, Val Loss: 0.5132\n",
      "Epoch [902/2000], Loss: 0.3055, Val Loss: 0.5128\n",
      "Epoch [903/2000], Loss: 0.3055, Val Loss: 0.5122\n",
      "Epoch [904/2000], Loss: 0.3054, Val Loss: 0.5120\n",
      "Epoch [905/2000], Loss: 0.3054, Val Loss: 0.5122\n",
      "Epoch [906/2000], Loss: 0.3053, Val Loss: 0.5124\n",
      "Epoch [907/2000], Loss: 0.3053, Val Loss: 0.5127\n",
      "Epoch [908/2000], Loss: 0.3052, Val Loss: 0.5127\n",
      "Epoch [909/2000], Loss: 0.3052, Val Loss: 0.5122\n",
      "Epoch [910/2000], Loss: 0.3051, Val Loss: 0.5116\n",
      "Epoch [911/2000], Loss: 0.3051, Val Loss: 0.5115\n",
      "Epoch [912/2000], Loss: 0.3050, Val Loss: 0.5114\n",
      "Epoch [913/2000], Loss: 0.3050, Val Loss: 0.5113\n",
      "Epoch [914/2000], Loss: 0.3049, Val Loss: 0.5117\n",
      "Epoch [915/2000], Loss: 0.3049, Val Loss: 0.5118\n",
      "Epoch [916/2000], Loss: 0.3048, Val Loss: 0.5116\n",
      "Epoch [917/2000], Loss: 0.3048, Val Loss: 0.5115\n",
      "Epoch [918/2000], Loss: 0.3047, Val Loss: 0.5115\n",
      "Epoch [919/2000], Loss: 0.3047, Val Loss: 0.5112\n",
      "Epoch [920/2000], Loss: 0.3046, Val Loss: 0.5110\n",
      "Epoch [921/2000], Loss: 0.3046, Val Loss: 0.5110\n",
      "Epoch [922/2000], Loss: 0.3045, Val Loss: 0.5112\n",
      "Epoch [923/2000], Loss: 0.3044, Val Loss: 0.5111\n",
      "Epoch [924/2000], Loss: 0.3044, Val Loss: 0.5112\n",
      "Epoch [925/2000], Loss: 0.3043, Val Loss: 0.5108\n",
      "Epoch [926/2000], Loss: 0.3043, Val Loss: 0.5106\n",
      "Epoch [927/2000], Loss: 0.3043, Val Loss: 0.5107\n",
      "Epoch [928/2000], Loss: 0.3042, Val Loss: 0.5106\n",
      "Epoch [929/2000], Loss: 0.3041, Val Loss: 0.5102\n",
      "Epoch [930/2000], Loss: 0.3041, Val Loss: 0.5103\n",
      "Epoch [931/2000], Loss: 0.3040, Val Loss: 0.5107\n",
      "Epoch [932/2000], Loss: 0.3040, Val Loss: 0.5106\n",
      "Epoch [933/2000], Loss: 0.3040, Val Loss: 0.5105\n",
      "Epoch [934/2000], Loss: 0.3039, Val Loss: 0.5104\n",
      "Epoch [935/2000], Loss: 0.3039, Val Loss: 0.5102\n",
      "Epoch [936/2000], Loss: 0.3038, Val Loss: 0.5104\n",
      "Epoch [937/2000], Loss: 0.3037, Val Loss: 0.5101\n",
      "Epoch [938/2000], Loss: 0.3037, Val Loss: 0.5096\n",
      "Epoch [939/2000], Loss: 0.3036, Val Loss: 0.5097\n",
      "Epoch [940/2000], Loss: 0.3036, Val Loss: 0.5099\n",
      "Epoch [941/2000], Loss: 0.3035, Val Loss: 0.5096\n",
      "Epoch [942/2000], Loss: 0.3035, Val Loss: 0.5096\n",
      "Epoch [943/2000], Loss: 0.3035, Val Loss: 0.5099\n",
      "Epoch [944/2000], Loss: 0.3034, Val Loss: 0.5099\n",
      "Epoch [945/2000], Loss: 0.3034, Val Loss: 0.5098\n",
      "Epoch [946/2000], Loss: 0.3033, Val Loss: 0.5096\n",
      "Epoch [947/2000], Loss: 0.3033, Val Loss: 0.5094\n",
      "Epoch [948/2000], Loss: 0.3032, Val Loss: 0.5096\n",
      "Epoch [949/2000], Loss: 0.3031, Val Loss: 0.5100\n",
      "Epoch [950/2000], Loss: 0.3031, Val Loss: 0.5096\n",
      "Epoch [951/2000], Loss: 0.3031, Val Loss: 0.5093\n",
      "Epoch [952/2000], Loss: 0.3030, Val Loss: 0.5095\n",
      "Epoch [953/2000], Loss: 0.3030, Val Loss: 0.5093\n",
      "Epoch [954/2000], Loss: 0.3029, Val Loss: 0.5091\n",
      "Epoch [955/2000], Loss: 0.3029, Val Loss: 0.5095\n",
      "Epoch [956/2000], Loss: 0.3028, Val Loss: 0.5093\n",
      "Epoch [957/2000], Loss: 0.3028, Val Loss: 0.5088\n",
      "Epoch [958/2000], Loss: 0.3027, Val Loss: 0.5089\n",
      "Epoch [959/2000], Loss: 0.3027, Val Loss: 0.5093\n",
      "Epoch [960/2000], Loss: 0.3026, Val Loss: 0.5091\n",
      "Epoch [961/2000], Loss: 0.3026, Val Loss: 0.5088\n",
      "Epoch [962/2000], Loss: 0.3025, Val Loss: 0.5088\n",
      "Epoch [963/2000], Loss: 0.3025, Val Loss: 0.5087\n",
      "Epoch [964/2000], Loss: 0.3024, Val Loss: 0.5092\n",
      "Epoch [965/2000], Loss: 0.3024, Val Loss: 0.5098\n",
      "Epoch [966/2000], Loss: 0.3023, Val Loss: 0.5094\n",
      "Epoch [967/2000], Loss: 0.3023, Val Loss: 0.5087\n",
      "Epoch [968/2000], Loss: 0.3023, Val Loss: 0.5085\n",
      "Epoch [969/2000], Loss: 0.3022, Val Loss: 0.5088\n",
      "Epoch [970/2000], Loss: 0.3022, Val Loss: 0.5090\n",
      "Epoch [971/2000], Loss: 0.3021, Val Loss: 0.5089\n",
      "Epoch [972/2000], Loss: 0.3021, Val Loss: 0.5089\n",
      "Epoch [973/2000], Loss: 0.3020, Val Loss: 0.5088\n",
      "Epoch [974/2000], Loss: 0.3020, Val Loss: 0.5089\n",
      "Epoch [975/2000], Loss: 0.3019, Val Loss: 0.5087\n",
      "Epoch [976/2000], Loss: 0.3019, Val Loss: 0.5087\n",
      "Epoch [977/2000], Loss: 0.3018, Val Loss: 0.5086\n",
      "Epoch [978/2000], Loss: 0.3018, Val Loss: 0.5085\n",
      "Epoch [979/2000], Loss: 0.3017, Val Loss: 0.5089\n",
      "Epoch [980/2000], Loss: 0.3017, Val Loss: 0.5087\n",
      "Epoch [981/2000], Loss: 0.3016, Val Loss: 0.5083\n",
      "Epoch [982/2000], Loss: 0.3016, Val Loss: 0.5080\n",
      "Epoch [983/2000], Loss: 0.3016, Val Loss: 0.5080\n",
      "Epoch [984/2000], Loss: 0.3015, Val Loss: 0.5083\n",
      "Epoch [985/2000], Loss: 0.3015, Val Loss: 0.5085\n",
      "Epoch [986/2000], Loss: 0.3014, Val Loss: 0.5083\n",
      "Epoch [987/2000], Loss: 0.3014, Val Loss: 0.5084\n",
      "Epoch [988/2000], Loss: 0.3013, Val Loss: 0.5082\n",
      "Epoch [989/2000], Loss: 0.3013, Val Loss: 0.5080\n",
      "Epoch [990/2000], Loss: 0.3012, Val Loss: 0.5082\n",
      "Epoch [991/2000], Loss: 0.3012, Val Loss: 0.5083\n",
      "Epoch [992/2000], Loss: 0.3012, Val Loss: 0.5079\n",
      "Epoch [993/2000], Loss: 0.3011, Val Loss: 0.5080\n",
      "Epoch [994/2000], Loss: 0.3011, Val Loss: 0.5079\n",
      "Epoch [995/2000], Loss: 0.3010, Val Loss: 0.5077\n",
      "Epoch [996/2000], Loss: 0.3010, Val Loss: 0.5083\n",
      "Epoch [997/2000], Loss: 0.3009, Val Loss: 0.5080\n",
      "Epoch [998/2000], Loss: 0.3009, Val Loss: 0.5077\n",
      "Epoch [999/2000], Loss: 0.3008, Val Loss: 0.5081\n",
      "Epoch [1000/2000], Loss: 0.3008, Val Loss: 0.5082\n",
      "Epoch [1001/2000], Loss: 0.3007, Val Loss: 0.5079\n",
      "Epoch [1002/2000], Loss: 0.3007, Val Loss: 0.5082\n",
      "Epoch [1003/2000], Loss: 0.3007, Val Loss: 0.5078\n",
      "Epoch [1004/2000], Loss: 0.3006, Val Loss: 0.5074\n",
      "Epoch [1005/2000], Loss: 0.3006, Val Loss: 0.5075\n",
      "Epoch [1006/2000], Loss: 0.3005, Val Loss: 0.5074\n",
      "Epoch [1007/2000], Loss: 0.3005, Val Loss: 0.5077\n",
      "Epoch [1008/2000], Loss: 0.3004, Val Loss: 0.5083\n",
      "Epoch [1009/2000], Loss: 0.3004, Val Loss: 0.5082\n",
      "Epoch [1010/2000], Loss: 0.3003, Val Loss: 0.5076\n",
      "Epoch [1011/2000], Loss: 0.3003, Val Loss: 0.5076\n",
      "Epoch [1012/2000], Loss: 0.3003, Val Loss: 0.5078\n",
      "Epoch [1013/2000], Loss: 0.3002, Val Loss: 0.5079\n",
      "Epoch [1014/2000], Loss: 0.3002, Val Loss: 0.5078\n",
      "Epoch [1015/2000], Loss: 0.3001, Val Loss: 0.5075\n",
      "Epoch [1016/2000], Loss: 0.3001, Val Loss: 0.5076\n",
      "Epoch [1017/2000], Loss: 0.3000, Val Loss: 0.5082\n",
      "Epoch [1018/2000], Loss: 0.3000, Val Loss: 0.5077\n",
      "Epoch [1019/2000], Loss: 0.3000, Val Loss: 0.5069\n",
      "Epoch [1020/2000], Loss: 0.2999, Val Loss: 0.5070\n",
      "Epoch [1021/2000], Loss: 0.2999, Val Loss: 0.5073\n",
      "Epoch [1022/2000], Loss: 0.2998, Val Loss: 0.5076\n",
      "Epoch [1023/2000], Loss: 0.2998, Val Loss: 0.5075\n",
      "Epoch [1024/2000], Loss: 0.2997, Val Loss: 0.5074\n",
      "Epoch [1025/2000], Loss: 0.2997, Val Loss: 0.5075\n",
      "Epoch [1026/2000], Loss: 0.2996, Val Loss: 0.5075\n",
      "Epoch [1027/2000], Loss: 0.2996, Val Loss: 0.5074\n",
      "Epoch [1028/2000], Loss: 0.2996, Val Loss: 0.5075\n",
      "Epoch [1029/2000], Loss: 0.2995, Val Loss: 0.5076\n",
      "Epoch [1030/2000], Loss: 0.2995, Val Loss: 0.5068\n",
      "Epoch [1031/2000], Loss: 0.2994, Val Loss: 0.5066\n",
      "Epoch [1032/2000], Loss: 0.2994, Val Loss: 0.5076\n",
      "Epoch [1033/2000], Loss: 0.2993, Val Loss: 0.5077\n",
      "Epoch [1034/2000], Loss: 0.2993, Val Loss: 0.5073\n",
      "Epoch [1035/2000], Loss: 0.2993, Val Loss: 0.5071\n",
      "Epoch [1036/2000], Loss: 0.2992, Val Loss: 0.5071\n",
      "Epoch [1037/2000], Loss: 0.2992, Val Loss: 0.5073\n",
      "Epoch [1038/2000], Loss: 0.2991, Val Loss: 0.5069\n",
      "Epoch [1039/2000], Loss: 0.2991, Val Loss: 0.5068\n",
      "Epoch [1040/2000], Loss: 0.2990, Val Loss: 0.5075\n",
      "Epoch [1041/2000], Loss: 0.2990, Val Loss: 0.5076\n",
      "Epoch [1042/2000], Loss: 0.2990, Val Loss: 0.5078\n",
      "Epoch [1043/2000], Loss: 0.2989, Val Loss: 0.5080\n",
      "Epoch [1044/2000], Loss: 0.2989, Val Loss: 0.5074\n",
      "Epoch [1045/2000], Loss: 0.2988, Val Loss: 0.5068\n",
      "Epoch [1046/2000], Loss: 0.2988, Val Loss: 0.5064\n",
      "Epoch [1047/2000], Loss: 0.2988, Val Loss: 0.5067\n",
      "Epoch [1048/2000], Loss: 0.2987, Val Loss: 0.5074\n",
      "Epoch [1049/2000], Loss: 0.2987, Val Loss: 0.5072\n",
      "Epoch [1050/2000], Loss: 0.2986, Val Loss: 0.5073\n",
      "Epoch [1051/2000], Loss: 0.2986, Val Loss: 0.5078\n",
      "Epoch [1052/2000], Loss: 0.2985, Val Loss: 0.5072\n",
      "Epoch [1053/2000], Loss: 0.2985, Val Loss: 0.5066\n",
      "Epoch [1054/2000], Loss: 0.2985, Val Loss: 0.5061\n",
      "Epoch [1055/2000], Loss: 0.2984, Val Loss: 0.5062\n",
      "Epoch [1056/2000], Loss: 0.2984, Val Loss: 0.5073\n",
      "Epoch [1057/2000], Loss: 0.2983, Val Loss: 0.5070\n",
      "Epoch [1058/2000], Loss: 0.2983, Val Loss: 0.5066\n",
      "Epoch [1059/2000], Loss: 0.2982, Val Loss: 0.5069\n",
      "Epoch [1060/2000], Loss: 0.2982, Val Loss: 0.5070\n",
      "Epoch [1061/2000], Loss: 0.2982, Val Loss: 0.5071\n",
      "Epoch [1062/2000], Loss: 0.2981, Val Loss: 0.5069\n",
      "Epoch [1063/2000], Loss: 0.2981, Val Loss: 0.5067\n",
      "Epoch [1064/2000], Loss: 0.2980, Val Loss: 0.5070\n",
      "Epoch [1065/2000], Loss: 0.2980, Val Loss: 0.5068\n",
      "Epoch [1066/2000], Loss: 0.2980, Val Loss: 0.5064\n",
      "Epoch [1067/2000], Loss: 0.2979, Val Loss: 0.5067\n",
      "Epoch [1068/2000], Loss: 0.2979, Val Loss: 0.5072\n",
      "Epoch [1069/2000], Loss: 0.2978, Val Loss: 0.5076\n",
      "Epoch [1070/2000], Loss: 0.2978, Val Loss: 0.5073\n",
      "Epoch [1071/2000], Loss: 0.2978, Val Loss: 0.5067\n",
      "Epoch [1072/2000], Loss: 0.2977, Val Loss: 0.5069\n",
      "Epoch [1073/2000], Loss: 0.2977, Val Loss: 0.5070\n",
      "Epoch [1074/2000], Loss: 0.2976, Val Loss: 0.5065\n",
      "Epoch [1075/2000], Loss: 0.2976, Val Loss: 0.5064\n",
      "Epoch [1076/2000], Loss: 0.2976, Val Loss: 0.5070\n",
      "Epoch [1077/2000], Loss: 0.2975, Val Loss: 0.5074\n",
      "Epoch [1078/2000], Loss: 0.2975, Val Loss: 0.5065\n",
      "Epoch [1079/2000], Loss: 0.2975, Val Loss: 0.5060\n",
      "Epoch [1080/2000], Loss: 0.2974, Val Loss: 0.5071\n",
      "Epoch [1081/2000], Loss: 0.2974, Val Loss: 0.5068\n",
      "Epoch [1082/2000], Loss: 0.2973, Val Loss: 0.5062\n",
      "Epoch [1083/2000], Loss: 0.2973, Val Loss: 0.5064\n",
      "Epoch [1084/2000], Loss: 0.2973, Val Loss: 0.5069\n",
      "Epoch [1085/2000], Loss: 0.2972, Val Loss: 0.5070\n",
      "Epoch [1086/2000], Loss: 0.2972, Val Loss: 0.5063\n",
      "Epoch [1087/2000], Loss: 0.2971, Val Loss: 0.5059\n",
      "Epoch [1088/2000], Loss: 0.2971, Val Loss: 0.5065\n",
      "Epoch [1089/2000], Loss: 0.2970, Val Loss: 0.5062\n",
      "Epoch [1090/2000], Loss: 0.2970, Val Loss: 0.5063\n",
      "Epoch [1091/2000], Loss: 0.2970, Val Loss: 0.5063\n",
      "Epoch [1092/2000], Loss: 0.2969, Val Loss: 0.5060\n",
      "Epoch [1093/2000], Loss: 0.2969, Val Loss: 0.5065\n",
      "Epoch [1094/2000], Loss: 0.2968, Val Loss: 0.5061\n",
      "Epoch [1095/2000], Loss: 0.2968, Val Loss: 0.5058\n",
      "Epoch [1096/2000], Loss: 0.2968, Val Loss: 0.5056\n",
      "Epoch [1097/2000], Loss: 0.2967, Val Loss: 0.5054\n",
      "Epoch [1098/2000], Loss: 0.2967, Val Loss: 0.5058\n",
      "Epoch [1099/2000], Loss: 0.2966, Val Loss: 0.5062\n",
      "Epoch [1100/2000], Loss: 0.2966, Val Loss: 0.5060\n",
      "Epoch [1101/2000], Loss: 0.2966, Val Loss: 0.5057\n",
      "Epoch [1102/2000], Loss: 0.2965, Val Loss: 0.5052\n",
      "Epoch [1103/2000], Loss: 0.2965, Val Loss: 0.5058\n",
      "Epoch [1104/2000], Loss: 0.2964, Val Loss: 0.5059\n",
      "Epoch [1105/2000], Loss: 0.2964, Val Loss: 0.5058\n",
      "Epoch [1106/2000], Loss: 0.2964, Val Loss: 0.5059\n",
      "Epoch [1107/2000], Loss: 0.2963, Val Loss: 0.5062\n",
      "Epoch [1108/2000], Loss: 0.2963, Val Loss: 0.5069\n",
      "Epoch [1109/2000], Loss: 0.2963, Val Loss: 0.5066\n",
      "Epoch [1110/2000], Loss: 0.2962, Val Loss: 0.5060\n",
      "Epoch [1111/2000], Loss: 0.2962, Val Loss: 0.5059\n",
      "Epoch [1112/2000], Loss: 0.2961, Val Loss: 0.5056\n",
      "Epoch [1113/2000], Loss: 0.2961, Val Loss: 0.5061\n",
      "Epoch [1114/2000], Loss: 0.2961, Val Loss: 0.5064\n",
      "Epoch [1115/2000], Loss: 0.2960, Val Loss: 0.5064\n",
      "Epoch [1116/2000], Loss: 0.2960, Val Loss: 0.5071\n",
      "Epoch [1117/2000], Loss: 0.2959, Val Loss: 0.5063\n",
      "Epoch [1118/2000], Loss: 0.2959, Val Loss: 0.5055\n",
      "Epoch [1119/2000], Loss: 0.2959, Val Loss: 0.5055\n",
      "Epoch [1120/2000], Loss: 0.2958, Val Loss: 0.5063\n",
      "Epoch [1121/2000], Loss: 0.2958, Val Loss: 0.5074\n",
      "Epoch [1122/2000], Loss: 0.2958, Val Loss: 0.5067\n",
      "Epoch [1123/2000], Loss: 0.2957, Val Loss: 0.5063\n",
      "Epoch [1124/2000], Loss: 0.2957, Val Loss: 0.5067\n",
      "Epoch [1125/2000], Loss: 0.2957, Val Loss: 0.5062\n",
      "Epoch [1126/2000], Loss: 0.2956, Val Loss: 0.5068\n",
      "Epoch [1127/2000], Loss: 0.2956, Val Loss: 0.5071\n",
      "Epoch [1128/2000], Loss: 0.2955, Val Loss: 0.5065\n",
      "Epoch [1129/2000], Loss: 0.2955, Val Loss: 0.5065\n",
      "Epoch [1130/2000], Loss: 0.2955, Val Loss: 0.5069\n",
      "Epoch [1131/2000], Loss: 0.2954, Val Loss: 0.5075\n",
      "Epoch [1132/2000], Loss: 0.2954, Val Loss: 0.5072\n",
      "Epoch [1133/2000], Loss: 0.2953, Val Loss: 0.5074\n",
      "Epoch [1134/2000], Loss: 0.2953, Val Loss: 0.5075\n",
      "Epoch [1135/2000], Loss: 0.2953, Val Loss: 0.5070\n",
      "Epoch [1136/2000], Loss: 0.2952, Val Loss: 0.5072\n",
      "Epoch [1137/2000], Loss: 0.2952, Val Loss: 0.5073\n",
      "Epoch [1138/2000], Loss: 0.2952, Val Loss: 0.5075\n",
      "Epoch [1139/2000], Loss: 0.2951, Val Loss: 0.5082\n",
      "Epoch [1140/2000], Loss: 0.2951, Val Loss: 0.5080\n",
      "Epoch [1141/2000], Loss: 0.2950, Val Loss: 0.5082\n",
      "Epoch [1142/2000], Loss: 0.2950, Val Loss: 0.5076\n",
      "Epoch [1143/2000], Loss: 0.2950, Val Loss: 0.5070\n",
      "Epoch [1144/2000], Loss: 0.2950, Val Loss: 0.5073\n",
      "Epoch [1145/2000], Loss: 0.2949, Val Loss: 0.5067\n",
      "Epoch [1146/2000], Loss: 0.2948, Val Loss: 0.5069\n",
      "Epoch [1147/2000], Loss: 0.2948, Val Loss: 0.5080\n",
      "Epoch [1148/2000], Loss: 0.2948, Val Loss: 0.5076\n",
      "Epoch [1149/2000], Loss: 0.2947, Val Loss: 0.5069\n",
      "Epoch [1150/2000], Loss: 0.2947, Val Loss: 0.5073\n",
      "Epoch [1151/2000], Loss: 0.2947, Val Loss: 0.5082\n",
      "Epoch [1152/2000], Loss: 0.2946, Val Loss: 0.5078\n",
      "Epoch [1153/2000], Loss: 0.2946, Val Loss: 0.5075\n",
      "Epoch [1154/2000], Loss: 0.2946, Val Loss: 0.5079\n",
      "Epoch [1155/2000], Loss: 0.2945, Val Loss: 0.5073\n",
      "Epoch [1156/2000], Loss: 0.2945, Val Loss: 0.5071\n",
      "Epoch [1157/2000], Loss: 0.2945, Val Loss: 0.5077\n",
      "Epoch [1158/2000], Loss: 0.2944, Val Loss: 0.5073\n",
      "Epoch [1159/2000], Loss: 0.2944, Val Loss: 0.5072\n",
      "Epoch [1160/2000], Loss: 0.2944, Val Loss: 0.5072\n",
      "Epoch [1161/2000], Loss: 0.2943, Val Loss: 0.5075\n",
      "Epoch [1162/2000], Loss: 0.2943, Val Loss: 0.5069\n",
      "Epoch [1163/2000], Loss: 0.2942, Val Loss: 0.5069\n",
      "Epoch [1164/2000], Loss: 0.2942, Val Loss: 0.5080\n",
      "Epoch [1165/2000], Loss: 0.2942, Val Loss: 0.5072\n",
      "Epoch [1166/2000], Loss: 0.2941, Val Loss: 0.5066\n",
      "Epoch [1167/2000], Loss: 0.2941, Val Loss: 0.5068\n",
      "Epoch [1168/2000], Loss: 0.2940, Val Loss: 0.5066\n",
      "Epoch [1169/2000], Loss: 0.2940, Val Loss: 0.5072\n",
      "Epoch [1170/2000], Loss: 0.2940, Val Loss: 0.5074\n",
      "Epoch [1171/2000], Loss: 0.2939, Val Loss: 0.5073\n",
      "Epoch [1172/2000], Loss: 0.2939, Val Loss: 0.5073\n",
      "Epoch [1173/2000], Loss: 0.2939, Val Loss: 0.5069\n",
      "Epoch [1174/2000], Loss: 0.2938, Val Loss: 0.5066\n",
      "Epoch [1175/2000], Loss: 0.2938, Val Loss: 0.5065\n",
      "Epoch [1176/2000], Loss: 0.2938, Val Loss: 0.5061\n",
      "Epoch [1177/2000], Loss: 0.2937, Val Loss: 0.5065\n",
      "Epoch [1178/2000], Loss: 0.2937, Val Loss: 0.5070\n",
      "Epoch [1179/2000], Loss: 0.2937, Val Loss: 0.5064\n",
      "Epoch [1180/2000], Loss: 0.2936, Val Loss: 0.5063\n",
      "Epoch [1181/2000], Loss: 0.2936, Val Loss: 0.5072\n",
      "Epoch [1182/2000], Loss: 0.2935, Val Loss: 0.5065\n",
      "Epoch [1183/2000], Loss: 0.2935, Val Loss: 0.5061\n",
      "Epoch [1184/2000], Loss: 0.2935, Val Loss: 0.5069\n",
      "Epoch [1185/2000], Loss: 0.2934, Val Loss: 0.5066\n",
      "Epoch [1186/2000], Loss: 0.2934, Val Loss: 0.5056\n",
      "Epoch [1187/2000], Loss: 0.2934, Val Loss: 0.5057\n",
      "Epoch [1188/2000], Loss: 0.2933, Val Loss: 0.5064\n",
      "Epoch [1189/2000], Loss: 0.2933, Val Loss: 0.5064\n",
      "Epoch [1190/2000], Loss: 0.2932, Val Loss: 0.5058\n",
      "Epoch [1191/2000], Loss: 0.2932, Val Loss: 0.5063\n",
      "Epoch [1192/2000], Loss: 0.2932, Val Loss: 0.5065\n",
      "Epoch [1193/2000], Loss: 0.2931, Val Loss: 0.5056\n",
      "Epoch [1194/2000], Loss: 0.2931, Val Loss: 0.5056\n",
      "Epoch [1195/2000], Loss: 0.2931, Val Loss: 0.5071\n",
      "Epoch [1196/2000], Loss: 0.2931, Val Loss: 0.5069\n",
      "Epoch [1197/2000], Loss: 0.2930, Val Loss: 0.5069\n",
      "Epoch [1198/2000], Loss: 0.2930, Val Loss: 0.5066\n",
      "Epoch [1199/2000], Loss: 0.2929, Val Loss: 0.5056\n",
      "Epoch [1200/2000], Loss: 0.2929, Val Loss: 0.5060\n",
      "Epoch [1201/2000], Loss: 0.2929, Val Loss: 0.5059\n",
      "Epoch [1202/2000], Loss: 0.2928, Val Loss: 0.5059\n",
      "Early stopping triggered\n",
      "Epoch [1/2000], Loss: 1.0192, Val Loss: 1.6778\n",
      "Epoch [2/2000], Loss: 1.0092, Val Loss: 1.6663\n",
      "Epoch [3/2000], Loss: 0.9998, Val Loss: 1.6553\n",
      "Epoch [4/2000], Loss: 0.9908, Val Loss: 1.6446\n",
      "Epoch [5/2000], Loss: 0.9821, Val Loss: 1.6336\n",
      "Epoch [6/2000], Loss: 0.9734, Val Loss: 1.6223\n",
      "Epoch [7/2000], Loss: 0.9645, Val Loss: 1.6106\n",
      "Epoch [8/2000], Loss: 0.9553, Val Loss: 1.5984\n",
      "Epoch [9/2000], Loss: 0.9457, Val Loss: 1.5855\n",
      "Epoch [10/2000], Loss: 0.9357, Val Loss: 1.5719\n",
      "Epoch [11/2000], Loss: 0.9251, Val Loss: 1.5575\n",
      "Epoch [12/2000], Loss: 0.9140, Val Loss: 1.5419\n",
      "Epoch [13/2000], Loss: 0.9024, Val Loss: 1.5256\n",
      "Epoch [14/2000], Loss: 0.8903, Val Loss: 1.5086\n",
      "Epoch [15/2000], Loss: 0.8777, Val Loss: 1.4909\n",
      "Epoch [16/2000], Loss: 0.8647, Val Loss: 1.4726\n",
      "Epoch [17/2000], Loss: 0.8514, Val Loss: 1.4538\n",
      "Epoch [18/2000], Loss: 0.8380, Val Loss: 1.4348\n",
      "Epoch [19/2000], Loss: 0.8245, Val Loss: 1.4157\n",
      "Epoch [20/2000], Loss: 0.8112, Val Loss: 1.3969\n",
      "Epoch [21/2000], Loss: 0.7982, Val Loss: 1.3788\n",
      "Epoch [22/2000], Loss: 0.7855, Val Loss: 1.3616\n",
      "Epoch [23/2000], Loss: 0.7735, Val Loss: 1.3458\n",
      "Epoch [24/2000], Loss: 0.7623, Val Loss: 1.3318\n",
      "Epoch [25/2000], Loss: 0.7518, Val Loss: 1.3197\n",
      "Epoch [26/2000], Loss: 0.7423, Val Loss: 1.3096\n",
      "Epoch [27/2000], Loss: 0.7336, Val Loss: 1.3016\n",
      "Epoch [28/2000], Loss: 0.7257, Val Loss: 1.2954\n",
      "Epoch [29/2000], Loss: 0.7185, Val Loss: 1.2908\n",
      "Epoch [30/2000], Loss: 0.7119, Val Loss: 1.2874\n",
      "Epoch [31/2000], Loss: 0.7057, Val Loss: 1.2848\n",
      "Epoch [32/2000], Loss: 0.6996, Val Loss: 1.2827\n",
      "Epoch [33/2000], Loss: 0.6937, Val Loss: 1.2809\n",
      "Epoch [34/2000], Loss: 0.6878, Val Loss: 1.2791\n",
      "Epoch [35/2000], Loss: 0.6818, Val Loss: 1.2772\n",
      "Epoch [36/2000], Loss: 0.6759, Val Loss: 1.2752\n",
      "Epoch [37/2000], Loss: 0.6699, Val Loss: 1.2734\n",
      "Epoch [38/2000], Loss: 0.6641, Val Loss: 1.2719\n",
      "Epoch [39/2000], Loss: 0.6585, Val Loss: 1.2709\n",
      "Epoch [40/2000], Loss: 0.6532, Val Loss: 1.2704\n",
      "Epoch [41/2000], Loss: 0.6483, Val Loss: 1.2705\n",
      "Epoch [42/2000], Loss: 0.6440, Val Loss: 1.2714\n",
      "Epoch [43/2000], Loss: 0.6401, Val Loss: 1.2729\n",
      "Epoch [44/2000], Loss: 0.6367, Val Loss: 1.2750\n",
      "Epoch [45/2000], Loss: 0.6337, Val Loss: 1.2775\n",
      "Epoch [46/2000], Loss: 0.6312, Val Loss: 1.2802\n",
      "Epoch [47/2000], Loss: 0.6289, Val Loss: 1.2829\n",
      "Epoch [48/2000], Loss: 0.6269, Val Loss: 1.2855\n",
      "Epoch [49/2000], Loss: 0.6250, Val Loss: 1.2878\n",
      "Epoch [50/2000], Loss: 0.6232, Val Loss: 1.2897\n",
      "Epoch [51/2000], Loss: 0.6214, Val Loss: 1.2912\n",
      "Epoch [52/2000], Loss: 0.6195, Val Loss: 1.2923\n",
      "Epoch [53/2000], Loss: 0.6177, Val Loss: 1.2931\n",
      "Epoch [54/2000], Loss: 0.6158, Val Loss: 1.2936\n",
      "Epoch [55/2000], Loss: 0.6139, Val Loss: 1.2938\n",
      "Epoch [56/2000], Loss: 0.6120, Val Loss: 1.2939\n",
      "Epoch [57/2000], Loss: 0.6101, Val Loss: 1.2938\n",
      "Epoch [58/2000], Loss: 0.6082, Val Loss: 1.2937\n",
      "Epoch [59/2000], Loss: 0.6064, Val Loss: 1.2936\n",
      "Epoch [60/2000], Loss: 0.6047, Val Loss: 1.2935\n",
      "Epoch [61/2000], Loss: 0.6030, Val Loss: 1.2934\n",
      "Epoch [62/2000], Loss: 0.6014, Val Loss: 1.2934\n",
      "Epoch [63/2000], Loss: 0.5998, Val Loss: 1.2935\n",
      "Epoch [64/2000], Loss: 0.5984, Val Loss: 1.2936\n",
      "Epoch [65/2000], Loss: 0.5969, Val Loss: 1.2938\n",
      "Epoch [66/2000], Loss: 0.5956, Val Loss: 1.2940\n",
      "Epoch [67/2000], Loss: 0.5942, Val Loss: 1.2942\n",
      "Epoch [68/2000], Loss: 0.5929, Val Loss: 1.2944\n",
      "Epoch [69/2000], Loss: 0.5917, Val Loss: 1.2946\n",
      "Epoch [70/2000], Loss: 0.5905, Val Loss: 1.2948\n",
      "Epoch [71/2000], Loss: 0.5893, Val Loss: 1.2951\n",
      "Epoch [72/2000], Loss: 0.5882, Val Loss: 1.2954\n",
      "Epoch [73/2000], Loss: 0.5871, Val Loss: 1.2957\n",
      "Epoch [74/2000], Loss: 0.5861, Val Loss: 1.2960\n",
      "Epoch [75/2000], Loss: 0.5850, Val Loss: 1.2963\n",
      "Epoch [76/2000], Loss: 0.5841, Val Loss: 1.2966\n",
      "Epoch [77/2000], Loss: 0.5831, Val Loss: 1.2969\n",
      "Epoch [78/2000], Loss: 0.5822, Val Loss: 1.2972\n",
      "Epoch [79/2000], Loss: 0.5814, Val Loss: 1.2975\n",
      "Epoch [80/2000], Loss: 0.5805, Val Loss: 1.2978\n",
      "Epoch [81/2000], Loss: 0.5797, Val Loss: 1.2980\n",
      "Epoch [82/2000], Loss: 0.5789, Val Loss: 1.2981\n",
      "Epoch [83/2000], Loss: 0.5781, Val Loss: 1.2983\n",
      "Epoch [84/2000], Loss: 0.5773, Val Loss: 1.2985\n",
      "Epoch [85/2000], Loss: 0.5765, Val Loss: 1.2988\n",
      "Epoch [86/2000], Loss: 0.5758, Val Loss: 1.2990\n",
      "Epoch [87/2000], Loss: 0.5751, Val Loss: 1.2992\n",
      "Epoch [88/2000], Loss: 0.5744, Val Loss: 1.2994\n",
      "Epoch [89/2000], Loss: 0.5737, Val Loss: 1.2996\n",
      "Epoch [90/2000], Loss: 0.5730, Val Loss: 1.2998\n",
      "Epoch [91/2000], Loss: 0.5724, Val Loss: 1.3000\n",
      "Epoch [92/2000], Loss: 0.5717, Val Loss: 1.3001\n",
      "Epoch [93/2000], Loss: 0.5711, Val Loss: 1.3003\n",
      "Epoch [94/2000], Loss: 0.5705, Val Loss: 1.3004\n",
      "Epoch [95/2000], Loss: 0.5699, Val Loss: 1.3006\n",
      "Epoch [96/2000], Loss: 0.5694, Val Loss: 1.3007\n",
      "Epoch [97/2000], Loss: 0.5688, Val Loss: 1.3008\n",
      "Epoch [98/2000], Loss: 0.5682, Val Loss: 1.3009\n",
      "Epoch [99/2000], Loss: 0.5677, Val Loss: 1.3010\n",
      "Epoch [100/2000], Loss: 0.5672, Val Loss: 1.3010\n",
      "Epoch [101/2000], Loss: 0.5666, Val Loss: 1.3010\n",
      "Epoch [102/2000], Loss: 0.5661, Val Loss: 1.3009\n",
      "Epoch [103/2000], Loss: 0.5656, Val Loss: 1.3009\n",
      "Epoch [104/2000], Loss: 0.5651, Val Loss: 1.3008\n",
      "Epoch [105/2000], Loss: 0.5646, Val Loss: 1.3007\n",
      "Epoch [106/2000], Loss: 0.5641, Val Loss: 1.3005\n",
      "Epoch [107/2000], Loss: 0.5637, Val Loss: 1.3003\n",
      "Epoch [108/2000], Loss: 0.5632, Val Loss: 1.3001\n",
      "Epoch [109/2000], Loss: 0.5627, Val Loss: 1.2999\n",
      "Epoch [110/2000], Loss: 0.5623, Val Loss: 1.2997\n",
      "Epoch [111/2000], Loss: 0.5618, Val Loss: 1.2994\n",
      "Epoch [112/2000], Loss: 0.5614, Val Loss: 1.2992\n",
      "Epoch [113/2000], Loss: 0.5609, Val Loss: 1.2990\n",
      "Epoch [114/2000], Loss: 0.5605, Val Loss: 1.2988\n",
      "Epoch [115/2000], Loss: 0.5601, Val Loss: 1.2986\n",
      "Epoch [116/2000], Loss: 0.5597, Val Loss: 1.2984\n",
      "Epoch [117/2000], Loss: 0.5593, Val Loss: 1.2983\n",
      "Epoch [118/2000], Loss: 0.5589, Val Loss: 1.2982\n",
      "Epoch [119/2000], Loss: 0.5585, Val Loss: 1.2982\n",
      "Epoch [120/2000], Loss: 0.5581, Val Loss: 1.2981\n",
      "Epoch [121/2000], Loss: 0.5577, Val Loss: 1.2981\n",
      "Epoch [122/2000], Loss: 0.5573, Val Loss: 1.2980\n",
      "Epoch [123/2000], Loss: 0.5569, Val Loss: 1.2980\n",
      "Epoch [124/2000], Loss: 0.5565, Val Loss: 1.2979\n",
      "Epoch [125/2000], Loss: 0.5561, Val Loss: 1.2978\n",
      "Epoch [126/2000], Loss: 0.5558, Val Loss: 1.2976\n",
      "Epoch [127/2000], Loss: 0.5554, Val Loss: 1.2975\n",
      "Epoch [128/2000], Loss: 0.5550, Val Loss: 1.2973\n",
      "Epoch [129/2000], Loss: 0.5547, Val Loss: 1.2971\n",
      "Epoch [130/2000], Loss: 0.5543, Val Loss: 1.2969\n",
      "Epoch [131/2000], Loss: 0.5539, Val Loss: 1.2967\n",
      "Epoch [132/2000], Loss: 0.5536, Val Loss: 1.2965\n",
      "Epoch [133/2000], Loss: 0.5532, Val Loss: 1.2964\n",
      "Epoch [134/2000], Loss: 0.5529, Val Loss: 1.2962\n",
      "Epoch [135/2000], Loss: 0.5526, Val Loss: 1.2960\n",
      "Epoch [136/2000], Loss: 0.5522, Val Loss: 1.2958\n",
      "Epoch [137/2000], Loss: 0.5519, Val Loss: 1.2957\n",
      "Epoch [138/2000], Loss: 0.5515, Val Loss: 1.2955\n",
      "Epoch [139/2000], Loss: 0.5512, Val Loss: 1.2953\n",
      "Early stopping triggered\n",
      "Epoch [1/2000], Loss: 1.0423, Val Loss: 0.6777\n",
      "Epoch [2/2000], Loss: 1.0270, Val Loss: 0.6685\n",
      "Epoch [3/2000], Loss: 1.0134, Val Loss: 0.6586\n",
      "Epoch [4/2000], Loss: 1.0007, Val Loss: 0.6479\n",
      "Epoch [5/2000], Loss: 0.9880, Val Loss: 0.6363\n",
      "Epoch [6/2000], Loss: 0.9747, Val Loss: 0.6236\n",
      "Epoch [7/2000], Loss: 0.9602, Val Loss: 0.6100\n",
      "Epoch [8/2000], Loss: 0.9444, Val Loss: 0.5954\n",
      "Epoch [9/2000], Loss: 0.9268, Val Loss: 0.5799\n",
      "Epoch [10/2000], Loss: 0.9075, Val Loss: 0.5635\n",
      "Epoch [11/2000], Loss: 0.8862, Val Loss: 0.5459\n",
      "Epoch [12/2000], Loss: 0.8629, Val Loss: 0.5270\n",
      "Epoch [13/2000], Loss: 0.8379, Val Loss: 0.5070\n",
      "Epoch [14/2000], Loss: 0.8111, Val Loss: 0.4864\n",
      "Epoch [15/2000], Loss: 0.7831, Val Loss: 0.4653\n",
      "Epoch [16/2000], Loss: 0.7541, Val Loss: 0.4439\n",
      "Epoch [17/2000], Loss: 0.7247, Val Loss: 0.4222\n",
      "Epoch [18/2000], Loss: 0.6953, Val Loss: 0.4005\n",
      "Epoch [19/2000], Loss: 0.6666, Val Loss: 0.3790\n",
      "Epoch [20/2000], Loss: 0.6395, Val Loss: 0.3582\n",
      "Epoch [21/2000], Loss: 0.6146, Val Loss: 0.3387\n",
      "Epoch [22/2000], Loss: 0.5926, Val Loss: 0.3209\n",
      "Epoch [23/2000], Loss: 0.5740, Val Loss: 0.3052\n",
      "Epoch [24/2000], Loss: 0.5590, Val Loss: 0.2919\n",
      "Epoch [25/2000], Loss: 0.5472, Val Loss: 0.2811\n",
      "Epoch [26/2000], Loss: 0.5378, Val Loss: 0.2726\n",
      "Epoch [27/2000], Loss: 0.5298, Val Loss: 0.2660\n",
      "Epoch [28/2000], Loss: 0.5220, Val Loss: 0.2606\n",
      "Epoch [29/2000], Loss: 0.5136, Val Loss: 0.2557\n",
      "Epoch [30/2000], Loss: 0.5043, Val Loss: 0.2508\n",
      "Epoch [31/2000], Loss: 0.4940, Val Loss: 0.2453\n",
      "Epoch [32/2000], Loss: 0.4828, Val Loss: 0.2389\n",
      "Epoch [33/2000], Loss: 0.4711, Val Loss: 0.2319\n",
      "Epoch [34/2000], Loss: 0.4592, Val Loss: 0.2243\n",
      "Epoch [35/2000], Loss: 0.4473, Val Loss: 0.2167\n",
      "Epoch [36/2000], Loss: 0.4356, Val Loss: 0.2092\n",
      "Epoch [37/2000], Loss: 0.4241, Val Loss: 0.2023\n",
      "Epoch [38/2000], Loss: 0.4128, Val Loss: 0.1963\n",
      "Epoch [39/2000], Loss: 0.4016, Val Loss: 0.1910\n",
      "Epoch [40/2000], Loss: 0.3904, Val Loss: 0.1864\n",
      "Epoch [41/2000], Loss: 0.3793, Val Loss: 0.1823\n",
      "Epoch [42/2000], Loss: 0.3683, Val Loss: 0.1786\n",
      "Epoch [43/2000], Loss: 0.3574, Val Loss: 0.1750\n",
      "Epoch [44/2000], Loss: 0.3469, Val Loss: 0.1715\n",
      "Epoch [45/2000], Loss: 0.3368, Val Loss: 0.1681\n",
      "Epoch [46/2000], Loss: 0.3274, Val Loss: 0.1646\n",
      "Epoch [47/2000], Loss: 0.3188, Val Loss: 0.1613\n",
      "Epoch [48/2000], Loss: 0.3111, Val Loss: 0.1581\n",
      "Epoch [49/2000], Loss: 0.3044, Val Loss: 0.1552\n",
      "Epoch [50/2000], Loss: 0.2987, Val Loss: 0.1524\n",
      "Epoch [51/2000], Loss: 0.2940, Val Loss: 0.1500\n",
      "Epoch [52/2000], Loss: 0.2903, Val Loss: 0.1477\n",
      "Epoch [53/2000], Loss: 0.2872, Val Loss: 0.1456\n",
      "Epoch [54/2000], Loss: 0.2848, Val Loss: 0.1434\n",
      "Epoch [55/2000], Loss: 0.2826, Val Loss: 0.1410\n",
      "Epoch [56/2000], Loss: 0.2806, Val Loss: 0.1385\n",
      "Epoch [57/2000], Loss: 0.2785, Val Loss: 0.1358\n",
      "Epoch [58/2000], Loss: 0.2763, Val Loss: 0.1331\n",
      "Epoch [59/2000], Loss: 0.2740, Val Loss: 0.1304\n",
      "Epoch [60/2000], Loss: 0.2717, Val Loss: 0.1279\n",
      "Epoch [61/2000], Loss: 0.2693, Val Loss: 0.1258\n",
      "Epoch [62/2000], Loss: 0.2671, Val Loss: 0.1240\n",
      "Epoch [63/2000], Loss: 0.2651, Val Loss: 0.1226\n",
      "Epoch [64/2000], Loss: 0.2634, Val Loss: 0.1214\n",
      "Epoch [65/2000], Loss: 0.2618, Val Loss: 0.1204\n",
      "Epoch [66/2000], Loss: 0.2605, Val Loss: 0.1194\n",
      "Epoch [67/2000], Loss: 0.2593, Val Loss: 0.1185\n",
      "Epoch [68/2000], Loss: 0.2582, Val Loss: 0.1176\n",
      "Epoch [69/2000], Loss: 0.2572, Val Loss: 0.1166\n",
      "Epoch [70/2000], Loss: 0.2562, Val Loss: 0.1155\n",
      "Epoch [71/2000], Loss: 0.2552, Val Loss: 0.1144\n",
      "Epoch [72/2000], Loss: 0.2542, Val Loss: 0.1133\n",
      "Epoch [73/2000], Loss: 0.2532, Val Loss: 0.1123\n",
      "Epoch [74/2000], Loss: 0.2522, Val Loss: 0.1113\n",
      "Epoch [75/2000], Loss: 0.2512, Val Loss: 0.1105\n",
      "Epoch [76/2000], Loss: 0.2501, Val Loss: 0.1097\n",
      "Epoch [77/2000], Loss: 0.2491, Val Loss: 0.1090\n",
      "Epoch [78/2000], Loss: 0.2482, Val Loss: 0.1085\n",
      "Epoch [79/2000], Loss: 0.2472, Val Loss: 0.1080\n",
      "Epoch [80/2000], Loss: 0.2463, Val Loss: 0.1075\n",
      "Epoch [81/2000], Loss: 0.2455, Val Loss: 0.1072\n",
      "Epoch [82/2000], Loss: 0.2447, Val Loss: 0.1068\n",
      "Epoch [83/2000], Loss: 0.2439, Val Loss: 0.1065\n",
      "Epoch [84/2000], Loss: 0.2432, Val Loss: 0.1062\n",
      "Epoch [85/2000], Loss: 0.2425, Val Loss: 0.1059\n",
      "Epoch [86/2000], Loss: 0.2419, Val Loss: 0.1056\n",
      "Epoch [87/2000], Loss: 0.2412, Val Loss: 0.1053\n",
      "Epoch [88/2000], Loss: 0.2406, Val Loss: 0.1050\n",
      "Epoch [89/2000], Loss: 0.2399, Val Loss: 0.1047\n",
      "Epoch [90/2000], Loss: 0.2393, Val Loss: 0.1044\n",
      "Epoch [91/2000], Loss: 0.2387, Val Loss: 0.1041\n",
      "Epoch [92/2000], Loss: 0.2380, Val Loss: 0.1038\n",
      "Epoch [93/2000], Loss: 0.2374, Val Loss: 0.1036\n",
      "Epoch [94/2000], Loss: 0.2367, Val Loss: 0.1033\n",
      "Epoch [95/2000], Loss: 0.2361, Val Loss: 0.1030\n",
      "Epoch [96/2000], Loss: 0.2355, Val Loss: 0.1028\n",
      "Epoch [97/2000], Loss: 0.2349, Val Loss: 0.1025\n",
      "Epoch [98/2000], Loss: 0.2343, Val Loss: 0.1023\n",
      "Epoch [99/2000], Loss: 0.2337, Val Loss: 0.1021\n",
      "Epoch [100/2000], Loss: 0.2331, Val Loss: 0.1019\n",
      "Epoch [101/2000], Loss: 0.2326, Val Loss: 0.1016\n",
      "Epoch [102/2000], Loss: 0.2320, Val Loss: 0.1014\n",
      "Epoch [103/2000], Loss: 0.2314, Val Loss: 0.1012\n",
      "Epoch [104/2000], Loss: 0.2309, Val Loss: 0.1010\n",
      "Epoch [105/2000], Loss: 0.2303, Val Loss: 0.1008\n",
      "Epoch [106/2000], Loss: 0.2298, Val Loss: 0.1006\n",
      "Epoch [107/2000], Loss: 0.2292, Val Loss: 0.1004\n",
      "Epoch [108/2000], Loss: 0.2287, Val Loss: 0.1002\n",
      "Epoch [109/2000], Loss: 0.2282, Val Loss: 0.1000\n",
      "Epoch [110/2000], Loss: 0.2276, Val Loss: 0.0999\n",
      "Epoch [111/2000], Loss: 0.2271, Val Loss: 0.0997\n",
      "Epoch [112/2000], Loss: 0.2266, Val Loss: 0.0996\n",
      "Epoch [113/2000], Loss: 0.2261, Val Loss: 0.0995\n",
      "Epoch [114/2000], Loss: 0.2256, Val Loss: 0.0994\n",
      "Epoch [115/2000], Loss: 0.2252, Val Loss: 0.0993\n",
      "Epoch [116/2000], Loss: 0.2247, Val Loss: 0.0992\n",
      "Epoch [117/2000], Loss: 0.2242, Val Loss: 0.0991\n",
      "Epoch [118/2000], Loss: 0.2238, Val Loss: 0.0991\n",
      "Epoch [119/2000], Loss: 0.2234, Val Loss: 0.0990\n",
      "Epoch [120/2000], Loss: 0.2229, Val Loss: 0.0989\n",
      "Epoch [121/2000], Loss: 0.2225, Val Loss: 0.0989\n",
      "Epoch [122/2000], Loss: 0.2221, Val Loss: 0.0988\n",
      "Epoch [123/2000], Loss: 0.2217, Val Loss: 0.0988\n",
      "Epoch [124/2000], Loss: 0.2213, Val Loss: 0.0987\n",
      "Epoch [125/2000], Loss: 0.2209, Val Loss: 0.0987\n",
      "Epoch [126/2000], Loss: 0.2205, Val Loss: 0.0986\n",
      "Epoch [127/2000], Loss: 0.2201, Val Loss: 0.0986\n",
      "Epoch [128/2000], Loss: 0.2197, Val Loss: 0.0985\n",
      "Epoch [129/2000], Loss: 0.2194, Val Loss: 0.0985\n",
      "Epoch [130/2000], Loss: 0.2190, Val Loss: 0.0985\n",
      "Epoch [131/2000], Loss: 0.2187, Val Loss: 0.0984\n",
      "Epoch [132/2000], Loss: 0.2183, Val Loss: 0.0984\n",
      "Epoch [133/2000], Loss: 0.2180, Val Loss: 0.0984\n",
      "Epoch [134/2000], Loss: 0.2176, Val Loss: 0.0984\n",
      "Epoch [135/2000], Loss: 0.2173, Val Loss: 0.0984\n",
      "Epoch [136/2000], Loss: 0.2170, Val Loss: 0.0984\n",
      "Epoch [137/2000], Loss: 0.2167, Val Loss: 0.0983\n",
      "Epoch [138/2000], Loss: 0.2164, Val Loss: 0.0983\n",
      "Epoch [139/2000], Loss: 0.2161, Val Loss: 0.0983\n",
      "Epoch [140/2000], Loss: 0.2158, Val Loss: 0.0983\n",
      "Epoch [141/2000], Loss: 0.2155, Val Loss: 0.0983\n",
      "Epoch [142/2000], Loss: 0.2153, Val Loss: 0.0983\n",
      "Epoch [143/2000], Loss: 0.2150, Val Loss: 0.0983\n",
      "Epoch [144/2000], Loss: 0.2147, Val Loss: 0.0983\n",
      "Epoch [145/2000], Loss: 0.2145, Val Loss: 0.0983\n",
      "Epoch [146/2000], Loss: 0.2142, Val Loss: 0.0983\n",
      "Epoch [147/2000], Loss: 0.2139, Val Loss: 0.0982\n",
      "Epoch [148/2000], Loss: 0.2137, Val Loss: 0.0982\n",
      "Epoch [149/2000], Loss: 0.2135, Val Loss: 0.0982\n",
      "Epoch [150/2000], Loss: 0.2132, Val Loss: 0.0982\n",
      "Epoch [151/2000], Loss: 0.2130, Val Loss: 0.0982\n",
      "Epoch [152/2000], Loss: 0.2128, Val Loss: 0.0982\n",
      "Epoch [153/2000], Loss: 0.2125, Val Loss: 0.0982\n",
      "Epoch [154/2000], Loss: 0.2123, Val Loss: 0.0982\n",
      "Epoch [155/2000], Loss: 0.2121, Val Loss: 0.0982\n",
      "Epoch [156/2000], Loss: 0.2119, Val Loss: 0.0982\n",
      "Epoch [157/2000], Loss: 0.2117, Val Loss: 0.0982\n",
      "Epoch [158/2000], Loss: 0.2115, Val Loss: 0.0982\n",
      "Epoch [159/2000], Loss: 0.2113, Val Loss: 0.0982\n",
      "Epoch [160/2000], Loss: 0.2111, Val Loss: 0.0982\n",
      "Epoch [161/2000], Loss: 0.2109, Val Loss: 0.0982\n",
      "Epoch [162/2000], Loss: 0.2107, Val Loss: 0.0982\n",
      "Epoch [163/2000], Loss: 0.2105, Val Loss: 0.0981\n",
      "Epoch [164/2000], Loss: 0.2103, Val Loss: 0.0981\n",
      "Epoch [165/2000], Loss: 0.2101, Val Loss: 0.0981\n",
      "Epoch [166/2000], Loss: 0.2100, Val Loss: 0.0981\n",
      "Epoch [167/2000], Loss: 0.2098, Val Loss: 0.0981\n",
      "Epoch [168/2000], Loss: 0.2096, Val Loss: 0.0981\n",
      "Epoch [169/2000], Loss: 0.2095, Val Loss: 0.0981\n",
      "Epoch [170/2000], Loss: 0.2093, Val Loss: 0.0981\n",
      "Epoch [171/2000], Loss: 0.2091, Val Loss: 0.0981\n",
      "Epoch [172/2000], Loss: 0.2090, Val Loss: 0.0981\n",
      "Epoch [173/2000], Loss: 0.2088, Val Loss: 0.0981\n",
      "Epoch [174/2000], Loss: 0.2086, Val Loss: 0.0981\n",
      "Epoch [175/2000], Loss: 0.2085, Val Loss: 0.0981\n",
      "Epoch [176/2000], Loss: 0.2083, Val Loss: 0.0981\n",
      "Epoch [177/2000], Loss: 0.2082, Val Loss: 0.0981\n",
      "Epoch [178/2000], Loss: 0.2080, Val Loss: 0.0981\n",
      "Epoch [179/2000], Loss: 0.2079, Val Loss: 0.0981\n",
      "Epoch [180/2000], Loss: 0.2077, Val Loss: 0.0981\n",
      "Epoch [181/2000], Loss: 0.2076, Val Loss: 0.0981\n",
      "Epoch [182/2000], Loss: 0.2074, Val Loss: 0.0981\n",
      "Epoch [183/2000], Loss: 0.2073, Val Loss: 0.0981\n",
      "Epoch [184/2000], Loss: 0.2071, Val Loss: 0.0981\n",
      "Epoch [185/2000], Loss: 0.2070, Val Loss: 0.0982\n",
      "Epoch [186/2000], Loss: 0.2069, Val Loss: 0.0982\n",
      "Epoch [187/2000], Loss: 0.2067, Val Loss: 0.0982\n",
      "Epoch [188/2000], Loss: 0.2066, Val Loss: 0.0982\n",
      "Epoch [189/2000], Loss: 0.2064, Val Loss: 0.0982\n",
      "Epoch [190/2000], Loss: 0.2063, Val Loss: 0.0982\n",
      "Epoch [191/2000], Loss: 0.2062, Val Loss: 0.0982\n",
      "Epoch [192/2000], Loss: 0.2060, Val Loss: 0.0982\n",
      "Epoch [193/2000], Loss: 0.2059, Val Loss: 0.0982\n",
      "Epoch [194/2000], Loss: 0.2058, Val Loss: 0.0982\n",
      "Epoch [195/2000], Loss: 0.2056, Val Loss: 0.0982\n",
      "Epoch [196/2000], Loss: 0.2055, Val Loss: 0.0982\n",
      "Epoch [197/2000], Loss: 0.2054, Val Loss: 0.0982\n",
      "Epoch [198/2000], Loss: 0.2052, Val Loss: 0.0982\n",
      "Epoch [199/2000], Loss: 0.2051, Val Loss: 0.0982\n",
      "Epoch [200/2000], Loss: 0.2050, Val Loss: 0.0982\n",
      "Epoch [201/2000], Loss: 0.2048, Val Loss: 0.0982\n",
      "Epoch [202/2000], Loss: 0.2047, Val Loss: 0.0982\n",
      "Epoch [203/2000], Loss: 0.2046, Val Loss: 0.0982\n",
      "Epoch [204/2000], Loss: 0.2045, Val Loss: 0.0982\n",
      "Epoch [205/2000], Loss: 0.2043, Val Loss: 0.0982\n",
      "Epoch [206/2000], Loss: 0.2042, Val Loss: 0.0982\n",
      "Epoch [207/2000], Loss: 0.2041, Val Loss: 0.0982\n",
      "Epoch [208/2000], Loss: 0.2040, Val Loss: 0.0983\n",
      "Epoch [209/2000], Loss: 0.2038, Val Loss: 0.0983\n",
      "Epoch [210/2000], Loss: 0.2037, Val Loss: 0.0983\n",
      "Epoch [211/2000], Loss: 0.2036, Val Loss: 0.0983\n",
      "Epoch [212/2000], Loss: 0.2035, Val Loss: 0.0983\n",
      "Epoch [213/2000], Loss: 0.2034, Val Loss: 0.0983\n",
      "Epoch [214/2000], Loss: 0.2032, Val Loss: 0.0983\n",
      "Epoch [215/2000], Loss: 0.2031, Val Loss: 0.0983\n",
      "Epoch [216/2000], Loss: 0.2030, Val Loss: 0.0984\n",
      "Epoch [217/2000], Loss: 0.2029, Val Loss: 0.0984\n",
      "Epoch [218/2000], Loss: 0.2028, Val Loss: 0.0984\n",
      "Epoch [219/2000], Loss: 0.2026, Val Loss: 0.0984\n",
      "Epoch [220/2000], Loss: 0.2025, Val Loss: 0.0984\n",
      "Epoch [221/2000], Loss: 0.2024, Val Loss: 0.0984\n",
      "Early stopping triggered\n",
      "Epoch [1/2000], Loss: 1.0147, Val Loss: 1.0395\n",
      "Epoch [2/2000], Loss: 1.0053, Val Loss: 1.0313\n",
      "Epoch [3/2000], Loss: 0.9960, Val Loss: 1.0236\n",
      "Epoch [4/2000], Loss: 0.9861, Val Loss: 1.0158\n",
      "Epoch [5/2000], Loss: 0.9750, Val Loss: 1.0075\n",
      "Epoch [6/2000], Loss: 0.9625, Val Loss: 0.9985\n",
      "Epoch [7/2000], Loss: 0.9484, Val Loss: 0.9889\n",
      "Epoch [8/2000], Loss: 0.9326, Val Loss: 0.9784\n",
      "Epoch [9/2000], Loss: 0.9150, Val Loss: 0.9670\n",
      "Epoch [10/2000], Loss: 0.8956, Val Loss: 0.9549\n",
      "Epoch [11/2000], Loss: 0.8745, Val Loss: 0.9419\n",
      "Epoch [12/2000], Loss: 0.8520, Val Loss: 0.9282\n",
      "Epoch [13/2000], Loss: 0.8282, Val Loss: 0.9136\n",
      "Epoch [14/2000], Loss: 0.8034, Val Loss: 0.8983\n",
      "Epoch [15/2000], Loss: 0.7780, Val Loss: 0.8823\n",
      "Epoch [16/2000], Loss: 0.7526, Val Loss: 0.8656\n",
      "Epoch [17/2000], Loss: 0.7278, Val Loss: 0.8485\n",
      "Epoch [18/2000], Loss: 0.7043, Val Loss: 0.8310\n",
      "Epoch [19/2000], Loss: 0.6828, Val Loss: 0.8132\n",
      "Epoch [20/2000], Loss: 0.6638, Val Loss: 0.7953\n",
      "Epoch [21/2000], Loss: 0.6475, Val Loss: 0.7772\n",
      "Epoch [22/2000], Loss: 0.6335, Val Loss: 0.7592\n",
      "Epoch [23/2000], Loss: 0.6208, Val Loss: 0.7416\n",
      "Epoch [24/2000], Loss: 0.6087, Val Loss: 0.7247\n",
      "Epoch [25/2000], Loss: 0.5965, Val Loss: 0.7090\n",
      "Epoch [26/2000], Loss: 0.5842, Val Loss: 0.6945\n",
      "Epoch [27/2000], Loss: 0.5721, Val Loss: 0.6811\n",
      "Epoch [28/2000], Loss: 0.5605, Val Loss: 0.6687\n",
      "Epoch [29/2000], Loss: 0.5499, Val Loss: 0.6566\n",
      "Epoch [30/2000], Loss: 0.5403, Val Loss: 0.6445\n",
      "Epoch [31/2000], Loss: 0.5317, Val Loss: 0.6321\n",
      "Epoch [32/2000], Loss: 0.5238, Val Loss: 0.6191\n",
      "Epoch [33/2000], Loss: 0.5163, Val Loss: 0.6056\n",
      "Epoch [34/2000], Loss: 0.5089, Val Loss: 0.5917\n",
      "Epoch [35/2000], Loss: 0.5015, Val Loss: 0.5778\n",
      "Epoch [36/2000], Loss: 0.4941, Val Loss: 0.5643\n",
      "Epoch [37/2000], Loss: 0.4868, Val Loss: 0.5516\n",
      "Epoch [38/2000], Loss: 0.4799, Val Loss: 0.5401\n",
      "Epoch [39/2000], Loss: 0.4734, Val Loss: 0.5300\n",
      "Epoch [40/2000], Loss: 0.4676, Val Loss: 0.5213\n",
      "Epoch [41/2000], Loss: 0.4626, Val Loss: 0.5139\n",
      "Epoch [42/2000], Loss: 0.4584, Val Loss: 0.5076\n",
      "Epoch [43/2000], Loss: 0.4548, Val Loss: 0.5021\n",
      "Epoch [44/2000], Loss: 0.4517, Val Loss: 0.4970\n",
      "Epoch [45/2000], Loss: 0.4489, Val Loss: 0.4922\n",
      "Epoch [46/2000], Loss: 0.4461, Val Loss: 0.4874\n",
      "Epoch [47/2000], Loss: 0.4433, Val Loss: 0.4826\n",
      "Epoch [48/2000], Loss: 0.4403, Val Loss: 0.4777\n",
      "Epoch [49/2000], Loss: 0.4372, Val Loss: 0.4728\n",
      "Epoch [50/2000], Loss: 0.4340, Val Loss: 0.4683\n",
      "Epoch [51/2000], Loss: 0.4309, Val Loss: 0.4640\n",
      "Epoch [52/2000], Loss: 0.4279, Val Loss: 0.4601\n",
      "Epoch [53/2000], Loss: 0.4250, Val Loss: 0.4566\n",
      "Epoch [54/2000], Loss: 0.4223, Val Loss: 0.4535\n",
      "Epoch [55/2000], Loss: 0.4198, Val Loss: 0.4508\n",
      "Epoch [56/2000], Loss: 0.4174, Val Loss: 0.4483\n",
      "Epoch [57/2000], Loss: 0.4151, Val Loss: 0.4462\n",
      "Epoch [58/2000], Loss: 0.4130, Val Loss: 0.4442\n",
      "Epoch [59/2000], Loss: 0.4110, Val Loss: 0.4423\n",
      "Epoch [60/2000], Loss: 0.4092, Val Loss: 0.4406\n",
      "Epoch [61/2000], Loss: 0.4075, Val Loss: 0.4389\n",
      "Epoch [62/2000], Loss: 0.4060, Val Loss: 0.4373\n",
      "Epoch [63/2000], Loss: 0.4047, Val Loss: 0.4358\n",
      "Epoch [64/2000], Loss: 0.4034, Val Loss: 0.4342\n",
      "Epoch [65/2000], Loss: 0.4022, Val Loss: 0.4326\n",
      "Epoch [66/2000], Loss: 0.4010, Val Loss: 0.4310\n",
      "Epoch [67/2000], Loss: 0.3999, Val Loss: 0.4294\n",
      "Epoch [68/2000], Loss: 0.3987, Val Loss: 0.4278\n",
      "Epoch [69/2000], Loss: 0.3976, Val Loss: 0.4263\n",
      "Epoch [70/2000], Loss: 0.3965, Val Loss: 0.4247\n",
      "Epoch [71/2000], Loss: 0.3954, Val Loss: 0.4233\n",
      "Epoch [72/2000], Loss: 0.3943, Val Loss: 0.4219\n",
      "Epoch [73/2000], Loss: 0.3932, Val Loss: 0.4205\n",
      "Epoch [74/2000], Loss: 0.3922, Val Loss: 0.4191\n",
      "Epoch [75/2000], Loss: 0.3912, Val Loss: 0.4177\n",
      "Epoch [76/2000], Loss: 0.3902, Val Loss: 0.4164\n",
      "Epoch [77/2000], Loss: 0.3892, Val Loss: 0.4150\n",
      "Epoch [78/2000], Loss: 0.3883, Val Loss: 0.4137\n",
      "Epoch [79/2000], Loss: 0.3874, Val Loss: 0.4123\n",
      "Epoch [80/2000], Loss: 0.3865, Val Loss: 0.4109\n",
      "Epoch [81/2000], Loss: 0.3856, Val Loss: 0.4094\n",
      "Epoch [82/2000], Loss: 0.3847, Val Loss: 0.4080\n",
      "Epoch [83/2000], Loss: 0.3838, Val Loss: 0.4065\n",
      "Epoch [84/2000], Loss: 0.3830, Val Loss: 0.4050\n",
      "Epoch [85/2000], Loss: 0.3822, Val Loss: 0.4036\n",
      "Epoch [86/2000], Loss: 0.3813, Val Loss: 0.4022\n",
      "Epoch [87/2000], Loss: 0.3805, Val Loss: 0.4008\n",
      "Epoch [88/2000], Loss: 0.3797, Val Loss: 0.3994\n",
      "Epoch [89/2000], Loss: 0.3789, Val Loss: 0.3981\n",
      "Epoch [90/2000], Loss: 0.3781, Val Loss: 0.3968\n",
      "Epoch [91/2000], Loss: 0.3773, Val Loss: 0.3956\n",
      "Epoch [92/2000], Loss: 0.3765, Val Loss: 0.3945\n",
      "Epoch [93/2000], Loss: 0.3757, Val Loss: 0.3934\n",
      "Epoch [94/2000], Loss: 0.3749, Val Loss: 0.3923\n",
      "Epoch [95/2000], Loss: 0.3741, Val Loss: 0.3913\n",
      "Epoch [96/2000], Loss: 0.3733, Val Loss: 0.3904\n",
      "Epoch [97/2000], Loss: 0.3725, Val Loss: 0.3895\n",
      "Epoch [98/2000], Loss: 0.3717, Val Loss: 0.3886\n",
      "Epoch [99/2000], Loss: 0.3709, Val Loss: 0.3878\n",
      "Epoch [100/2000], Loss: 0.3701, Val Loss: 0.3871\n",
      "Epoch [101/2000], Loss: 0.3693, Val Loss: 0.3864\n",
      "Epoch [102/2000], Loss: 0.3685, Val Loss: 0.3857\n",
      "Epoch [103/2000], Loss: 0.3678, Val Loss: 0.3851\n",
      "Epoch [104/2000], Loss: 0.3670, Val Loss: 0.3845\n",
      "Epoch [105/2000], Loss: 0.3662, Val Loss: 0.3839\n",
      "Epoch [106/2000], Loss: 0.3654, Val Loss: 0.3833\n",
      "Epoch [107/2000], Loss: 0.3646, Val Loss: 0.3828\n",
      "Epoch [108/2000], Loss: 0.3638, Val Loss: 0.3822\n",
      "Epoch [109/2000], Loss: 0.3631, Val Loss: 0.3817\n",
      "Epoch [110/2000], Loss: 0.3623, Val Loss: 0.3811\n",
      "Epoch [111/2000], Loss: 0.3615, Val Loss: 0.3806\n",
      "Epoch [112/2000], Loss: 0.3607, Val Loss: 0.3801\n",
      "Epoch [113/2000], Loss: 0.3599, Val Loss: 0.3796\n",
      "Epoch [114/2000], Loss: 0.3592, Val Loss: 0.3790\n",
      "Epoch [115/2000], Loss: 0.3584, Val Loss: 0.3785\n",
      "Epoch [116/2000], Loss: 0.3576, Val Loss: 0.3779\n",
      "Epoch [117/2000], Loss: 0.3568, Val Loss: 0.3774\n",
      "Epoch [118/2000], Loss: 0.3561, Val Loss: 0.3768\n",
      "Epoch [119/2000], Loss: 0.3553, Val Loss: 0.3763\n",
      "Epoch [120/2000], Loss: 0.3546, Val Loss: 0.3757\n",
      "Epoch [121/2000], Loss: 0.3538, Val Loss: 0.3752\n",
      "Epoch [122/2000], Loss: 0.3531, Val Loss: 0.3747\n",
      "Epoch [123/2000], Loss: 0.3524, Val Loss: 0.3742\n",
      "Epoch [124/2000], Loss: 0.3517, Val Loss: 0.3738\n",
      "Epoch [125/2000], Loss: 0.3509, Val Loss: 0.3733\n",
      "Epoch [126/2000], Loss: 0.3502, Val Loss: 0.3729\n",
      "Epoch [127/2000], Loss: 0.3495, Val Loss: 0.3724\n",
      "Epoch [128/2000], Loss: 0.3488, Val Loss: 0.3720\n",
      "Epoch [129/2000], Loss: 0.3482, Val Loss: 0.3717\n",
      "Epoch [130/2000], Loss: 0.3475, Val Loss: 0.3713\n",
      "Epoch [131/2000], Loss: 0.3468, Val Loss: 0.3709\n",
      "Epoch [132/2000], Loss: 0.3462, Val Loss: 0.3706\n",
      "Epoch [133/2000], Loss: 0.3455, Val Loss: 0.3703\n",
      "Epoch [134/2000], Loss: 0.3449, Val Loss: 0.3700\n",
      "Epoch [135/2000], Loss: 0.3443, Val Loss: 0.3696\n",
      "Epoch [136/2000], Loss: 0.3437, Val Loss: 0.3693\n",
      "Epoch [137/2000], Loss: 0.3431, Val Loss: 0.3690\n",
      "Epoch [138/2000], Loss: 0.3425, Val Loss: 0.3687\n",
      "Epoch [139/2000], Loss: 0.3419, Val Loss: 0.3684\n",
      "Epoch [140/2000], Loss: 0.3413, Val Loss: 0.3681\n",
      "Epoch [141/2000], Loss: 0.3408, Val Loss: 0.3678\n",
      "Epoch [142/2000], Loss: 0.3402, Val Loss: 0.3675\n",
      "Epoch [143/2000], Loss: 0.3397, Val Loss: 0.3672\n",
      "Epoch [144/2000], Loss: 0.3391, Val Loss: 0.3669\n",
      "Epoch [145/2000], Loss: 0.3386, Val Loss: 0.3666\n",
      "Epoch [146/2000], Loss: 0.3381, Val Loss: 0.3663\n",
      "Epoch [147/2000], Loss: 0.3376, Val Loss: 0.3660\n",
      "Epoch [148/2000], Loss: 0.3371, Val Loss: 0.3656\n",
      "Epoch [149/2000], Loss: 0.3366, Val Loss: 0.3653\n",
      "Epoch [150/2000], Loss: 0.3362, Val Loss: 0.3650\n",
      "Epoch [151/2000], Loss: 0.3357, Val Loss: 0.3646\n",
      "Epoch [152/2000], Loss: 0.3352, Val Loss: 0.3643\n",
      "Epoch [153/2000], Loss: 0.3348, Val Loss: 0.3639\n",
      "Epoch [154/2000], Loss: 0.3343, Val Loss: 0.3636\n",
      "Epoch [155/2000], Loss: 0.3339, Val Loss: 0.3632\n",
      "Epoch [156/2000], Loss: 0.3335, Val Loss: 0.3628\n",
      "Epoch [157/2000], Loss: 0.3331, Val Loss: 0.3625\n",
      "Epoch [158/2000], Loss: 0.3326, Val Loss: 0.3621\n",
      "Epoch [159/2000], Loss: 0.3322, Val Loss: 0.3617\n",
      "Epoch [160/2000], Loss: 0.3318, Val Loss: 0.3613\n",
      "Epoch [161/2000], Loss: 0.3314, Val Loss: 0.3610\n",
      "Epoch [162/2000], Loss: 0.3310, Val Loss: 0.3606\n",
      "Epoch [163/2000], Loss: 0.3306, Val Loss: 0.3602\n",
      "Epoch [164/2000], Loss: 0.3303, Val Loss: 0.3598\n",
      "Epoch [165/2000], Loss: 0.3299, Val Loss: 0.3594\n",
      "Epoch [166/2000], Loss: 0.3295, Val Loss: 0.3590\n",
      "Epoch [167/2000], Loss: 0.3291, Val Loss: 0.3586\n",
      "Epoch [168/2000], Loss: 0.3288, Val Loss: 0.3582\n",
      "Epoch [169/2000], Loss: 0.3284, Val Loss: 0.3577\n",
      "Epoch [170/2000], Loss: 0.3280, Val Loss: 0.3573\n",
      "Epoch [171/2000], Loss: 0.3277, Val Loss: 0.3569\n",
      "Epoch [172/2000], Loss: 0.3273, Val Loss: 0.3565\n",
      "Epoch [173/2000], Loss: 0.3270, Val Loss: 0.3561\n",
      "Epoch [174/2000], Loss: 0.3266, Val Loss: 0.3556\n",
      "Epoch [175/2000], Loss: 0.3263, Val Loss: 0.3552\n",
      "Epoch [176/2000], Loss: 0.3260, Val Loss: 0.3548\n",
      "Epoch [177/2000], Loss: 0.3256, Val Loss: 0.3543\n",
      "Epoch [178/2000], Loss: 0.3253, Val Loss: 0.3539\n",
      "Epoch [179/2000], Loss: 0.3250, Val Loss: 0.3534\n",
      "Epoch [180/2000], Loss: 0.3247, Val Loss: 0.3530\n",
      "Epoch [181/2000], Loss: 0.3243, Val Loss: 0.3525\n",
      "Epoch [182/2000], Loss: 0.3240, Val Loss: 0.3521\n",
      "Epoch [183/2000], Loss: 0.3237, Val Loss: 0.3516\n",
      "Epoch [184/2000], Loss: 0.3234, Val Loss: 0.3512\n",
      "Epoch [185/2000], Loss: 0.3231, Val Loss: 0.3507\n",
      "Epoch [186/2000], Loss: 0.3228, Val Loss: 0.3503\n",
      "Epoch [187/2000], Loss: 0.3225, Val Loss: 0.3498\n",
      "Epoch [188/2000], Loss: 0.3222, Val Loss: 0.3494\n",
      "Epoch [189/2000], Loss: 0.3219, Val Loss: 0.3489\n",
      "Epoch [190/2000], Loss: 0.3216, Val Loss: 0.3485\n",
      "Epoch [191/2000], Loss: 0.3213, Val Loss: 0.3480\n",
      "Epoch [192/2000], Loss: 0.3210, Val Loss: 0.3476\n",
      "Epoch [193/2000], Loss: 0.3207, Val Loss: 0.3471\n",
      "Epoch [194/2000], Loss: 0.3204, Val Loss: 0.3467\n",
      "Epoch [195/2000], Loss: 0.3201, Val Loss: 0.3463\n",
      "Epoch [196/2000], Loss: 0.3198, Val Loss: 0.3458\n",
      "Epoch [197/2000], Loss: 0.3195, Val Loss: 0.3454\n",
      "Epoch [198/2000], Loss: 0.3193, Val Loss: 0.3450\n",
      "Epoch [199/2000], Loss: 0.3190, Val Loss: 0.3445\n",
      "Epoch [200/2000], Loss: 0.3187, Val Loss: 0.3441\n",
      "Epoch [201/2000], Loss: 0.3184, Val Loss: 0.3437\n",
      "Epoch [202/2000], Loss: 0.3182, Val Loss: 0.3433\n",
      "Epoch [203/2000], Loss: 0.3179, Val Loss: 0.3428\n",
      "Epoch [204/2000], Loss: 0.3176, Val Loss: 0.3424\n",
      "Epoch [205/2000], Loss: 0.3174, Val Loss: 0.3420\n",
      "Epoch [206/2000], Loss: 0.3171, Val Loss: 0.3416\n",
      "Epoch [207/2000], Loss: 0.3169, Val Loss: 0.3412\n",
      "Epoch [208/2000], Loss: 0.3166, Val Loss: 0.3408\n",
      "Epoch [209/2000], Loss: 0.3163, Val Loss: 0.3404\n",
      "Epoch [210/2000], Loss: 0.3161, Val Loss: 0.3399\n",
      "Epoch [211/2000], Loss: 0.3158, Val Loss: 0.3395\n",
      "Epoch [212/2000], Loss: 0.3156, Val Loss: 0.3391\n",
      "Epoch [213/2000], Loss: 0.3153, Val Loss: 0.3387\n",
      "Epoch [214/2000], Loss: 0.3151, Val Loss: 0.3383\n",
      "Epoch [215/2000], Loss: 0.3148, Val Loss: 0.3379\n",
      "Epoch [216/2000], Loss: 0.3146, Val Loss: 0.3375\n",
      "Epoch [217/2000], Loss: 0.3144, Val Loss: 0.3371\n",
      "Epoch [218/2000], Loss: 0.3141, Val Loss: 0.3368\n",
      "Epoch [219/2000], Loss: 0.3139, Val Loss: 0.3364\n",
      "Epoch [220/2000], Loss: 0.3137, Val Loss: 0.3360\n",
      "Epoch [221/2000], Loss: 0.3134, Val Loss: 0.3356\n",
      "Epoch [222/2000], Loss: 0.3132, Val Loss: 0.3353\n",
      "Epoch [223/2000], Loss: 0.3130, Val Loss: 0.3349\n",
      "Epoch [224/2000], Loss: 0.3127, Val Loss: 0.3346\n",
      "Epoch [225/2000], Loss: 0.3125, Val Loss: 0.3342\n",
      "Epoch [226/2000], Loss: 0.3123, Val Loss: 0.3339\n",
      "Epoch [227/2000], Loss: 0.3121, Val Loss: 0.3335\n",
      "Epoch [228/2000], Loss: 0.3119, Val Loss: 0.3332\n",
      "Epoch [229/2000], Loss: 0.3117, Val Loss: 0.3329\n",
      "Epoch [230/2000], Loss: 0.3114, Val Loss: 0.3326\n",
      "Epoch [231/2000], Loss: 0.3112, Val Loss: 0.3322\n",
      "Epoch [232/2000], Loss: 0.3110, Val Loss: 0.3319\n",
      "Epoch [233/2000], Loss: 0.3108, Val Loss: 0.3316\n",
      "Epoch [234/2000], Loss: 0.3106, Val Loss: 0.3313\n",
      "Epoch [235/2000], Loss: 0.3104, Val Loss: 0.3310\n",
      "Epoch [236/2000], Loss: 0.3102, Val Loss: 0.3307\n",
      "Epoch [237/2000], Loss: 0.3100, Val Loss: 0.3304\n",
      "Epoch [238/2000], Loss: 0.3098, Val Loss: 0.3301\n",
      "Epoch [239/2000], Loss: 0.3096, Val Loss: 0.3299\n",
      "Epoch [240/2000], Loss: 0.3094, Val Loss: 0.3296\n",
      "Epoch [241/2000], Loss: 0.3092, Val Loss: 0.3293\n",
      "Epoch [242/2000], Loss: 0.3090, Val Loss: 0.3291\n",
      "Epoch [243/2000], Loss: 0.3088, Val Loss: 0.3288\n",
      "Epoch [244/2000], Loss: 0.3086, Val Loss: 0.3285\n",
      "Epoch [245/2000], Loss: 0.3085, Val Loss: 0.3283\n",
      "Epoch [246/2000], Loss: 0.3083, Val Loss: 0.3280\n",
      "Epoch [247/2000], Loss: 0.3081, Val Loss: 0.3278\n",
      "Epoch [248/2000], Loss: 0.3079, Val Loss: 0.3275\n",
      "Epoch [249/2000], Loss: 0.3077, Val Loss: 0.3273\n",
      "Epoch [250/2000], Loss: 0.3075, Val Loss: 0.3270\n",
      "Epoch [251/2000], Loss: 0.3073, Val Loss: 0.3268\n",
      "Epoch [252/2000], Loss: 0.3072, Val Loss: 0.3266\n",
      "Epoch [253/2000], Loss: 0.3070, Val Loss: 0.3263\n",
      "Epoch [254/2000], Loss: 0.3068, Val Loss: 0.3261\n",
      "Epoch [255/2000], Loss: 0.3066, Val Loss: 0.3259\n",
      "Epoch [256/2000], Loss: 0.3065, Val Loss: 0.3257\n",
      "Epoch [257/2000], Loss: 0.3063, Val Loss: 0.3255\n",
      "Epoch [258/2000], Loss: 0.3061, Val Loss: 0.3252\n",
      "Epoch [259/2000], Loss: 0.3059, Val Loss: 0.3250\n",
      "Epoch [260/2000], Loss: 0.3058, Val Loss: 0.3248\n",
      "Epoch [261/2000], Loss: 0.3056, Val Loss: 0.3246\n",
      "Epoch [262/2000], Loss: 0.3054, Val Loss: 0.3244\n",
      "Epoch [263/2000], Loss: 0.3053, Val Loss: 0.3242\n",
      "Epoch [264/2000], Loss: 0.3051, Val Loss: 0.3240\n",
      "Epoch [265/2000], Loss: 0.3049, Val Loss: 0.3238\n",
      "Epoch [266/2000], Loss: 0.3048, Val Loss: 0.3236\n",
      "Epoch [267/2000], Loss: 0.3046, Val Loss: 0.3234\n",
      "Epoch [268/2000], Loss: 0.3044, Val Loss: 0.3232\n",
      "Epoch [269/2000], Loss: 0.3043, Val Loss: 0.3230\n",
      "Epoch [270/2000], Loss: 0.3041, Val Loss: 0.3228\n",
      "Epoch [271/2000], Loss: 0.3040, Val Loss: 0.3226\n",
      "Epoch [272/2000], Loss: 0.3038, Val Loss: 0.3224\n",
      "Epoch [273/2000], Loss: 0.3036, Val Loss: 0.3222\n",
      "Epoch [274/2000], Loss: 0.3035, Val Loss: 0.3220\n",
      "Epoch [275/2000], Loss: 0.3033, Val Loss: 0.3218\n",
      "Epoch [276/2000], Loss: 0.3032, Val Loss: 0.3217\n",
      "Epoch [277/2000], Loss: 0.3030, Val Loss: 0.3215\n",
      "Epoch [278/2000], Loss: 0.3028, Val Loss: 0.3213\n",
      "Epoch [279/2000], Loss: 0.3027, Val Loss: 0.3211\n",
      "Epoch [280/2000], Loss: 0.3025, Val Loss: 0.3209\n",
      "Epoch [281/2000], Loss: 0.3024, Val Loss: 0.3207\n",
      "Epoch [282/2000], Loss: 0.3022, Val Loss: 0.3205\n",
      "Epoch [283/2000], Loss: 0.3021, Val Loss: 0.3203\n",
      "Epoch [284/2000], Loss: 0.3019, Val Loss: 0.3201\n",
      "Epoch [285/2000], Loss: 0.3018, Val Loss: 0.3199\n",
      "Epoch [286/2000], Loss: 0.3016, Val Loss: 0.3197\n",
      "Epoch [287/2000], Loss: 0.3015, Val Loss: 0.3195\n",
      "Epoch [288/2000], Loss: 0.3013, Val Loss: 0.3194\n",
      "Epoch [289/2000], Loss: 0.3012, Val Loss: 0.3192\n",
      "Epoch [290/2000], Loss: 0.3010, Val Loss: 0.3190\n",
      "Epoch [291/2000], Loss: 0.3009, Val Loss: 0.3188\n",
      "Epoch [292/2000], Loss: 0.3007, Val Loss: 0.3186\n",
      "Epoch [293/2000], Loss: 0.3006, Val Loss: 0.3184\n",
      "Epoch [294/2000], Loss: 0.3004, Val Loss: 0.3182\n",
      "Epoch [295/2000], Loss: 0.3003, Val Loss: 0.3181\n",
      "Epoch [296/2000], Loss: 0.3001, Val Loss: 0.3179\n",
      "Epoch [297/2000], Loss: 0.3000, Val Loss: 0.3177\n",
      "Epoch [298/2000], Loss: 0.2998, Val Loss: 0.3175\n",
      "Epoch [299/2000], Loss: 0.2997, Val Loss: 0.3173\n",
      "Epoch [300/2000], Loss: 0.2995, Val Loss: 0.3172\n",
      "Epoch [301/2000], Loss: 0.2994, Val Loss: 0.3170\n",
      "Epoch [302/2000], Loss: 0.2993, Val Loss: 0.3168\n",
      "Epoch [303/2000], Loss: 0.2991, Val Loss: 0.3167\n",
      "Epoch [304/2000], Loss: 0.2990, Val Loss: 0.3165\n",
      "Epoch [305/2000], Loss: 0.2988, Val Loss: 0.3163\n",
      "Epoch [306/2000], Loss: 0.2987, Val Loss: 0.3161\n",
      "Epoch [307/2000], Loss: 0.2986, Val Loss: 0.3160\n",
      "Epoch [308/2000], Loss: 0.2984, Val Loss: 0.3158\n",
      "Epoch [309/2000], Loss: 0.2983, Val Loss: 0.3157\n",
      "Epoch [310/2000], Loss: 0.2981, Val Loss: 0.3155\n",
      "Epoch [311/2000], Loss: 0.2980, Val Loss: 0.3154\n",
      "Epoch [312/2000], Loss: 0.2979, Val Loss: 0.3152\n",
      "Epoch [313/2000], Loss: 0.2977, Val Loss: 0.3150\n",
      "Epoch [314/2000], Loss: 0.2976, Val Loss: 0.3149\n",
      "Epoch [315/2000], Loss: 0.2974, Val Loss: 0.3147\n",
      "Epoch [316/2000], Loss: 0.2973, Val Loss: 0.3146\n",
      "Epoch [317/2000], Loss: 0.2972, Val Loss: 0.3144\n",
      "Epoch [318/2000], Loss: 0.2970, Val Loss: 0.3143\n",
      "Epoch [319/2000], Loss: 0.2969, Val Loss: 0.3141\n",
      "Epoch [320/2000], Loss: 0.2968, Val Loss: 0.3139\n",
      "Epoch [321/2000], Loss: 0.2966, Val Loss: 0.3138\n",
      "Epoch [322/2000], Loss: 0.2965, Val Loss: 0.3136\n",
      "Epoch [323/2000], Loss: 0.2964, Val Loss: 0.3135\n",
      "Epoch [324/2000], Loss: 0.2962, Val Loss: 0.3133\n",
      "Epoch [325/2000], Loss: 0.2961, Val Loss: 0.3131\n",
      "Epoch [326/2000], Loss: 0.2960, Val Loss: 0.3130\n",
      "Epoch [327/2000], Loss: 0.2958, Val Loss: 0.3128\n",
      "Epoch [328/2000], Loss: 0.2957, Val Loss: 0.3127\n",
      "Epoch [329/2000], Loss: 0.2956, Val Loss: 0.3125\n",
      "Epoch [330/2000], Loss: 0.2954, Val Loss: 0.3124\n",
      "Epoch [331/2000], Loss: 0.2953, Val Loss: 0.3122\n",
      "Epoch [332/2000], Loss: 0.2952, Val Loss: 0.3121\n",
      "Epoch [333/2000], Loss: 0.2951, Val Loss: 0.3120\n",
      "Epoch [334/2000], Loss: 0.2949, Val Loss: 0.3118\n",
      "Epoch [335/2000], Loss: 0.2948, Val Loss: 0.3117\n",
      "Epoch [336/2000], Loss: 0.2947, Val Loss: 0.3116\n",
      "Epoch [337/2000], Loss: 0.2945, Val Loss: 0.3114\n",
      "Epoch [338/2000], Loss: 0.2944, Val Loss: 0.3113\n",
      "Epoch [339/2000], Loss: 0.2943, Val Loss: 0.3112\n",
      "Epoch [340/2000], Loss: 0.2942, Val Loss: 0.3111\n",
      "Epoch [341/2000], Loss: 0.2940, Val Loss: 0.3109\n",
      "Epoch [342/2000], Loss: 0.2939, Val Loss: 0.3108\n",
      "Epoch [343/2000], Loss: 0.2938, Val Loss: 0.3107\n",
      "Epoch [344/2000], Loss: 0.2937, Val Loss: 0.3106\n",
      "Epoch [345/2000], Loss: 0.2935, Val Loss: 0.3105\n",
      "Epoch [346/2000], Loss: 0.2934, Val Loss: 0.3104\n",
      "Epoch [347/2000], Loss: 0.2933, Val Loss: 0.3103\n",
      "Epoch [348/2000], Loss: 0.2932, Val Loss: 0.3101\n",
      "Epoch [349/2000], Loss: 0.2931, Val Loss: 0.3100\n",
      "Epoch [350/2000], Loss: 0.2929, Val Loss: 0.3099\n",
      "Epoch [351/2000], Loss: 0.2928, Val Loss: 0.3098\n",
      "Epoch [352/2000], Loss: 0.2927, Val Loss: 0.3097\n",
      "Epoch [353/2000], Loss: 0.2926, Val Loss: 0.3096\n",
      "Epoch [354/2000], Loss: 0.2925, Val Loss: 0.3095\n",
      "Epoch [355/2000], Loss: 0.2923, Val Loss: 0.3094\n",
      "Epoch [356/2000], Loss: 0.2922, Val Loss: 0.3094\n",
      "Epoch [357/2000], Loss: 0.2921, Val Loss: 0.3093\n",
      "Epoch [358/2000], Loss: 0.2920, Val Loss: 0.3092\n",
      "Epoch [359/2000], Loss: 0.2919, Val Loss: 0.3091\n",
      "Epoch [360/2000], Loss: 0.2917, Val Loss: 0.3090\n",
      "Epoch [361/2000], Loss: 0.2916, Val Loss: 0.3089\n",
      "Epoch [362/2000], Loss: 0.2915, Val Loss: 0.3088\n",
      "Epoch [363/2000], Loss: 0.2914, Val Loss: 0.3088\n",
      "Epoch [364/2000], Loss: 0.2913, Val Loss: 0.3087\n",
      "Epoch [365/2000], Loss: 0.2912, Val Loss: 0.3086\n",
      "Early stopping triggered\n",
      "Epoch [1/2000], Loss: 1.0382, Val Loss: 0.9401\n",
      "Epoch [2/2000], Loss: 1.0267, Val Loss: 0.9320\n",
      "Epoch [3/2000], Loss: 1.0165, Val Loss: 0.9247\n",
      "Epoch [4/2000], Loss: 1.0072, Val Loss: 0.9181\n",
      "Epoch [5/2000], Loss: 0.9987, Val Loss: 0.9119\n",
      "Epoch [6/2000], Loss: 0.9908, Val Loss: 0.9061\n",
      "Epoch [7/2000], Loss: 0.9833, Val Loss: 0.9004\n",
      "Epoch [8/2000], Loss: 0.9759, Val Loss: 0.8947\n",
      "Epoch [9/2000], Loss: 0.9686, Val Loss: 0.8889\n",
      "Epoch [10/2000], Loss: 0.9613, Val Loss: 0.8830\n",
      "Epoch [11/2000], Loss: 0.9538, Val Loss: 0.8768\n",
      "Epoch [12/2000], Loss: 0.9459, Val Loss: 0.8702\n",
      "Epoch [13/2000], Loss: 0.9376, Val Loss: 0.8631\n",
      "Epoch [14/2000], Loss: 0.9287, Val Loss: 0.8557\n",
      "Epoch [15/2000], Loss: 0.9194, Val Loss: 0.8479\n",
      "Epoch [16/2000], Loss: 0.9094, Val Loss: 0.8395\n",
      "Epoch [17/2000], Loss: 0.8987, Val Loss: 0.8304\n",
      "Epoch [18/2000], Loss: 0.8874, Val Loss: 0.8209\n",
      "Epoch [19/2000], Loss: 0.8754, Val Loss: 0.8109\n",
      "Epoch [20/2000], Loss: 0.8628, Val Loss: 0.8005\n",
      "Epoch [21/2000], Loss: 0.8496, Val Loss: 0.7895\n",
      "Epoch [22/2000], Loss: 0.8359, Val Loss: 0.7782\n",
      "Epoch [23/2000], Loss: 0.8216, Val Loss: 0.7665\n",
      "Epoch [24/2000], Loss: 0.8070, Val Loss: 0.7546\n",
      "Epoch [25/2000], Loss: 0.7919, Val Loss: 0.7424\n",
      "Epoch [26/2000], Loss: 0.7766, Val Loss: 0.7303\n",
      "Epoch [27/2000], Loss: 0.7612, Val Loss: 0.7181\n",
      "Epoch [28/2000], Loss: 0.7458, Val Loss: 0.7060\n",
      "Epoch [29/2000], Loss: 0.7305, Val Loss: 0.6942\n",
      "Epoch [30/2000], Loss: 0.7155, Val Loss: 0.6826\n",
      "Epoch [31/2000], Loss: 0.7008, Val Loss: 0.6716\n",
      "Epoch [32/2000], Loss: 0.6867, Val Loss: 0.6610\n",
      "Epoch [33/2000], Loss: 0.6731, Val Loss: 0.6510\n",
      "Epoch [34/2000], Loss: 0.6602, Val Loss: 0.6417\n",
      "Epoch [35/2000], Loss: 0.6480, Val Loss: 0.6330\n",
      "Epoch [36/2000], Loss: 0.6365, Val Loss: 0.6249\n",
      "Epoch [37/2000], Loss: 0.6258, Val Loss: 0.6176\n",
      "Epoch [38/2000], Loss: 0.6158, Val Loss: 0.6108\n",
      "Epoch [39/2000], Loss: 0.6067, Val Loss: 0.6048\n",
      "Epoch [40/2000], Loss: 0.5982, Val Loss: 0.5994\n",
      "Epoch [41/2000], Loss: 0.5906, Val Loss: 0.5947\n",
      "Epoch [42/2000], Loss: 0.5837, Val Loss: 0.5907\n",
      "Epoch [43/2000], Loss: 0.5777, Val Loss: 0.5874\n",
      "Epoch [44/2000], Loss: 0.5724, Val Loss: 0.5846\n",
      "Epoch [45/2000], Loss: 0.5678, Val Loss: 0.5823\n",
      "Epoch [46/2000], Loss: 0.5638, Val Loss: 0.5804\n",
      "Epoch [47/2000], Loss: 0.5603, Val Loss: 0.5786\n",
      "Epoch [48/2000], Loss: 0.5571, Val Loss: 0.5769\n",
      "Epoch [49/2000], Loss: 0.5540, Val Loss: 0.5750\n",
      "Epoch [50/2000], Loss: 0.5508, Val Loss: 0.5729\n",
      "Epoch [51/2000], Loss: 0.5474, Val Loss: 0.5705\n",
      "Epoch [52/2000], Loss: 0.5438, Val Loss: 0.5678\n",
      "Epoch [53/2000], Loss: 0.5399, Val Loss: 0.5650\n",
      "Epoch [54/2000], Loss: 0.5357, Val Loss: 0.5620\n",
      "Epoch [55/2000], Loss: 0.5312, Val Loss: 0.5591\n",
      "Epoch [56/2000], Loss: 0.5266, Val Loss: 0.5561\n",
      "Epoch [57/2000], Loss: 0.5220, Val Loss: 0.5533\n",
      "Epoch [58/2000], Loss: 0.5174, Val Loss: 0.5507\n",
      "Epoch [59/2000], Loss: 0.5129, Val Loss: 0.5482\n",
      "Epoch [60/2000], Loss: 0.5086, Val Loss: 0.5460\n",
      "Epoch [61/2000], Loss: 0.5045, Val Loss: 0.5439\n",
      "Epoch [62/2000], Loss: 0.5006, Val Loss: 0.5421\n",
      "Epoch [63/2000], Loss: 0.4970, Val Loss: 0.5404\n",
      "Epoch [64/2000], Loss: 0.4936, Val Loss: 0.5388\n",
      "Epoch [65/2000], Loss: 0.4904, Val Loss: 0.5373\n",
      "Epoch [66/2000], Loss: 0.4874, Val Loss: 0.5358\n",
      "Epoch [67/2000], Loss: 0.4847, Val Loss: 0.5344\n",
      "Epoch [68/2000], Loss: 0.4821, Val Loss: 0.5330\n",
      "Epoch [69/2000], Loss: 0.4796, Val Loss: 0.5315\n",
      "Epoch [70/2000], Loss: 0.4774, Val Loss: 0.5300\n",
      "Epoch [71/2000], Loss: 0.4752, Val Loss: 0.5283\n",
      "Epoch [72/2000], Loss: 0.4732, Val Loss: 0.5267\n",
      "Epoch [73/2000], Loss: 0.4713, Val Loss: 0.5249\n",
      "Epoch [74/2000], Loss: 0.4695, Val Loss: 0.5231\n",
      "Epoch [75/2000], Loss: 0.4678, Val Loss: 0.5212\n",
      "Epoch [76/2000], Loss: 0.4661, Val Loss: 0.5192\n",
      "Epoch [77/2000], Loss: 0.4645, Val Loss: 0.5172\n",
      "Epoch [78/2000], Loss: 0.4629, Val Loss: 0.5152\n",
      "Epoch [79/2000], Loss: 0.4613, Val Loss: 0.5131\n",
      "Epoch [80/2000], Loss: 0.4597, Val Loss: 0.5109\n",
      "Epoch [81/2000], Loss: 0.4582, Val Loss: 0.5088\n",
      "Epoch [82/2000], Loss: 0.4567, Val Loss: 0.5066\n",
      "Epoch [83/2000], Loss: 0.4552, Val Loss: 0.5044\n",
      "Epoch [84/2000], Loss: 0.4537, Val Loss: 0.5022\n",
      "Epoch [85/2000], Loss: 0.4522, Val Loss: 0.4999\n",
      "Epoch [86/2000], Loss: 0.4508, Val Loss: 0.4977\n",
      "Epoch [87/2000], Loss: 0.4493, Val Loss: 0.4954\n",
      "Epoch [88/2000], Loss: 0.4479, Val Loss: 0.4931\n",
      "Epoch [89/2000], Loss: 0.4465, Val Loss: 0.4909\n",
      "Epoch [90/2000], Loss: 0.4451, Val Loss: 0.4886\n",
      "Epoch [91/2000], Loss: 0.4437, Val Loss: 0.4864\n",
      "Epoch [92/2000], Loss: 0.4423, Val Loss: 0.4841\n",
      "Epoch [93/2000], Loss: 0.4410, Val Loss: 0.4819\n",
      "Epoch [94/2000], Loss: 0.4396, Val Loss: 0.4797\n",
      "Epoch [95/2000], Loss: 0.4383, Val Loss: 0.4775\n",
      "Epoch [96/2000], Loss: 0.4369, Val Loss: 0.4753\n",
      "Epoch [97/2000], Loss: 0.4356, Val Loss: 0.4731\n",
      "Epoch [98/2000], Loss: 0.4343, Val Loss: 0.4709\n",
      "Epoch [99/2000], Loss: 0.4330, Val Loss: 0.4686\n",
      "Epoch [100/2000], Loss: 0.4318, Val Loss: 0.4664\n",
      "Epoch [101/2000], Loss: 0.4305, Val Loss: 0.4642\n",
      "Epoch [102/2000], Loss: 0.4293, Val Loss: 0.4619\n",
      "Epoch [103/2000], Loss: 0.4281, Val Loss: 0.4597\n",
      "Epoch [104/2000], Loss: 0.4270, Val Loss: 0.4575\n",
      "Epoch [105/2000], Loss: 0.4258, Val Loss: 0.4553\n",
      "Epoch [106/2000], Loss: 0.4247, Val Loss: 0.4532\n",
      "Epoch [107/2000], Loss: 0.4235, Val Loss: 0.4511\n",
      "Epoch [108/2000], Loss: 0.4224, Val Loss: 0.4491\n",
      "Epoch [109/2000], Loss: 0.4214, Val Loss: 0.4471\n",
      "Epoch [110/2000], Loss: 0.4203, Val Loss: 0.4452\n",
      "Epoch [111/2000], Loss: 0.4192, Val Loss: 0.4433\n",
      "Epoch [112/2000], Loss: 0.4182, Val Loss: 0.4415\n",
      "Epoch [113/2000], Loss: 0.4172, Val Loss: 0.4397\n",
      "Epoch [114/2000], Loss: 0.4162, Val Loss: 0.4380\n",
      "Epoch [115/2000], Loss: 0.4152, Val Loss: 0.4364\n",
      "Epoch [116/2000], Loss: 0.4142, Val Loss: 0.4347\n",
      "Epoch [117/2000], Loss: 0.4133, Val Loss: 0.4332\n",
      "Epoch [118/2000], Loss: 0.4124, Val Loss: 0.4316\n",
      "Epoch [119/2000], Loss: 0.4115, Val Loss: 0.4301\n",
      "Epoch [120/2000], Loss: 0.4106, Val Loss: 0.4287\n",
      "Epoch [121/2000], Loss: 0.4097, Val Loss: 0.4273\n",
      "Epoch [122/2000], Loss: 0.4089, Val Loss: 0.4259\n",
      "Epoch [123/2000], Loss: 0.4081, Val Loss: 0.4246\n",
      "Epoch [124/2000], Loss: 0.4072, Val Loss: 0.4234\n",
      "Epoch [125/2000], Loss: 0.4065, Val Loss: 0.4222\n",
      "Epoch [126/2000], Loss: 0.4057, Val Loss: 0.4210\n",
      "Epoch [127/2000], Loss: 0.4049, Val Loss: 0.4199\n",
      "Epoch [128/2000], Loss: 0.4042, Val Loss: 0.4189\n",
      "Epoch [129/2000], Loss: 0.4034, Val Loss: 0.4179\n",
      "Epoch [130/2000], Loss: 0.4027, Val Loss: 0.4169\n",
      "Epoch [131/2000], Loss: 0.4020, Val Loss: 0.4160\n",
      "Epoch [132/2000], Loss: 0.4013, Val Loss: 0.4152\n",
      "Epoch [133/2000], Loss: 0.4006, Val Loss: 0.4144\n",
      "Epoch [134/2000], Loss: 0.4000, Val Loss: 0.4136\n",
      "Epoch [135/2000], Loss: 0.3993, Val Loss: 0.4129\n",
      "Epoch [136/2000], Loss: 0.3987, Val Loss: 0.4122\n",
      "Epoch [137/2000], Loss: 0.3980, Val Loss: 0.4116\n",
      "Epoch [138/2000], Loss: 0.3974, Val Loss: 0.4110\n",
      "Epoch [139/2000], Loss: 0.3968, Val Loss: 0.4103\n",
      "Epoch [140/2000], Loss: 0.3962, Val Loss: 0.4098\n",
      "Epoch [141/2000], Loss: 0.3956, Val Loss: 0.4092\n",
      "Epoch [142/2000], Loss: 0.3950, Val Loss: 0.4087\n",
      "Epoch [143/2000], Loss: 0.3945, Val Loss: 0.4082\n",
      "Epoch [144/2000], Loss: 0.3939, Val Loss: 0.4077\n",
      "Epoch [145/2000], Loss: 0.3933, Val Loss: 0.4072\n",
      "Epoch [146/2000], Loss: 0.3928, Val Loss: 0.4068\n",
      "Epoch [147/2000], Loss: 0.3923, Val Loss: 0.4064\n",
      "Epoch [148/2000], Loss: 0.3917, Val Loss: 0.4061\n",
      "Epoch [149/2000], Loss: 0.3912, Val Loss: 0.4057\n",
      "Epoch [150/2000], Loss: 0.3907, Val Loss: 0.4053\n",
      "Epoch [151/2000], Loss: 0.3902, Val Loss: 0.4050\n",
      "Epoch [152/2000], Loss: 0.3897, Val Loss: 0.4046\n",
      "Epoch [153/2000], Loss: 0.3892, Val Loss: 0.4043\n",
      "Epoch [154/2000], Loss: 0.3887, Val Loss: 0.4039\n",
      "Epoch [155/2000], Loss: 0.3882, Val Loss: 0.4035\n",
      "Epoch [156/2000], Loss: 0.3877, Val Loss: 0.4032\n",
      "Epoch [157/2000], Loss: 0.3872, Val Loss: 0.4029\n",
      "Epoch [158/2000], Loss: 0.3868, Val Loss: 0.4025\n",
      "Epoch [159/2000], Loss: 0.3863, Val Loss: 0.4022\n",
      "Epoch [160/2000], Loss: 0.3858, Val Loss: 0.4019\n",
      "Epoch [161/2000], Loss: 0.3854, Val Loss: 0.4016\n",
      "Epoch [162/2000], Loss: 0.3849, Val Loss: 0.4012\n",
      "Epoch [163/2000], Loss: 0.3845, Val Loss: 0.4009\n",
      "Epoch [164/2000], Loss: 0.3840, Val Loss: 0.4006\n",
      "Epoch [165/2000], Loss: 0.3836, Val Loss: 0.4003\n",
      "Epoch [166/2000], Loss: 0.3831, Val Loss: 0.3999\n",
      "Epoch [167/2000], Loss: 0.3827, Val Loss: 0.3996\n",
      "Epoch [168/2000], Loss: 0.3823, Val Loss: 0.3992\n",
      "Epoch [169/2000], Loss: 0.3818, Val Loss: 0.3989\n",
      "Epoch [170/2000], Loss: 0.3814, Val Loss: 0.3985\n",
      "Epoch [171/2000], Loss: 0.3810, Val Loss: 0.3982\n",
      "Epoch [172/2000], Loss: 0.3806, Val Loss: 0.3978\n",
      "Epoch [173/2000], Loss: 0.3802, Val Loss: 0.3974\n",
      "Epoch [174/2000], Loss: 0.3797, Val Loss: 0.3971\n",
      "Epoch [175/2000], Loss: 0.3793, Val Loss: 0.3967\n",
      "Epoch [176/2000], Loss: 0.3789, Val Loss: 0.3963\n",
      "Epoch [177/2000], Loss: 0.3785, Val Loss: 0.3960\n",
      "Epoch [178/2000], Loss: 0.3781, Val Loss: 0.3956\n",
      "Epoch [179/2000], Loss: 0.3777, Val Loss: 0.3952\n",
      "Epoch [180/2000], Loss: 0.3773, Val Loss: 0.3948\n",
      "Epoch [181/2000], Loss: 0.3769, Val Loss: 0.3945\n",
      "Epoch [182/2000], Loss: 0.3765, Val Loss: 0.3941\n",
      "Epoch [183/2000], Loss: 0.3761, Val Loss: 0.3937\n",
      "Epoch [184/2000], Loss: 0.3757, Val Loss: 0.3934\n",
      "Epoch [185/2000], Loss: 0.3753, Val Loss: 0.3930\n",
      "Epoch [186/2000], Loss: 0.3749, Val Loss: 0.3926\n",
      "Epoch [187/2000], Loss: 0.3746, Val Loss: 0.3922\n",
      "Epoch [188/2000], Loss: 0.3742, Val Loss: 0.3918\n",
      "Epoch [189/2000], Loss: 0.3738, Val Loss: 0.3914\n",
      "Epoch [190/2000], Loss: 0.3734, Val Loss: 0.3910\n",
      "Epoch [191/2000], Loss: 0.3730, Val Loss: 0.3906\n",
      "Epoch [192/2000], Loss: 0.3726, Val Loss: 0.3902\n",
      "Epoch [193/2000], Loss: 0.3722, Val Loss: 0.3898\n",
      "Epoch [194/2000], Loss: 0.3719, Val Loss: 0.3893\n",
      "Epoch [195/2000], Loss: 0.3715, Val Loss: 0.3889\n",
      "Epoch [196/2000], Loss: 0.3711, Val Loss: 0.3886\n",
      "Epoch [197/2000], Loss: 0.3707, Val Loss: 0.3882\n",
      "Epoch [198/2000], Loss: 0.3704, Val Loss: 0.3878\n",
      "Epoch [199/2000], Loss: 0.3700, Val Loss: 0.3874\n",
      "Epoch [200/2000], Loss: 0.3696, Val Loss: 0.3870\n",
      "Epoch [201/2000], Loss: 0.3692, Val Loss: 0.3866\n",
      "Epoch [202/2000], Loss: 0.3689, Val Loss: 0.3862\n",
      "Epoch [203/2000], Loss: 0.3685, Val Loss: 0.3858\n",
      "Epoch [204/2000], Loss: 0.3681, Val Loss: 0.3854\n",
      "Epoch [205/2000], Loss: 0.3677, Val Loss: 0.3850\n",
      "Epoch [206/2000], Loss: 0.3674, Val Loss: 0.3846\n",
      "Epoch [207/2000], Loss: 0.3670, Val Loss: 0.3842\n",
      "Epoch [208/2000], Loss: 0.3666, Val Loss: 0.3838\n",
      "Epoch [209/2000], Loss: 0.3663, Val Loss: 0.3834\n",
      "Epoch [210/2000], Loss: 0.3659, Val Loss: 0.3830\n",
      "Epoch [211/2000], Loss: 0.3655, Val Loss: 0.3826\n",
      "Epoch [212/2000], Loss: 0.3652, Val Loss: 0.3821\n",
      "Epoch [213/2000], Loss: 0.3648, Val Loss: 0.3817\n",
      "Epoch [214/2000], Loss: 0.3644, Val Loss: 0.3813\n",
      "Epoch [215/2000], Loss: 0.3641, Val Loss: 0.3809\n",
      "Epoch [216/2000], Loss: 0.3637, Val Loss: 0.3805\n",
      "Epoch [217/2000], Loss: 0.3633, Val Loss: 0.3801\n",
      "Epoch [218/2000], Loss: 0.3630, Val Loss: 0.3797\n",
      "Epoch [219/2000], Loss: 0.3626, Val Loss: 0.3793\n",
      "Epoch [220/2000], Loss: 0.3622, Val Loss: 0.3789\n",
      "Epoch [221/2000], Loss: 0.3619, Val Loss: 0.3785\n",
      "Epoch [222/2000], Loss: 0.3615, Val Loss: 0.3782\n",
      "Epoch [223/2000], Loss: 0.3611, Val Loss: 0.3778\n",
      "Epoch [224/2000], Loss: 0.3608, Val Loss: 0.3774\n",
      "Epoch [225/2000], Loss: 0.3604, Val Loss: 0.3770\n",
      "Epoch [226/2000], Loss: 0.3600, Val Loss: 0.3766\n",
      "Epoch [227/2000], Loss: 0.3597, Val Loss: 0.3762\n",
      "Epoch [228/2000], Loss: 0.3593, Val Loss: 0.3758\n",
      "Epoch [229/2000], Loss: 0.3589, Val Loss: 0.3754\n",
      "Epoch [230/2000], Loss: 0.3586, Val Loss: 0.3750\n",
      "Epoch [231/2000], Loss: 0.3582, Val Loss: 0.3746\n",
      "Epoch [232/2000], Loss: 0.3578, Val Loss: 0.3743\n",
      "Epoch [233/2000], Loss: 0.3575, Val Loss: 0.3739\n",
      "Epoch [234/2000], Loss: 0.3571, Val Loss: 0.3735\n",
      "Epoch [235/2000], Loss: 0.3567, Val Loss: 0.3731\n",
      "Epoch [236/2000], Loss: 0.3564, Val Loss: 0.3727\n",
      "Epoch [237/2000], Loss: 0.3560, Val Loss: 0.3723\n",
      "Epoch [238/2000], Loss: 0.3557, Val Loss: 0.3718\n",
      "Epoch [239/2000], Loss: 0.3553, Val Loss: 0.3714\n",
      "Epoch [240/2000], Loss: 0.3549, Val Loss: 0.3710\n",
      "Epoch [241/2000], Loss: 0.3546, Val Loss: 0.3706\n",
      "Epoch [242/2000], Loss: 0.3542, Val Loss: 0.3702\n",
      "Epoch [243/2000], Loss: 0.3538, Val Loss: 0.3698\n",
      "Epoch [244/2000], Loss: 0.3535, Val Loss: 0.3694\n",
      "Epoch [245/2000], Loss: 0.3531, Val Loss: 0.3690\n",
      "Epoch [246/2000], Loss: 0.3527, Val Loss: 0.3686\n",
      "Epoch [247/2000], Loss: 0.3524, Val Loss: 0.3682\n",
      "Epoch [248/2000], Loss: 0.3520, Val Loss: 0.3678\n",
      "Epoch [249/2000], Loss: 0.3517, Val Loss: 0.3674\n",
      "Epoch [250/2000], Loss: 0.3513, Val Loss: 0.3670\n",
      "Epoch [251/2000], Loss: 0.3509, Val Loss: 0.3666\n",
      "Epoch [252/2000], Loss: 0.3506, Val Loss: 0.3661\n",
      "Epoch [253/2000], Loss: 0.3502, Val Loss: 0.3657\n",
      "Epoch [254/2000], Loss: 0.3499, Val Loss: 0.3653\n",
      "Epoch [255/2000], Loss: 0.3495, Val Loss: 0.3649\n",
      "Epoch [256/2000], Loss: 0.3491, Val Loss: 0.3644\n",
      "Epoch [257/2000], Loss: 0.3488, Val Loss: 0.3640\n",
      "Epoch [258/2000], Loss: 0.3484, Val Loss: 0.3636\n",
      "Epoch [259/2000], Loss: 0.3481, Val Loss: 0.3632\n",
      "Epoch [260/2000], Loss: 0.3477, Val Loss: 0.3628\n",
      "Epoch [261/2000], Loss: 0.3474, Val Loss: 0.3623\n",
      "Epoch [262/2000], Loss: 0.3470, Val Loss: 0.3619\n",
      "Epoch [263/2000], Loss: 0.3467, Val Loss: 0.3615\n",
      "Epoch [264/2000], Loss: 0.3463, Val Loss: 0.3611\n",
      "Epoch [265/2000], Loss: 0.3460, Val Loss: 0.3606\n",
      "Epoch [266/2000], Loss: 0.3456, Val Loss: 0.3602\n",
      "Epoch [267/2000], Loss: 0.3453, Val Loss: 0.3598\n",
      "Epoch [268/2000], Loss: 0.3449, Val Loss: 0.3594\n",
      "Epoch [269/2000], Loss: 0.3446, Val Loss: 0.3589\n",
      "Epoch [270/2000], Loss: 0.3443, Val Loss: 0.3585\n",
      "Epoch [271/2000], Loss: 0.3439, Val Loss: 0.3582\n",
      "Epoch [272/2000], Loss: 0.3436, Val Loss: 0.3578\n",
      "Epoch [273/2000], Loss: 0.3432, Val Loss: 0.3574\n",
      "Epoch [274/2000], Loss: 0.3429, Val Loss: 0.3570\n",
      "Epoch [275/2000], Loss: 0.3426, Val Loss: 0.3566\n",
      "Epoch [276/2000], Loss: 0.3423, Val Loss: 0.3562\n",
      "Epoch [277/2000], Loss: 0.3419, Val Loss: 0.3557\n",
      "Epoch [278/2000], Loss: 0.3416, Val Loss: 0.3553\n",
      "Epoch [279/2000], Loss: 0.3413, Val Loss: 0.3549\n",
      "Epoch [280/2000], Loss: 0.3410, Val Loss: 0.3545\n",
      "Epoch [281/2000], Loss: 0.3406, Val Loss: 0.3541\n",
      "Epoch [282/2000], Loss: 0.3403, Val Loss: 0.3537\n",
      "Epoch [283/2000], Loss: 0.3400, Val Loss: 0.3533\n",
      "Epoch [284/2000], Loss: 0.3397, Val Loss: 0.3529\n",
      "Epoch [285/2000], Loss: 0.3394, Val Loss: 0.3525\n",
      "Epoch [286/2000], Loss: 0.3391, Val Loss: 0.3522\n",
      "Epoch [287/2000], Loss: 0.3388, Val Loss: 0.3518\n",
      "Epoch [288/2000], Loss: 0.3384, Val Loss: 0.3514\n",
      "Epoch [289/2000], Loss: 0.3381, Val Loss: 0.3510\n",
      "Epoch [290/2000], Loss: 0.3378, Val Loss: 0.3507\n",
      "Epoch [291/2000], Loss: 0.3375, Val Loss: 0.3503\n",
      "Epoch [292/2000], Loss: 0.3372, Val Loss: 0.3499\n",
      "Epoch [293/2000], Loss: 0.3369, Val Loss: 0.3495\n",
      "Epoch [294/2000], Loss: 0.3366, Val Loss: 0.3492\n",
      "Epoch [295/2000], Loss: 0.3363, Val Loss: 0.3488\n",
      "Epoch [296/2000], Loss: 0.3360, Val Loss: 0.3484\n",
      "Epoch [297/2000], Loss: 0.3357, Val Loss: 0.3481\n",
      "Epoch [298/2000], Loss: 0.3354, Val Loss: 0.3477\n",
      "Epoch [299/2000], Loss: 0.3351, Val Loss: 0.3474\n",
      "Epoch [300/2000], Loss: 0.3349, Val Loss: 0.3470\n",
      "Epoch [301/2000], Loss: 0.3346, Val Loss: 0.3466\n",
      "Epoch [302/2000], Loss: 0.3343, Val Loss: 0.3462\n",
      "Epoch [303/2000], Loss: 0.3340, Val Loss: 0.3458\n",
      "Epoch [304/2000], Loss: 0.3337, Val Loss: 0.3454\n",
      "Epoch [305/2000], Loss: 0.3334, Val Loss: 0.3451\n",
      "Epoch [306/2000], Loss: 0.3332, Val Loss: 0.3447\n",
      "Epoch [307/2000], Loss: 0.3329, Val Loss: 0.3443\n",
      "Epoch [308/2000], Loss: 0.3326, Val Loss: 0.3439\n",
      "Epoch [309/2000], Loss: 0.3323, Val Loss: 0.3436\n",
      "Epoch [310/2000], Loss: 0.3321, Val Loss: 0.3432\n",
      "Epoch [311/2000], Loss: 0.3318, Val Loss: 0.3428\n",
      "Epoch [312/2000], Loss: 0.3315, Val Loss: 0.3424\n",
      "Epoch [313/2000], Loss: 0.3312, Val Loss: 0.3420\n",
      "Epoch [314/2000], Loss: 0.3310, Val Loss: 0.3417\n",
      "Epoch [315/2000], Loss: 0.3307, Val Loss: 0.3413\n",
      "Epoch [316/2000], Loss: 0.3305, Val Loss: 0.3410\n",
      "Epoch [317/2000], Loss: 0.3302, Val Loss: 0.3406\n",
      "Epoch [318/2000], Loss: 0.3299, Val Loss: 0.3403\n",
      "Epoch [319/2000], Loss: 0.3297, Val Loss: 0.3400\n",
      "Epoch [320/2000], Loss: 0.3294, Val Loss: 0.3397\n",
      "Epoch [321/2000], Loss: 0.3292, Val Loss: 0.3393\n",
      "Epoch [322/2000], Loss: 0.3289, Val Loss: 0.3390\n",
      "Epoch [323/2000], Loss: 0.3287, Val Loss: 0.3387\n",
      "Epoch [324/2000], Loss: 0.3284, Val Loss: 0.3384\n",
      "Epoch [325/2000], Loss: 0.3282, Val Loss: 0.3381\n",
      "Epoch [326/2000], Loss: 0.3279, Val Loss: 0.3378\n",
      "Epoch [327/2000], Loss: 0.3277, Val Loss: 0.3375\n",
      "Epoch [328/2000], Loss: 0.3275, Val Loss: 0.3373\n",
      "Epoch [329/2000], Loss: 0.3272, Val Loss: 0.3370\n",
      "Epoch [330/2000], Loss: 0.3270, Val Loss: 0.3367\n",
      "Epoch [331/2000], Loss: 0.3268, Val Loss: 0.3364\n",
      "Epoch [332/2000], Loss: 0.3265, Val Loss: 0.3361\n",
      "Epoch [333/2000], Loss: 0.3263, Val Loss: 0.3358\n",
      "Epoch [334/2000], Loss: 0.3261, Val Loss: 0.3355\n",
      "Epoch [335/2000], Loss: 0.3258, Val Loss: 0.3352\n",
      "Epoch [336/2000], Loss: 0.3256, Val Loss: 0.3349\n",
      "Epoch [337/2000], Loss: 0.3254, Val Loss: 0.3347\n",
      "Epoch [338/2000], Loss: 0.3251, Val Loss: 0.3344\n",
      "Epoch [339/2000], Loss: 0.3249, Val Loss: 0.3341\n",
      "Epoch [340/2000], Loss: 0.3247, Val Loss: 0.3338\n",
      "Epoch [341/2000], Loss: 0.3245, Val Loss: 0.3335\n",
      "Epoch [342/2000], Loss: 0.3242, Val Loss: 0.3333\n",
      "Epoch [343/2000], Loss: 0.3240, Val Loss: 0.3330\n",
      "Epoch [344/2000], Loss: 0.3238, Val Loss: 0.3328\n",
      "Epoch [345/2000], Loss: 0.3236, Val Loss: 0.3325\n",
      "Epoch [346/2000], Loss: 0.3234, Val Loss: 0.3322\n",
      "Epoch [347/2000], Loss: 0.3232, Val Loss: 0.3320\n",
      "Epoch [348/2000], Loss: 0.3229, Val Loss: 0.3317\n",
      "Epoch [349/2000], Loss: 0.3227, Val Loss: 0.3314\n",
      "Epoch [350/2000], Loss: 0.3225, Val Loss: 0.3311\n",
      "Epoch [351/2000], Loss: 0.3223, Val Loss: 0.3309\n",
      "Epoch [352/2000], Loss: 0.3221, Val Loss: 0.3306\n",
      "Epoch [353/2000], Loss: 0.3219, Val Loss: 0.3304\n",
      "Epoch [354/2000], Loss: 0.3217, Val Loss: 0.3301\n",
      "Epoch [355/2000], Loss: 0.3215, Val Loss: 0.3299\n",
      "Epoch [356/2000], Loss: 0.3213, Val Loss: 0.3297\n",
      "Epoch [357/2000], Loss: 0.3211, Val Loss: 0.3294\n",
      "Epoch [358/2000], Loss: 0.3209, Val Loss: 0.3292\n",
      "Epoch [359/2000], Loss: 0.3207, Val Loss: 0.3290\n",
      "Epoch [360/2000], Loss: 0.3205, Val Loss: 0.3287\n",
      "Epoch [361/2000], Loss: 0.3203, Val Loss: 0.3285\n",
      "Epoch [362/2000], Loss: 0.3201, Val Loss: 0.3283\n",
      "Epoch [363/2000], Loss: 0.3199, Val Loss: 0.3281\n",
      "Epoch [364/2000], Loss: 0.3197, Val Loss: 0.3279\n",
      "Epoch [365/2000], Loss: 0.3195, Val Loss: 0.3277\n",
      "Epoch [366/2000], Loss: 0.3193, Val Loss: 0.3275\n",
      "Epoch [367/2000], Loss: 0.3191, Val Loss: 0.3274\n",
      "Epoch [368/2000], Loss: 0.3189, Val Loss: 0.3272\n",
      "Epoch [369/2000], Loss: 0.3187, Val Loss: 0.3270\n",
      "Epoch [370/2000], Loss: 0.3186, Val Loss: 0.3268\n",
      "Epoch [371/2000], Loss: 0.3184, Val Loss: 0.3266\n",
      "Epoch [372/2000], Loss: 0.3182, Val Loss: 0.3264\n",
      "Epoch [373/2000], Loss: 0.3180, Val Loss: 0.3262\n",
      "Epoch [374/2000], Loss: 0.3178, Val Loss: 0.3260\n",
      "Epoch [375/2000], Loss: 0.3176, Val Loss: 0.3258\n",
      "Epoch [376/2000], Loss: 0.3174, Val Loss: 0.3256\n",
      "Epoch [377/2000], Loss: 0.3173, Val Loss: 0.3254\n",
      "Epoch [378/2000], Loss: 0.3171, Val Loss: 0.3252\n",
      "Epoch [379/2000], Loss: 0.3169, Val Loss: 0.3250\n",
      "Epoch [380/2000], Loss: 0.3167, Val Loss: 0.3249\n",
      "Epoch [381/2000], Loss: 0.3165, Val Loss: 0.3247\n",
      "Epoch [382/2000], Loss: 0.3163, Val Loss: 0.3245\n",
      "Epoch [383/2000], Loss: 0.3162, Val Loss: 0.3243\n",
      "Epoch [384/2000], Loss: 0.3160, Val Loss: 0.3241\n",
      "Epoch [385/2000], Loss: 0.3158, Val Loss: 0.3239\n",
      "Epoch [386/2000], Loss: 0.3156, Val Loss: 0.3237\n",
      "Epoch [387/2000], Loss: 0.3155, Val Loss: 0.3235\n",
      "Epoch [388/2000], Loss: 0.3153, Val Loss: 0.3234\n",
      "Epoch [389/2000], Loss: 0.3151, Val Loss: 0.3232\n",
      "Epoch [390/2000], Loss: 0.3149, Val Loss: 0.3230\n",
      "Epoch [391/2000], Loss: 0.3148, Val Loss: 0.3229\n",
      "Epoch [392/2000], Loss: 0.3146, Val Loss: 0.3227\n",
      "Epoch [393/2000], Loss: 0.3144, Val Loss: 0.3226\n",
      "Epoch [394/2000], Loss: 0.3143, Val Loss: 0.3224\n",
      "Epoch [395/2000], Loss: 0.3141, Val Loss: 0.3223\n",
      "Epoch [396/2000], Loss: 0.3139, Val Loss: 0.3221\n",
      "Epoch [397/2000], Loss: 0.3138, Val Loss: 0.3220\n",
      "Epoch [398/2000], Loss: 0.3136, Val Loss: 0.3218\n",
      "Epoch [399/2000], Loss: 0.3134, Val Loss: 0.3217\n",
      "Epoch [400/2000], Loss: 0.3133, Val Loss: 0.3215\n",
      "Epoch [401/2000], Loss: 0.3131, Val Loss: 0.3214\n",
      "Epoch [402/2000], Loss: 0.3129, Val Loss: 0.3213\n",
      "Epoch [403/2000], Loss: 0.3128, Val Loss: 0.3211\n",
      "Epoch [404/2000], Loss: 0.3126, Val Loss: 0.3210\n",
      "Epoch [405/2000], Loss: 0.3124, Val Loss: 0.3209\n",
      "Epoch [406/2000], Loss: 0.3123, Val Loss: 0.3208\n",
      "Epoch [407/2000], Loss: 0.3121, Val Loss: 0.3206\n",
      "Epoch [408/2000], Loss: 0.3120, Val Loss: 0.3205\n",
      "Epoch [409/2000], Loss: 0.3118, Val Loss: 0.3204\n",
      "Epoch [410/2000], Loss: 0.3116, Val Loss: 0.3203\n",
      "Epoch [411/2000], Loss: 0.3115, Val Loss: 0.3202\n",
      "Epoch [412/2000], Loss: 0.3113, Val Loss: 0.3200\n",
      "Epoch [413/2000], Loss: 0.3112, Val Loss: 0.3199\n",
      "Epoch [414/2000], Loss: 0.3110, Val Loss: 0.3198\n",
      "Epoch [415/2000], Loss: 0.3109, Val Loss: 0.3196\n",
      "Epoch [416/2000], Loss: 0.3107, Val Loss: 0.3195\n",
      "Epoch [417/2000], Loss: 0.3106, Val Loss: 0.3194\n",
      "Epoch [418/2000], Loss: 0.3104, Val Loss: 0.3193\n",
      "Epoch [419/2000], Loss: 0.3102, Val Loss: 0.3191\n",
      "Epoch [420/2000], Loss: 0.3101, Val Loss: 0.3190\n",
      "Epoch [421/2000], Loss: 0.3099, Val Loss: 0.3188\n",
      "Epoch [422/2000], Loss: 0.3098, Val Loss: 0.3187\n",
      "Epoch [423/2000], Loss: 0.3096, Val Loss: 0.3186\n",
      "Epoch [424/2000], Loss: 0.3095, Val Loss: 0.3184\n",
      "Epoch [425/2000], Loss: 0.3093, Val Loss: 0.3183\n",
      "Epoch [426/2000], Loss: 0.3092, Val Loss: 0.3182\n",
      "Epoch [427/2000], Loss: 0.3090, Val Loss: 0.3181\n",
      "Epoch [428/2000], Loss: 0.3089, Val Loss: 0.3179\n",
      "Epoch [429/2000], Loss: 0.3087, Val Loss: 0.3178\n",
      "Epoch [430/2000], Loss: 0.3086, Val Loss: 0.3177\n",
      "Epoch [431/2000], Loss: 0.3084, Val Loss: 0.3175\n",
      "Epoch [432/2000], Loss: 0.3083, Val Loss: 0.3174\n",
      "Epoch [433/2000], Loss: 0.3082, Val Loss: 0.3173\n",
      "Epoch [434/2000], Loss: 0.3080, Val Loss: 0.3172\n",
      "Epoch [435/2000], Loss: 0.3079, Val Loss: 0.3170\n",
      "Epoch [436/2000], Loss: 0.3077, Val Loss: 0.3169\n",
      "Epoch [437/2000], Loss: 0.3076, Val Loss: 0.3168\n",
      "Epoch [438/2000], Loss: 0.3074, Val Loss: 0.3167\n",
      "Epoch [439/2000], Loss: 0.3073, Val Loss: 0.3165\n",
      "Epoch [440/2000], Loss: 0.3072, Val Loss: 0.3164\n",
      "Epoch [441/2000], Loss: 0.3070, Val Loss: 0.3164\n",
      "Epoch [442/2000], Loss: 0.3069, Val Loss: 0.3163\n",
      "Epoch [443/2000], Loss: 0.3067, Val Loss: 0.3162\n",
      "Epoch [444/2000], Loss: 0.3066, Val Loss: 0.3160\n",
      "Epoch [445/2000], Loss: 0.3065, Val Loss: 0.3159\n",
      "Epoch [446/2000], Loss: 0.3063, Val Loss: 0.3158\n",
      "Epoch [447/2000], Loss: 0.3062, Val Loss: 0.3157\n",
      "Epoch [448/2000], Loss: 0.3060, Val Loss: 0.3156\n",
      "Epoch [449/2000], Loss: 0.3059, Val Loss: 0.3156\n",
      "Epoch [450/2000], Loss: 0.3058, Val Loss: 0.3155\n",
      "Epoch [451/2000], Loss: 0.3056, Val Loss: 0.3154\n",
      "Epoch [452/2000], Loss: 0.3055, Val Loss: 0.3153\n",
      "Epoch [453/2000], Loss: 0.3054, Val Loss: 0.3152\n",
      "Epoch [454/2000], Loss: 0.3052, Val Loss: 0.3151\n",
      "Epoch [455/2000], Loss: 0.3051, Val Loss: 0.3150\n",
      "Epoch [456/2000], Loss: 0.3050, Val Loss: 0.3150\n",
      "Epoch [457/2000], Loss: 0.3048, Val Loss: 0.3149\n",
      "Epoch [458/2000], Loss: 0.3047, Val Loss: 0.3148\n",
      "Epoch [459/2000], Loss: 0.3046, Val Loss: 0.3147\n",
      "Epoch [460/2000], Loss: 0.3044, Val Loss: 0.3146\n",
      "Epoch [461/2000], Loss: 0.3043, Val Loss: 0.3145\n",
      "Epoch [462/2000], Loss: 0.3042, Val Loss: 0.3144\n",
      "Epoch [463/2000], Loss: 0.3040, Val Loss: 0.3143\n",
      "Epoch [464/2000], Loss: 0.3039, Val Loss: 0.3143\n",
      "Epoch [465/2000], Loss: 0.3038, Val Loss: 0.3142\n",
      "Epoch [466/2000], Loss: 0.3036, Val Loss: 0.3141\n",
      "Epoch [467/2000], Loss: 0.3035, Val Loss: 0.3140\n",
      "Epoch [468/2000], Loss: 0.3034, Val Loss: 0.3139\n",
      "Epoch [469/2000], Loss: 0.3033, Val Loss: 0.3138\n",
      "Epoch [470/2000], Loss: 0.3031, Val Loss: 0.3137\n",
      "Epoch [471/2000], Loss: 0.3030, Val Loss: 0.3136\n",
      "Epoch [472/2000], Loss: 0.3029, Val Loss: 0.3135\n",
      "Epoch [473/2000], Loss: 0.3027, Val Loss: 0.3134\n",
      "Epoch [474/2000], Loss: 0.3026, Val Loss: 0.3134\n",
      "Epoch [475/2000], Loss: 0.3025, Val Loss: 0.3133\n",
      "Epoch [476/2000], Loss: 0.3024, Val Loss: 0.3132\n",
      "Epoch [477/2000], Loss: 0.3022, Val Loss: 0.3131\n",
      "Epoch [478/2000], Loss: 0.3021, Val Loss: 0.3130\n",
      "Epoch [479/2000], Loss: 0.3020, Val Loss: 0.3130\n",
      "Epoch [480/2000], Loss: 0.3019, Val Loss: 0.3129\n",
      "Epoch [481/2000], Loss: 0.3017, Val Loss: 0.3128\n",
      "Epoch [482/2000], Loss: 0.3016, Val Loss: 0.3127\n",
      "Epoch [483/2000], Loss: 0.3015, Val Loss: 0.3127\n",
      "Epoch [484/2000], Loss: 0.3014, Val Loss: 0.3126\n",
      "Epoch [485/2000], Loss: 0.3012, Val Loss: 0.3125\n",
      "Epoch [486/2000], Loss: 0.3011, Val Loss: 0.3124\n",
      "Epoch [487/2000], Loss: 0.3010, Val Loss: 0.3124\n",
      "Epoch [488/2000], Loss: 0.3009, Val Loss: 0.3123\n",
      "Epoch [489/2000], Loss: 0.3007, Val Loss: 0.3122\n",
      "Epoch [490/2000], Loss: 0.3006, Val Loss: 0.3122\n",
      "Epoch [491/2000], Loss: 0.3005, Val Loss: 0.3121\n",
      "Epoch [492/2000], Loss: 0.3004, Val Loss: 0.3120\n",
      "Epoch [493/2000], Loss: 0.3003, Val Loss: 0.3119\n",
      "Epoch [494/2000], Loss: 0.3001, Val Loss: 0.3118\n",
      "Epoch [495/2000], Loss: 0.3000, Val Loss: 0.3117\n",
      "Epoch [496/2000], Loss: 0.2999, Val Loss: 0.3116\n",
      "Epoch [497/2000], Loss: 0.2998, Val Loss: 0.3116\n",
      "Epoch [498/2000], Loss: 0.2997, Val Loss: 0.3115\n",
      "Epoch [499/2000], Loss: 0.2996, Val Loss: 0.3114\n",
      "Epoch [500/2000], Loss: 0.2994, Val Loss: 0.3113\n",
      "Epoch [501/2000], Loss: 0.2993, Val Loss: 0.3113\n",
      "Epoch [502/2000], Loss: 0.2992, Val Loss: 0.3112\n",
      "Epoch [503/2000], Loss: 0.2991, Val Loss: 0.3112\n",
      "Epoch [504/2000], Loss: 0.2990, Val Loss: 0.3111\n",
      "Epoch [505/2000], Loss: 0.2988, Val Loss: 0.3110\n",
      "Epoch [506/2000], Loss: 0.2987, Val Loss: 0.3110\n",
      "Epoch [507/2000], Loss: 0.2986, Val Loss: 0.3109\n",
      "Epoch [508/2000], Loss: 0.2985, Val Loss: 0.3108\n",
      "Epoch [509/2000], Loss: 0.2984, Val Loss: 0.3108\n",
      "Epoch [510/2000], Loss: 0.2983, Val Loss: 0.3107\n",
      "Epoch [511/2000], Loss: 0.2982, Val Loss: 0.3106\n",
      "Epoch [512/2000], Loss: 0.2980, Val Loss: 0.3106\n",
      "Epoch [513/2000], Loss: 0.2979, Val Loss: 0.3105\n",
      "Epoch [514/2000], Loss: 0.2978, Val Loss: 0.3104\n",
      "Epoch [515/2000], Loss: 0.2977, Val Loss: 0.3104\n",
      "Epoch [516/2000], Loss: 0.2976, Val Loss: 0.3103\n",
      "Epoch [517/2000], Loss: 0.2975, Val Loss: 0.3102\n",
      "Epoch [518/2000], Loss: 0.2974, Val Loss: 0.3101\n",
      "Epoch [519/2000], Loss: 0.2972, Val Loss: 0.3100\n",
      "Epoch [520/2000], Loss: 0.2971, Val Loss: 0.3099\n",
      "Epoch [521/2000], Loss: 0.2970, Val Loss: 0.3099\n",
      "Epoch [522/2000], Loss: 0.2969, Val Loss: 0.3098\n",
      "Epoch [523/2000], Loss: 0.2968, Val Loss: 0.3097\n",
      "Epoch [524/2000], Loss: 0.2967, Val Loss: 0.3096\n",
      "Epoch [525/2000], Loss: 0.2966, Val Loss: 0.3096\n",
      "Epoch [526/2000], Loss: 0.2965, Val Loss: 0.3095\n",
      "Epoch [527/2000], Loss: 0.2964, Val Loss: 0.3094\n",
      "Epoch [528/2000], Loss: 0.2962, Val Loss: 0.3093\n",
      "Epoch [529/2000], Loss: 0.2961, Val Loss: 0.3093\n",
      "Epoch [530/2000], Loss: 0.2960, Val Loss: 0.3092\n",
      "Epoch [531/2000], Loss: 0.2959, Val Loss: 0.3091\n",
      "Epoch [532/2000], Loss: 0.2958, Val Loss: 0.3090\n",
      "Epoch [533/2000], Loss: 0.2957, Val Loss: 0.3089\n",
      "Epoch [534/2000], Loss: 0.2956, Val Loss: 0.3089\n",
      "Epoch [535/2000], Loss: 0.2955, Val Loss: 0.3088\n",
      "Epoch [536/2000], Loss: 0.2954, Val Loss: 0.3088\n",
      "Epoch [537/2000], Loss: 0.2953, Val Loss: 0.3087\n",
      "Epoch [538/2000], Loss: 0.2952, Val Loss: 0.3086\n",
      "Epoch [539/2000], Loss: 0.2950, Val Loss: 0.3086\n",
      "Epoch [540/2000], Loss: 0.2949, Val Loss: 0.3085\n",
      "Epoch [541/2000], Loss: 0.2948, Val Loss: 0.3084\n",
      "Epoch [542/2000], Loss: 0.2947, Val Loss: 0.3084\n",
      "Epoch [543/2000], Loss: 0.2946, Val Loss: 0.3083\n",
      "Epoch [544/2000], Loss: 0.2945, Val Loss: 0.3083\n",
      "Epoch [545/2000], Loss: 0.2944, Val Loss: 0.3082\n",
      "Epoch [546/2000], Loss: 0.2943, Val Loss: 0.3081\n",
      "Epoch [547/2000], Loss: 0.2942, Val Loss: 0.3080\n",
      "Epoch [548/2000], Loss: 0.2941, Val Loss: 0.3080\n",
      "Epoch [549/2000], Loss: 0.2940, Val Loss: 0.3079\n",
      "Epoch [550/2000], Loss: 0.2939, Val Loss: 0.3078\n",
      "Epoch [551/2000], Loss: 0.2938, Val Loss: 0.3077\n",
      "Epoch [552/2000], Loss: 0.2937, Val Loss: 0.3076\n",
      "Epoch [553/2000], Loss: 0.2936, Val Loss: 0.3075\n",
      "Epoch [554/2000], Loss: 0.2935, Val Loss: 0.3075\n",
      "Epoch [555/2000], Loss: 0.2934, Val Loss: 0.3074\n",
      "Epoch [556/2000], Loss: 0.2933, Val Loss: 0.3073\n",
      "Epoch [557/2000], Loss: 0.2932, Val Loss: 0.3072\n",
      "Epoch [558/2000], Loss: 0.2931, Val Loss: 0.3072\n",
      "Epoch [559/2000], Loss: 0.2930, Val Loss: 0.3071\n",
      "Epoch [560/2000], Loss: 0.2929, Val Loss: 0.3070\n",
      "Epoch [561/2000], Loss: 0.2927, Val Loss: 0.3069\n",
      "Epoch [562/2000], Loss: 0.2926, Val Loss: 0.3068\n",
      "Epoch [563/2000], Loss: 0.2925, Val Loss: 0.3067\n",
      "Epoch [564/2000], Loss: 0.2924, Val Loss: 0.3067\n",
      "Epoch [565/2000], Loss: 0.2923, Val Loss: 0.3066\n",
      "Epoch [566/2000], Loss: 0.2922, Val Loss: 0.3065\n",
      "Epoch [567/2000], Loss: 0.2921, Val Loss: 0.3064\n",
      "Epoch [568/2000], Loss: 0.2921, Val Loss: 0.3063\n",
      "Epoch [569/2000], Loss: 0.2920, Val Loss: 0.3063\n",
      "Epoch [570/2000], Loss: 0.2919, Val Loss: 0.3062\n",
      "Epoch [571/2000], Loss: 0.2918, Val Loss: 0.3061\n",
      "Epoch [572/2000], Loss: 0.2917, Val Loss: 0.3060\n",
      "Epoch [573/2000], Loss: 0.2916, Val Loss: 0.3059\n",
      "Epoch [574/2000], Loss: 0.2915, Val Loss: 0.3059\n",
      "Epoch [575/2000], Loss: 0.2914, Val Loss: 0.3058\n",
      "Epoch [576/2000], Loss: 0.2913, Val Loss: 0.3057\n",
      "Epoch [577/2000], Loss: 0.2912, Val Loss: 0.3056\n",
      "Epoch [578/2000], Loss: 0.2911, Val Loss: 0.3055\n",
      "Epoch [579/2000], Loss: 0.2910, Val Loss: 0.3054\n",
      "Epoch [580/2000], Loss: 0.2909, Val Loss: 0.3054\n",
      "Epoch [581/2000], Loss: 0.2908, Val Loss: 0.3052\n",
      "Epoch [582/2000], Loss: 0.2907, Val Loss: 0.3051\n",
      "Epoch [583/2000], Loss: 0.2906, Val Loss: 0.3051\n",
      "Epoch [584/2000], Loss: 0.2905, Val Loss: 0.3050\n",
      "Epoch [585/2000], Loss: 0.2904, Val Loss: 0.3049\n",
      "Epoch [586/2000], Loss: 0.2903, Val Loss: 0.3048\n",
      "Epoch [587/2000], Loss: 0.2902, Val Loss: 0.3047\n",
      "Epoch [588/2000], Loss: 0.2901, Val Loss: 0.3046\n",
      "Epoch [589/2000], Loss: 0.2900, Val Loss: 0.3045\n",
      "Epoch [590/2000], Loss: 0.2899, Val Loss: 0.3044\n",
      "Epoch [591/2000], Loss: 0.2899, Val Loss: 0.3044\n",
      "Epoch [592/2000], Loss: 0.2898, Val Loss: 0.3043\n",
      "Epoch [593/2000], Loss: 0.2897, Val Loss: 0.3042\n",
      "Epoch [594/2000], Loss: 0.2896, Val Loss: 0.3042\n",
      "Epoch [595/2000], Loss: 0.2895, Val Loss: 0.3041\n",
      "Epoch [596/2000], Loss: 0.2894, Val Loss: 0.3040\n",
      "Epoch [597/2000], Loss: 0.2893, Val Loss: 0.3039\n",
      "Epoch [598/2000], Loss: 0.2892, Val Loss: 0.3039\n",
      "Epoch [599/2000], Loss: 0.2891, Val Loss: 0.3038\n",
      "Epoch [600/2000], Loss: 0.2890, Val Loss: 0.3038\n",
      "Epoch [601/2000], Loss: 0.2889, Val Loss: 0.3037\n",
      "Epoch [602/2000], Loss: 0.2888, Val Loss: 0.3037\n",
      "Epoch [603/2000], Loss: 0.2888, Val Loss: 0.3036\n",
      "Epoch [604/2000], Loss: 0.2887, Val Loss: 0.3035\n",
      "Epoch [605/2000], Loss: 0.2886, Val Loss: 0.3035\n",
      "Epoch [606/2000], Loss: 0.2885, Val Loss: 0.3034\n",
      "Epoch [607/2000], Loss: 0.2884, Val Loss: 0.3033\n",
      "Epoch [608/2000], Loss: 0.2883, Val Loss: 0.3033\n",
      "Epoch [609/2000], Loss: 0.2882, Val Loss: 0.3032\n",
      "Epoch [610/2000], Loss: 0.2881, Val Loss: 0.3031\n",
      "Epoch [611/2000], Loss: 0.2880, Val Loss: 0.3031\n",
      "Epoch [612/2000], Loss: 0.2880, Val Loss: 0.3030\n",
      "Epoch [613/2000], Loss: 0.2879, Val Loss: 0.3030\n",
      "Epoch [614/2000], Loss: 0.2878, Val Loss: 0.3029\n",
      "Epoch [615/2000], Loss: 0.2877, Val Loss: 0.3028\n",
      "Epoch [616/2000], Loss: 0.2876, Val Loss: 0.3027\n",
      "Epoch [617/2000], Loss: 0.2875, Val Loss: 0.3027\n",
      "Epoch [618/2000], Loss: 0.2874, Val Loss: 0.3026\n",
      "Epoch [619/2000], Loss: 0.2873, Val Loss: 0.3026\n",
      "Epoch [620/2000], Loss: 0.2872, Val Loss: 0.3025\n",
      "Epoch [621/2000], Loss: 0.2872, Val Loss: 0.3024\n",
      "Epoch [622/2000], Loss: 0.2871, Val Loss: 0.3023\n",
      "Epoch [623/2000], Loss: 0.2870, Val Loss: 0.3023\n",
      "Epoch [624/2000], Loss: 0.2869, Val Loss: 0.3022\n",
      "Epoch [625/2000], Loss: 0.2868, Val Loss: 0.3021\n",
      "Epoch [626/2000], Loss: 0.2867, Val Loss: 0.3021\n",
      "Epoch [627/2000], Loss: 0.2866, Val Loss: 0.3020\n",
      "Epoch [628/2000], Loss: 0.2865, Val Loss: 0.3019\n",
      "Epoch [629/2000], Loss: 0.2865, Val Loss: 0.3018\n",
      "Epoch [630/2000], Loss: 0.2864, Val Loss: 0.3017\n",
      "Epoch [631/2000], Loss: 0.2863, Val Loss: 0.3016\n",
      "Epoch [632/2000], Loss: 0.2862, Val Loss: 0.3016\n",
      "Epoch [633/2000], Loss: 0.2861, Val Loss: 0.3015\n",
      "Epoch [634/2000], Loss: 0.2860, Val Loss: 0.3014\n",
      "Epoch [635/2000], Loss: 0.2859, Val Loss: 0.3013\n",
      "Epoch [636/2000], Loss: 0.2859, Val Loss: 0.3013\n",
      "Epoch [637/2000], Loss: 0.2858, Val Loss: 0.3012\n",
      "Epoch [638/2000], Loss: 0.2857, Val Loss: 0.3011\n",
      "Epoch [639/2000], Loss: 0.2856, Val Loss: 0.3010\n",
      "Epoch [640/2000], Loss: 0.2855, Val Loss: 0.3010\n",
      "Epoch [641/2000], Loss: 0.2854, Val Loss: 0.3009\n",
      "Epoch [642/2000], Loss: 0.2853, Val Loss: 0.3009\n",
      "Epoch [643/2000], Loss: 0.2853, Val Loss: 0.3008\n",
      "Epoch [644/2000], Loss: 0.2852, Val Loss: 0.3007\n",
      "Epoch [645/2000], Loss: 0.2851, Val Loss: 0.3006\n",
      "Epoch [646/2000], Loss: 0.2850, Val Loss: 0.3006\n",
      "Epoch [647/2000], Loss: 0.2849, Val Loss: 0.3005\n",
      "Epoch [648/2000], Loss: 0.2848, Val Loss: 0.3005\n",
      "Epoch [649/2000], Loss: 0.2847, Val Loss: 0.3004\n",
      "Epoch [650/2000], Loss: 0.2847, Val Loss: 0.3004\n",
      "Epoch [651/2000], Loss: 0.2846, Val Loss: 0.3003\n",
      "Epoch [652/2000], Loss: 0.2845, Val Loss: 0.3002\n",
      "Epoch [653/2000], Loss: 0.2844, Val Loss: 0.3001\n",
      "Epoch [654/2000], Loss: 0.2843, Val Loss: 0.3000\n",
      "Epoch [655/2000], Loss: 0.2842, Val Loss: 0.2999\n",
      "Epoch [656/2000], Loss: 0.2842, Val Loss: 0.2999\n",
      "Epoch [657/2000], Loss: 0.2841, Val Loss: 0.2998\n",
      "Epoch [658/2000], Loss: 0.2840, Val Loss: 0.2998\n",
      "Epoch [659/2000], Loss: 0.2839, Val Loss: 0.2997\n",
      "Epoch [660/2000], Loss: 0.2838, Val Loss: 0.2996\n",
      "Epoch [661/2000], Loss: 0.2837, Val Loss: 0.2996\n",
      "Epoch [662/2000], Loss: 0.2837, Val Loss: 0.2995\n",
      "Epoch [663/2000], Loss: 0.2836, Val Loss: 0.2995\n",
      "Epoch [664/2000], Loss: 0.2835, Val Loss: 0.2994\n",
      "Epoch [665/2000], Loss: 0.2834, Val Loss: 0.2994\n",
      "Epoch [666/2000], Loss: 0.2833, Val Loss: 0.2993\n",
      "Epoch [667/2000], Loss: 0.2833, Val Loss: 0.2993\n",
      "Epoch [668/2000], Loss: 0.2832, Val Loss: 0.2992\n",
      "Epoch [669/2000], Loss: 0.2831, Val Loss: 0.2992\n",
      "Epoch [670/2000], Loss: 0.2830, Val Loss: 0.2991\n",
      "Epoch [671/2000], Loss: 0.2829, Val Loss: 0.2991\n",
      "Epoch [672/2000], Loss: 0.2828, Val Loss: 0.2990\n",
      "Epoch [673/2000], Loss: 0.2828, Val Loss: 0.2989\n",
      "Epoch [674/2000], Loss: 0.2827, Val Loss: 0.2989\n",
      "Epoch [675/2000], Loss: 0.2826, Val Loss: 0.2988\n",
      "Epoch [676/2000], Loss: 0.2825, Val Loss: 0.2987\n",
      "Epoch [677/2000], Loss: 0.2824, Val Loss: 0.2986\n",
      "Epoch [678/2000], Loss: 0.2824, Val Loss: 0.2986\n",
      "Epoch [679/2000], Loss: 0.2823, Val Loss: 0.2985\n",
      "Epoch [680/2000], Loss: 0.2822, Val Loss: 0.2985\n",
      "Epoch [681/2000], Loss: 0.2821, Val Loss: 0.2984\n",
      "Epoch [682/2000], Loss: 0.2820, Val Loss: 0.2983\n",
      "Epoch [683/2000], Loss: 0.2820, Val Loss: 0.2982\n",
      "Epoch [684/2000], Loss: 0.2819, Val Loss: 0.2981\n",
      "Epoch [685/2000], Loss: 0.2818, Val Loss: 0.2981\n",
      "Epoch [686/2000], Loss: 0.2817, Val Loss: 0.2980\n",
      "Epoch [687/2000], Loss: 0.2816, Val Loss: 0.2979\n",
      "Epoch [688/2000], Loss: 0.2816, Val Loss: 0.2978\n",
      "Epoch [689/2000], Loss: 0.2815, Val Loss: 0.2978\n",
      "Epoch [690/2000], Loss: 0.2814, Val Loss: 0.2977\n",
      "Epoch [691/2000], Loss: 0.2813, Val Loss: 0.2976\n",
      "Epoch [692/2000], Loss: 0.2813, Val Loss: 0.2975\n",
      "Epoch [693/2000], Loss: 0.2812, Val Loss: 0.2975\n",
      "Epoch [694/2000], Loss: 0.2811, Val Loss: 0.2974\n",
      "Epoch [695/2000], Loss: 0.2810, Val Loss: 0.2973\n",
      "Epoch [696/2000], Loss: 0.2810, Val Loss: 0.2973\n",
      "Epoch [697/2000], Loss: 0.2809, Val Loss: 0.2972\n",
      "Epoch [698/2000], Loss: 0.2808, Val Loss: 0.2971\n",
      "Epoch [699/2000], Loss: 0.2807, Val Loss: 0.2971\n",
      "Epoch [700/2000], Loss: 0.2806, Val Loss: 0.2970\n",
      "Epoch [701/2000], Loss: 0.2806, Val Loss: 0.2970\n",
      "Epoch [702/2000], Loss: 0.2805, Val Loss: 0.2969\n",
      "Epoch [703/2000], Loss: 0.2804, Val Loss: 0.2968\n",
      "Epoch [704/2000], Loss: 0.2803, Val Loss: 0.2968\n",
      "Epoch [705/2000], Loss: 0.2803, Val Loss: 0.2967\n",
      "Epoch [706/2000], Loss: 0.2802, Val Loss: 0.2967\n",
      "Epoch [707/2000], Loss: 0.2801, Val Loss: 0.2966\n",
      "Epoch [708/2000], Loss: 0.2800, Val Loss: 0.2966\n",
      "Epoch [709/2000], Loss: 0.2800, Val Loss: 0.2965\n",
      "Epoch [710/2000], Loss: 0.2799, Val Loss: 0.2964\n",
      "Epoch [711/2000], Loss: 0.2798, Val Loss: 0.2964\n",
      "Epoch [712/2000], Loss: 0.2797, Val Loss: 0.2963\n",
      "Epoch [713/2000], Loss: 0.2797, Val Loss: 0.2962\n",
      "Epoch [714/2000], Loss: 0.2796, Val Loss: 0.2962\n",
      "Epoch [715/2000], Loss: 0.2795, Val Loss: 0.2961\n",
      "Epoch [716/2000], Loss: 0.2795, Val Loss: 0.2961\n",
      "Epoch [717/2000], Loss: 0.2794, Val Loss: 0.2960\n",
      "Epoch [718/2000], Loss: 0.2793, Val Loss: 0.2959\n",
      "Epoch [719/2000], Loss: 0.2792, Val Loss: 0.2959\n",
      "Epoch [720/2000], Loss: 0.2792, Val Loss: 0.2959\n",
      "Epoch [721/2000], Loss: 0.2791, Val Loss: 0.2958\n",
      "Epoch [722/2000], Loss: 0.2790, Val Loss: 0.2958\n",
      "Epoch [723/2000], Loss: 0.2789, Val Loss: 0.2958\n",
      "Epoch [724/2000], Loss: 0.2789, Val Loss: 0.2957\n",
      "Epoch [725/2000], Loss: 0.2788, Val Loss: 0.2956\n",
      "Epoch [726/2000], Loss: 0.2787, Val Loss: 0.2955\n",
      "Epoch [727/2000], Loss: 0.2786, Val Loss: 0.2955\n",
      "Epoch [728/2000], Loss: 0.2786, Val Loss: 0.2954\n",
      "Epoch [729/2000], Loss: 0.2785, Val Loss: 0.2953\n",
      "Epoch [730/2000], Loss: 0.2784, Val Loss: 0.2952\n",
      "Epoch [731/2000], Loss: 0.2784, Val Loss: 0.2951\n",
      "Epoch [732/2000], Loss: 0.2783, Val Loss: 0.2951\n",
      "Epoch [733/2000], Loss: 0.2782, Val Loss: 0.2950\n",
      "Epoch [734/2000], Loss: 0.2781, Val Loss: 0.2950\n",
      "Epoch [735/2000], Loss: 0.2781, Val Loss: 0.2949\n",
      "Epoch [736/2000], Loss: 0.2780, Val Loss: 0.2949\n",
      "Epoch [737/2000], Loss: 0.2779, Val Loss: 0.2948\n",
      "Epoch [738/2000], Loss: 0.2779, Val Loss: 0.2948\n",
      "Epoch [739/2000], Loss: 0.2778, Val Loss: 0.2947\n",
      "Epoch [740/2000], Loss: 0.2777, Val Loss: 0.2947\n",
      "Epoch [741/2000], Loss: 0.2776, Val Loss: 0.2946\n",
      "Epoch [742/2000], Loss: 0.2776, Val Loss: 0.2945\n",
      "Epoch [743/2000], Loss: 0.2775, Val Loss: 0.2944\n",
      "Epoch [744/2000], Loss: 0.2774, Val Loss: 0.2944\n",
      "Epoch [745/2000], Loss: 0.2774, Val Loss: 0.2943\n",
      "Epoch [746/2000], Loss: 0.2773, Val Loss: 0.2943\n",
      "Epoch [747/2000], Loss: 0.2772, Val Loss: 0.2942\n",
      "Epoch [748/2000], Loss: 0.2772, Val Loss: 0.2941\n",
      "Epoch [749/2000], Loss: 0.2771, Val Loss: 0.2941\n",
      "Epoch [750/2000], Loss: 0.2770, Val Loss: 0.2940\n",
      "Epoch [751/2000], Loss: 0.2770, Val Loss: 0.2940\n",
      "Epoch [752/2000], Loss: 0.2769, Val Loss: 0.2939\n",
      "Epoch [753/2000], Loss: 0.2768, Val Loss: 0.2938\n",
      "Epoch [754/2000], Loss: 0.2768, Val Loss: 0.2938\n",
      "Epoch [755/2000], Loss: 0.2767, Val Loss: 0.2937\n",
      "Epoch [756/2000], Loss: 0.2766, Val Loss: 0.2936\n",
      "Epoch [757/2000], Loss: 0.2766, Val Loss: 0.2935\n",
      "Epoch [758/2000], Loss: 0.2765, Val Loss: 0.2935\n",
      "Epoch [759/2000], Loss: 0.2764, Val Loss: 0.2934\n",
      "Epoch [760/2000], Loss: 0.2764, Val Loss: 0.2933\n",
      "Epoch [761/2000], Loss: 0.2763, Val Loss: 0.2933\n",
      "Epoch [762/2000], Loss: 0.2762, Val Loss: 0.2932\n",
      "Epoch [763/2000], Loss: 0.2761, Val Loss: 0.2931\n",
      "Epoch [764/2000], Loss: 0.2761, Val Loss: 0.2930\n",
      "Epoch [765/2000], Loss: 0.2760, Val Loss: 0.2930\n",
      "Epoch [766/2000], Loss: 0.2760, Val Loss: 0.2929\n",
      "Epoch [767/2000], Loss: 0.2759, Val Loss: 0.2929\n",
      "Epoch [768/2000], Loss: 0.2758, Val Loss: 0.2929\n",
      "Epoch [769/2000], Loss: 0.2758, Val Loss: 0.2929\n",
      "Epoch [770/2000], Loss: 0.2757, Val Loss: 0.2928\n",
      "Epoch [771/2000], Loss: 0.2756, Val Loss: 0.2927\n",
      "Epoch [772/2000], Loss: 0.2756, Val Loss: 0.2927\n",
      "Epoch [773/2000], Loss: 0.2755, Val Loss: 0.2926\n",
      "Epoch [774/2000], Loss: 0.2754, Val Loss: 0.2926\n",
      "Epoch [775/2000], Loss: 0.2754, Val Loss: 0.2925\n",
      "Epoch [776/2000], Loss: 0.2753, Val Loss: 0.2925\n",
      "Epoch [777/2000], Loss: 0.2752, Val Loss: 0.2924\n",
      "Epoch [778/2000], Loss: 0.2752, Val Loss: 0.2924\n",
      "Epoch [779/2000], Loss: 0.2751, Val Loss: 0.2923\n",
      "Epoch [780/2000], Loss: 0.2750, Val Loss: 0.2923\n",
      "Epoch [781/2000], Loss: 0.2750, Val Loss: 0.2922\n",
      "Epoch [782/2000], Loss: 0.2749, Val Loss: 0.2921\n",
      "Epoch [783/2000], Loss: 0.2748, Val Loss: 0.2921\n",
      "Epoch [784/2000], Loss: 0.2748, Val Loss: 0.2921\n",
      "Epoch [785/2000], Loss: 0.2747, Val Loss: 0.2921\n",
      "Epoch [786/2000], Loss: 0.2746, Val Loss: 0.2920\n",
      "Epoch [787/2000], Loss: 0.2746, Val Loss: 0.2920\n",
      "Epoch [788/2000], Loss: 0.2745, Val Loss: 0.2920\n",
      "Epoch [789/2000], Loss: 0.2745, Val Loss: 0.2919\n",
      "Epoch [790/2000], Loss: 0.2744, Val Loss: 0.2919\n",
      "Epoch [791/2000], Loss: 0.2743, Val Loss: 0.2918\n",
      "Epoch [792/2000], Loss: 0.2743, Val Loss: 0.2917\n",
      "Epoch [793/2000], Loss: 0.2742, Val Loss: 0.2917\n",
      "Epoch [794/2000], Loss: 0.2741, Val Loss: 0.2916\n",
      "Epoch [795/2000], Loss: 0.2741, Val Loss: 0.2916\n",
      "Epoch [796/2000], Loss: 0.2740, Val Loss: 0.2915\n",
      "Epoch [797/2000], Loss: 0.2739, Val Loss: 0.2915\n",
      "Epoch [798/2000], Loss: 0.2739, Val Loss: 0.2914\n",
      "Epoch [799/2000], Loss: 0.2738, Val Loss: 0.2913\n",
      "Epoch [800/2000], Loss: 0.2738, Val Loss: 0.2912\n",
      "Epoch [801/2000], Loss: 0.2737, Val Loss: 0.2912\n",
      "Epoch [802/2000], Loss: 0.2736, Val Loss: 0.2912\n",
      "Epoch [803/2000], Loss: 0.2736, Val Loss: 0.2912\n",
      "Epoch [804/2000], Loss: 0.2735, Val Loss: 0.2912\n",
      "Epoch [805/2000], Loss: 0.2735, Val Loss: 0.2911\n",
      "Epoch [806/2000], Loss: 0.2734, Val Loss: 0.2910\n",
      "Epoch [807/2000], Loss: 0.2733, Val Loss: 0.2911\n",
      "Epoch [808/2000], Loss: 0.2733, Val Loss: 0.2910\n",
      "Epoch [809/2000], Loss: 0.2732, Val Loss: 0.2909\n",
      "Epoch [810/2000], Loss: 0.2731, Val Loss: 0.2908\n",
      "Epoch [811/2000], Loss: 0.2731, Val Loss: 0.2908\n",
      "Epoch [812/2000], Loss: 0.2730, Val Loss: 0.2907\n",
      "Epoch [813/2000], Loss: 0.2729, Val Loss: 0.2906\n",
      "Epoch [814/2000], Loss: 0.2729, Val Loss: 0.2906\n",
      "Epoch [815/2000], Loss: 0.2728, Val Loss: 0.2905\n",
      "Epoch [816/2000], Loss: 0.2728, Val Loss: 0.2904\n",
      "Epoch [817/2000], Loss: 0.2727, Val Loss: 0.2903\n",
      "Epoch [818/2000], Loss: 0.2726, Val Loss: 0.2903\n",
      "Epoch [819/2000], Loss: 0.2726, Val Loss: 0.2901\n",
      "Epoch [820/2000], Loss: 0.2725, Val Loss: 0.2901\n",
      "Epoch [821/2000], Loss: 0.2724, Val Loss: 0.2900\n",
      "Epoch [822/2000], Loss: 0.2724, Val Loss: 0.2899\n",
      "Epoch [823/2000], Loss: 0.2723, Val Loss: 0.2899\n",
      "Epoch [824/2000], Loss: 0.2723, Val Loss: 0.2898\n",
      "Epoch [825/2000], Loss: 0.2722, Val Loss: 0.2898\n",
      "Epoch [826/2000], Loss: 0.2721, Val Loss: 0.2896\n",
      "Epoch [827/2000], Loss: 0.2721, Val Loss: 0.2896\n",
      "Epoch [828/2000], Loss: 0.2720, Val Loss: 0.2895\n",
      "Epoch [829/2000], Loss: 0.2720, Val Loss: 0.2895\n",
      "Epoch [830/2000], Loss: 0.2719, Val Loss: 0.2894\n",
      "Epoch [831/2000], Loss: 0.2718, Val Loss: 0.2893\n",
      "Epoch [832/2000], Loss: 0.2718, Val Loss: 0.2892\n",
      "Epoch [833/2000], Loss: 0.2717, Val Loss: 0.2892\n",
      "Epoch [834/2000], Loss: 0.2717, Val Loss: 0.2891\n",
      "Epoch [835/2000], Loss: 0.2716, Val Loss: 0.2890\n",
      "Epoch [836/2000], Loss: 0.2715, Val Loss: 0.2890\n",
      "Epoch [837/2000], Loss: 0.2715, Val Loss: 0.2889\n",
      "Epoch [838/2000], Loss: 0.2714, Val Loss: 0.2888\n",
      "Epoch [839/2000], Loss: 0.2714, Val Loss: 0.2888\n",
      "Epoch [840/2000], Loss: 0.2713, Val Loss: 0.2887\n",
      "Epoch [841/2000], Loss: 0.2712, Val Loss: 0.2887\n",
      "Epoch [842/2000], Loss: 0.2712, Val Loss: 0.2885\n",
      "Epoch [843/2000], Loss: 0.2711, Val Loss: 0.2886\n",
      "Epoch [844/2000], Loss: 0.2711, Val Loss: 0.2884\n",
      "Epoch [845/2000], Loss: 0.2710, Val Loss: 0.2884\n",
      "Epoch [846/2000], Loss: 0.2710, Val Loss: 0.2882\n",
      "Epoch [847/2000], Loss: 0.2709, Val Loss: 0.2882\n",
      "Epoch [848/2000], Loss: 0.2708, Val Loss: 0.2881\n",
      "Epoch [849/2000], Loss: 0.2708, Val Loss: 0.2881\n",
      "Epoch [850/2000], Loss: 0.2707, Val Loss: 0.2880\n",
      "Epoch [851/2000], Loss: 0.2707, Val Loss: 0.2879\n",
      "Epoch [852/2000], Loss: 0.2706, Val Loss: 0.2879\n",
      "Epoch [853/2000], Loss: 0.2705, Val Loss: 0.2878\n",
      "Epoch [854/2000], Loss: 0.2705, Val Loss: 0.2877\n",
      "Epoch [855/2000], Loss: 0.2704, Val Loss: 0.2876\n",
      "Epoch [856/2000], Loss: 0.2704, Val Loss: 0.2875\n",
      "Epoch [857/2000], Loss: 0.2703, Val Loss: 0.2875\n",
      "Epoch [858/2000], Loss: 0.2702, Val Loss: 0.2874\n",
      "Epoch [859/2000], Loss: 0.2702, Val Loss: 0.2874\n",
      "Epoch [860/2000], Loss: 0.2701, Val Loss: 0.2873\n",
      "Epoch [861/2000], Loss: 0.2701, Val Loss: 0.2872\n",
      "Epoch [862/2000], Loss: 0.2700, Val Loss: 0.2872\n",
      "Epoch [863/2000], Loss: 0.2700, Val Loss: 0.2871\n",
      "Epoch [864/2000], Loss: 0.2699, Val Loss: 0.2871\n",
      "Epoch [865/2000], Loss: 0.2698, Val Loss: 0.2870\n",
      "Epoch [866/2000], Loss: 0.2698, Val Loss: 0.2870\n",
      "Epoch [867/2000], Loss: 0.2697, Val Loss: 0.2869\n",
      "Epoch [868/2000], Loss: 0.2697, Val Loss: 0.2870\n",
      "Epoch [869/2000], Loss: 0.2696, Val Loss: 0.2869\n",
      "Epoch [870/2000], Loss: 0.2695, Val Loss: 0.2868\n",
      "Epoch [871/2000], Loss: 0.2695, Val Loss: 0.2868\n",
      "Epoch [872/2000], Loss: 0.2694, Val Loss: 0.2867\n",
      "Epoch [873/2000], Loss: 0.2694, Val Loss: 0.2867\n",
      "Epoch [874/2000], Loss: 0.2693, Val Loss: 0.2866\n",
      "Epoch [875/2000], Loss: 0.2693, Val Loss: 0.2866\n",
      "Epoch [876/2000], Loss: 0.2692, Val Loss: 0.2865\n",
      "Epoch [877/2000], Loss: 0.2692, Val Loss: 0.2864\n",
      "Epoch [878/2000], Loss: 0.2691, Val Loss: 0.2864\n",
      "Epoch [879/2000], Loss: 0.2690, Val Loss: 0.2863\n",
      "Epoch [880/2000], Loss: 0.2690, Val Loss: 0.2863\n",
      "Epoch [881/2000], Loss: 0.2689, Val Loss: 0.2862\n",
      "Epoch [882/2000], Loss: 0.2689, Val Loss: 0.2862\n",
      "Epoch [883/2000], Loss: 0.2688, Val Loss: 0.2861\n",
      "Epoch [884/2000], Loss: 0.2687, Val Loss: 0.2861\n",
      "Epoch [885/2000], Loss: 0.2687, Val Loss: 0.2860\n",
      "Epoch [886/2000], Loss: 0.2686, Val Loss: 0.2860\n",
      "Epoch [887/2000], Loss: 0.2686, Val Loss: 0.2859\n",
      "Epoch [888/2000], Loss: 0.2685, Val Loss: 0.2858\n",
      "Epoch [889/2000], Loss: 0.2685, Val Loss: 0.2858\n",
      "Epoch [890/2000], Loss: 0.2684, Val Loss: 0.2858\n",
      "Epoch [891/2000], Loss: 0.2683, Val Loss: 0.2858\n",
      "Epoch [892/2000], Loss: 0.2683, Val Loss: 0.2856\n",
      "Epoch [893/2000], Loss: 0.2682, Val Loss: 0.2856\n",
      "Epoch [894/2000], Loss: 0.2682, Val Loss: 0.2854\n",
      "Epoch [895/2000], Loss: 0.2682, Val Loss: 0.2854\n",
      "Epoch [896/2000], Loss: 0.2681, Val Loss: 0.2853\n",
      "Epoch [897/2000], Loss: 0.2681, Val Loss: 0.2854\n",
      "Epoch [898/2000], Loss: 0.2680, Val Loss: 0.2852\n",
      "Epoch [899/2000], Loss: 0.2679, Val Loss: 0.2852\n",
      "Epoch [900/2000], Loss: 0.2679, Val Loss: 0.2851\n",
      "Epoch [901/2000], Loss: 0.2678, Val Loss: 0.2850\n",
      "Epoch [902/2000], Loss: 0.2677, Val Loss: 0.2851\n",
      "Epoch [903/2000], Loss: 0.2677, Val Loss: 0.2850\n",
      "Epoch [904/2000], Loss: 0.2676, Val Loss: 0.2849\n",
      "Epoch [905/2000], Loss: 0.2676, Val Loss: 0.2848\n",
      "Epoch [906/2000], Loss: 0.2675, Val Loss: 0.2847\n",
      "Epoch [907/2000], Loss: 0.2675, Val Loss: 0.2846\n",
      "Epoch [908/2000], Loss: 0.2674, Val Loss: 0.2847\n",
      "Epoch [909/2000], Loss: 0.2674, Val Loss: 0.2845\n",
      "Epoch [910/2000], Loss: 0.2673, Val Loss: 0.2845\n",
      "Epoch [911/2000], Loss: 0.2673, Val Loss: 0.2844\n",
      "Epoch [912/2000], Loss: 0.2672, Val Loss: 0.2843\n",
      "Epoch [913/2000], Loss: 0.2672, Val Loss: 0.2843\n",
      "Epoch [914/2000], Loss: 0.2671, Val Loss: 0.2842\n",
      "Epoch [915/2000], Loss: 0.2670, Val Loss: 0.2843\n",
      "Epoch [916/2000], Loss: 0.2670, Val Loss: 0.2842\n",
      "Epoch [917/2000], Loss: 0.2669, Val Loss: 0.2841\n",
      "Epoch [918/2000], Loss: 0.2669, Val Loss: 0.2841\n",
      "Epoch [919/2000], Loss: 0.2668, Val Loss: 0.2840\n",
      "Epoch [920/2000], Loss: 0.2668, Val Loss: 0.2840\n",
      "Epoch [921/2000], Loss: 0.2667, Val Loss: 0.2840\n",
      "Epoch [922/2000], Loss: 0.2667, Val Loss: 0.2839\n",
      "Epoch [923/2000], Loss: 0.2666, Val Loss: 0.2838\n",
      "Epoch [924/2000], Loss: 0.2666, Val Loss: 0.2838\n",
      "Epoch [925/2000], Loss: 0.2665, Val Loss: 0.2837\n",
      "Epoch [926/2000], Loss: 0.2665, Val Loss: 0.2836\n",
      "Epoch [927/2000], Loss: 0.2664, Val Loss: 0.2836\n",
      "Epoch [928/2000], Loss: 0.2664, Val Loss: 0.2835\n",
      "Epoch [929/2000], Loss: 0.2663, Val Loss: 0.2836\n",
      "Epoch [930/2000], Loss: 0.2662, Val Loss: 0.2833\n",
      "Epoch [931/2000], Loss: 0.2662, Val Loss: 0.2835\n",
      "Epoch [932/2000], Loss: 0.2661, Val Loss: 0.2834\n",
      "Epoch [933/2000], Loss: 0.2661, Val Loss: 0.2834\n",
      "Epoch [934/2000], Loss: 0.2660, Val Loss: 0.2834\n",
      "Epoch [935/2000], Loss: 0.2660, Val Loss: 0.2832\n",
      "Epoch [936/2000], Loss: 0.2659, Val Loss: 0.2834\n",
      "Epoch [937/2000], Loss: 0.2659, Val Loss: 0.2833\n",
      "Epoch [938/2000], Loss: 0.2658, Val Loss: 0.2832\n",
      "Epoch [939/2000], Loss: 0.2658, Val Loss: 0.2832\n",
      "Epoch [940/2000], Loss: 0.2657, Val Loss: 0.2830\n",
      "Epoch [941/2000], Loss: 0.2657, Val Loss: 0.2831\n",
      "Epoch [942/2000], Loss: 0.2656, Val Loss: 0.2830\n",
      "Epoch [943/2000], Loss: 0.2656, Val Loss: 0.2830\n",
      "Epoch [944/2000], Loss: 0.2655, Val Loss: 0.2830\n",
      "Epoch [945/2000], Loss: 0.2654, Val Loss: 0.2829\n",
      "Epoch [946/2000], Loss: 0.2654, Val Loss: 0.2828\n",
      "Epoch [947/2000], Loss: 0.2653, Val Loss: 0.2828\n",
      "Epoch [948/2000], Loss: 0.2653, Val Loss: 0.2827\n",
      "Epoch [949/2000], Loss: 0.2652, Val Loss: 0.2827\n",
      "Epoch [950/2000], Loss: 0.2652, Val Loss: 0.2826\n",
      "Epoch [951/2000], Loss: 0.2651, Val Loss: 0.2826\n",
      "Epoch [952/2000], Loss: 0.2651, Val Loss: 0.2825\n",
      "Epoch [953/2000], Loss: 0.2650, Val Loss: 0.2825\n",
      "Epoch [954/2000], Loss: 0.2650, Val Loss: 0.2824\n",
      "Epoch [955/2000], Loss: 0.2649, Val Loss: 0.2825\n",
      "Epoch [956/2000], Loss: 0.2649, Val Loss: 0.2824\n",
      "Epoch [957/2000], Loss: 0.2648, Val Loss: 0.2824\n",
      "Epoch [958/2000], Loss: 0.2648, Val Loss: 0.2822\n",
      "Epoch [959/2000], Loss: 0.2647, Val Loss: 0.2822\n",
      "Epoch [960/2000], Loss: 0.2647, Val Loss: 0.2821\n",
      "Epoch [961/2000], Loss: 0.2646, Val Loss: 0.2821\n",
      "Epoch [962/2000], Loss: 0.2646, Val Loss: 0.2821\n",
      "Epoch [963/2000], Loss: 0.2645, Val Loss: 0.2819\n",
      "Epoch [964/2000], Loss: 0.2645, Val Loss: 0.2819\n",
      "Epoch [965/2000], Loss: 0.2644, Val Loss: 0.2818\n",
      "Epoch [966/2000], Loss: 0.2644, Val Loss: 0.2818\n",
      "Epoch [967/2000], Loss: 0.2643, Val Loss: 0.2818\n",
      "Epoch [968/2000], Loss: 0.2643, Val Loss: 0.2818\n",
      "Epoch [969/2000], Loss: 0.2642, Val Loss: 0.2817\n",
      "Epoch [970/2000], Loss: 0.2642, Val Loss: 0.2817\n",
      "Epoch [971/2000], Loss: 0.2641, Val Loss: 0.2815\n",
      "Epoch [972/2000], Loss: 0.2641, Val Loss: 0.2815\n",
      "Epoch [973/2000], Loss: 0.2641, Val Loss: 0.2815\n",
      "Epoch [974/2000], Loss: 0.2640, Val Loss: 0.2815\n",
      "Epoch [975/2000], Loss: 0.2640, Val Loss: 0.2814\n",
      "Epoch [976/2000], Loss: 0.2639, Val Loss: 0.2814\n",
      "Epoch [977/2000], Loss: 0.2638, Val Loss: 0.2814\n",
      "Epoch [978/2000], Loss: 0.2638, Val Loss: 0.2813\n",
      "Epoch [979/2000], Loss: 0.2637, Val Loss: 0.2814\n",
      "Epoch [980/2000], Loss: 0.2637, Val Loss: 0.2811\n",
      "Epoch [981/2000], Loss: 0.2636, Val Loss: 0.2812\n",
      "Epoch [982/2000], Loss: 0.2636, Val Loss: 0.2810\n",
      "Epoch [983/2000], Loss: 0.2635, Val Loss: 0.2811\n",
      "Epoch [984/2000], Loss: 0.2635, Val Loss: 0.2810\n",
      "Epoch [985/2000], Loss: 0.2634, Val Loss: 0.2810\n",
      "Epoch [986/2000], Loss: 0.2634, Val Loss: 0.2810\n",
      "Epoch [987/2000], Loss: 0.2633, Val Loss: 0.2810\n",
      "Epoch [988/2000], Loss: 0.2633, Val Loss: 0.2810\n",
      "Epoch [989/2000], Loss: 0.2632, Val Loss: 0.2808\n",
      "Epoch [990/2000], Loss: 0.2632, Val Loss: 0.2810\n",
      "Epoch [991/2000], Loss: 0.2631, Val Loss: 0.2808\n",
      "Epoch [992/2000], Loss: 0.2631, Val Loss: 0.2809\n",
      "Epoch [993/2000], Loss: 0.2630, Val Loss: 0.2808\n",
      "Epoch [994/2000], Loss: 0.2630, Val Loss: 0.2808\n",
      "Epoch [995/2000], Loss: 0.2629, Val Loss: 0.2806\n",
      "Epoch [996/2000], Loss: 0.2629, Val Loss: 0.2807\n",
      "Epoch [997/2000], Loss: 0.2628, Val Loss: 0.2806\n",
      "Epoch [998/2000], Loss: 0.2628, Val Loss: 0.2807\n",
      "Epoch [999/2000], Loss: 0.2627, Val Loss: 0.2805\n",
      "Epoch [1000/2000], Loss: 0.2627, Val Loss: 0.2806\n",
      "Epoch [1001/2000], Loss: 0.2626, Val Loss: 0.2804\n",
      "Epoch [1002/2000], Loss: 0.2626, Val Loss: 0.2806\n",
      "Epoch [1003/2000], Loss: 0.2625, Val Loss: 0.2805\n",
      "Epoch [1004/2000], Loss: 0.2625, Val Loss: 0.2805\n",
      "Epoch [1005/2000], Loss: 0.2624, Val Loss: 0.2804\n",
      "Epoch [1006/2000], Loss: 0.2624, Val Loss: 0.2802\n",
      "Epoch [1007/2000], Loss: 0.2624, Val Loss: 0.2803\n",
      "Epoch [1008/2000], Loss: 0.2623, Val Loss: 0.2801\n",
      "Epoch [1009/2000], Loss: 0.2623, Val Loss: 0.2803\n",
      "Epoch [1010/2000], Loss: 0.2622, Val Loss: 0.2800\n",
      "Epoch [1011/2000], Loss: 0.2622, Val Loss: 0.2801\n",
      "Epoch [1012/2000], Loss: 0.2621, Val Loss: 0.2799\n",
      "Epoch [1013/2000], Loss: 0.2620, Val Loss: 0.2800\n",
      "Epoch [1014/2000], Loss: 0.2620, Val Loss: 0.2800\n",
      "Epoch [1015/2000], Loss: 0.2619, Val Loss: 0.2799\n",
      "Epoch [1016/2000], Loss: 0.2619, Val Loss: 0.2798\n",
      "Epoch [1017/2000], Loss: 0.2619, Val Loss: 0.2798\n",
      "Epoch [1018/2000], Loss: 0.2619, Val Loss: 0.2796\n",
      "Epoch [1019/2000], Loss: 0.2618, Val Loss: 0.2796\n",
      "Epoch [1020/2000], Loss: 0.2617, Val Loss: 0.2796\n",
      "Epoch [1021/2000], Loss: 0.2616, Val Loss: 0.2796\n",
      "Epoch [1022/2000], Loss: 0.2616, Val Loss: 0.2795\n",
      "Epoch [1023/2000], Loss: 0.2615, Val Loss: 0.2796\n",
      "Epoch [1024/2000], Loss: 0.2615, Val Loss: 0.2794\n",
      "Epoch [1025/2000], Loss: 0.2615, Val Loss: 0.2794\n",
      "Epoch [1026/2000], Loss: 0.2614, Val Loss: 0.2793\n",
      "Epoch [1027/2000], Loss: 0.2614, Val Loss: 0.2793\n",
      "Epoch [1028/2000], Loss: 0.2613, Val Loss: 0.2793\n",
      "Epoch [1029/2000], Loss: 0.2613, Val Loss: 0.2791\n",
      "Epoch [1030/2000], Loss: 0.2612, Val Loss: 0.2792\n",
      "Epoch [1031/2000], Loss: 0.2612, Val Loss: 0.2790\n",
      "Epoch [1032/2000], Loss: 0.2611, Val Loss: 0.2790\n",
      "Epoch [1033/2000], Loss: 0.2611, Val Loss: 0.2791\n",
      "Epoch [1034/2000], Loss: 0.2610, Val Loss: 0.2789\n",
      "Epoch [1035/2000], Loss: 0.2610, Val Loss: 0.2789\n",
      "Epoch [1036/2000], Loss: 0.2609, Val Loss: 0.2789\n",
      "Epoch [1037/2000], Loss: 0.2609, Val Loss: 0.2789\n",
      "Epoch [1038/2000], Loss: 0.2608, Val Loss: 0.2788\n",
      "Epoch [1039/2000], Loss: 0.2608, Val Loss: 0.2788\n",
      "Epoch [1040/2000], Loss: 0.2608, Val Loss: 0.2787\n",
      "Epoch [1041/2000], Loss: 0.2607, Val Loss: 0.2787\n",
      "Epoch [1042/2000], Loss: 0.2607, Val Loss: 0.2786\n",
      "Epoch [1043/2000], Loss: 0.2606, Val Loss: 0.2787\n",
      "Epoch [1044/2000], Loss: 0.2606, Val Loss: 0.2785\n",
      "Epoch [1045/2000], Loss: 0.2605, Val Loss: 0.2786\n",
      "Epoch [1046/2000], Loss: 0.2605, Val Loss: 0.2784\n",
      "Epoch [1047/2000], Loss: 0.2605, Val Loss: 0.2786\n",
      "Epoch [1048/2000], Loss: 0.2605, Val Loss: 0.2782\n",
      "Epoch [1049/2000], Loss: 0.2605, Val Loss: 0.2788\n",
      "Epoch [1050/2000], Loss: 0.2605, Val Loss: 0.2781\n",
      "Epoch [1051/2000], Loss: 0.2605, Val Loss: 0.2786\n",
      "Epoch [1052/2000], Loss: 0.2604, Val Loss: 0.2780\n",
      "Epoch [1053/2000], Loss: 0.2603, Val Loss: 0.2781\n",
      "Epoch [1054/2000], Loss: 0.2602, Val Loss: 0.2784\n",
      "Epoch [1055/2000], Loss: 0.2601, Val Loss: 0.2779\n",
      "Epoch [1056/2000], Loss: 0.2601, Val Loss: 0.2785\n",
      "Epoch [1057/2000], Loss: 0.2601, Val Loss: 0.2779\n",
      "Epoch [1058/2000], Loss: 0.2600, Val Loss: 0.2779\n",
      "Epoch [1059/2000], Loss: 0.2599, Val Loss: 0.2782\n",
      "Epoch [1060/2000], Loss: 0.2599, Val Loss: 0.2775\n",
      "Epoch [1061/2000], Loss: 0.2599, Val Loss: 0.2782\n",
      "Epoch [1062/2000], Loss: 0.2598, Val Loss: 0.2777\n",
      "Epoch [1063/2000], Loss: 0.2598, Val Loss: 0.2776\n",
      "Epoch [1064/2000], Loss: 0.2597, Val Loss: 0.2780\n",
      "Epoch [1065/2000], Loss: 0.2597, Val Loss: 0.2774\n",
      "Epoch [1066/2000], Loss: 0.2596, Val Loss: 0.2779\n",
      "Epoch [1067/2000], Loss: 0.2596, Val Loss: 0.2776\n",
      "Epoch [1068/2000], Loss: 0.2596, Val Loss: 0.2774\n",
      "Epoch [1069/2000], Loss: 0.2595, Val Loss: 0.2777\n",
      "Epoch [1070/2000], Loss: 0.2595, Val Loss: 0.2773\n",
      "Epoch [1071/2000], Loss: 0.2594, Val Loss: 0.2775\n",
      "Epoch [1072/2000], Loss: 0.2594, Val Loss: 0.2773\n",
      "Epoch [1073/2000], Loss: 0.2593, Val Loss: 0.2773\n",
      "Epoch [1074/2000], Loss: 0.2593, Val Loss: 0.2771\n",
      "Epoch [1075/2000], Loss: 0.2592, Val Loss: 0.2771\n",
      "Epoch [1076/2000], Loss: 0.2592, Val Loss: 0.2771\n",
      "Epoch [1077/2000], Loss: 0.2592, Val Loss: 0.2769\n",
      "Epoch [1078/2000], Loss: 0.2591, Val Loss: 0.2771\n",
      "Epoch [1079/2000], Loss: 0.2591, Val Loss: 0.2768\n",
      "Epoch [1080/2000], Loss: 0.2590, Val Loss: 0.2770\n",
      "Epoch [1081/2000], Loss: 0.2590, Val Loss: 0.2768\n",
      "Epoch [1082/2000], Loss: 0.2589, Val Loss: 0.2767\n",
      "Epoch [1083/2000], Loss: 0.2589, Val Loss: 0.2769\n",
      "Epoch [1084/2000], Loss: 0.2589, Val Loss: 0.2765\n",
      "Epoch [1085/2000], Loss: 0.2588, Val Loss: 0.2765\n",
      "Epoch [1086/2000], Loss: 0.2588, Val Loss: 0.2764\n",
      "Epoch [1087/2000], Loss: 0.2587, Val Loss: 0.2764\n",
      "Epoch [1088/2000], Loss: 0.2587, Val Loss: 0.2765\n",
      "Epoch [1089/2000], Loss: 0.2586, Val Loss: 0.2763\n",
      "Epoch [1090/2000], Loss: 0.2586, Val Loss: 0.2763\n",
      "Epoch [1091/2000], Loss: 0.2585, Val Loss: 0.2762\n",
      "Epoch [1092/2000], Loss: 0.2585, Val Loss: 0.2760\n",
      "Epoch [1093/2000], Loss: 0.2585, Val Loss: 0.2762\n",
      "Epoch [1094/2000], Loss: 0.2584, Val Loss: 0.2760\n",
      "Epoch [1095/2000], Loss: 0.2584, Val Loss: 0.2761\n",
      "Epoch [1096/2000], Loss: 0.2584, Val Loss: 0.2759\n",
      "Epoch [1097/2000], Loss: 0.2583, Val Loss: 0.2759\n",
      "Epoch [1098/2000], Loss: 0.2583, Val Loss: 0.2759\n",
      "Epoch [1099/2000], Loss: 0.2583, Val Loss: 0.2760\n",
      "Epoch [1100/2000], Loss: 0.2583, Val Loss: 0.2759\n",
      "Epoch [1101/2000], Loss: 0.2583, Val Loss: 0.2760\n",
      "Epoch [1102/2000], Loss: 0.2582, Val Loss: 0.2756\n",
      "Epoch [1103/2000], Loss: 0.2582, Val Loss: 0.2759\n",
      "Epoch [1104/2000], Loss: 0.2581, Val Loss: 0.2756\n",
      "Epoch [1105/2000], Loss: 0.2580, Val Loss: 0.2758\n",
      "Epoch [1106/2000], Loss: 0.2579, Val Loss: 0.2757\n",
      "Epoch [1107/2000], Loss: 0.2579, Val Loss: 0.2756\n",
      "Epoch [1108/2000], Loss: 0.2579, Val Loss: 0.2756\n",
      "Epoch [1109/2000], Loss: 0.2579, Val Loss: 0.2755\n",
      "Epoch [1110/2000], Loss: 0.2578, Val Loss: 0.2755\n",
      "Epoch [1111/2000], Loss: 0.2578, Val Loss: 0.2755\n",
      "Epoch [1112/2000], Loss: 0.2577, Val Loss: 0.2755\n",
      "Epoch [1113/2000], Loss: 0.2576, Val Loss: 0.2754\n",
      "Epoch [1114/2000], Loss: 0.2576, Val Loss: 0.2753\n",
      "Epoch [1115/2000], Loss: 0.2576, Val Loss: 0.2755\n",
      "Epoch [1116/2000], Loss: 0.2575, Val Loss: 0.2754\n",
      "Epoch [1117/2000], Loss: 0.2575, Val Loss: 0.2754\n",
      "Epoch [1118/2000], Loss: 0.2575, Val Loss: 0.2754\n",
      "Epoch [1119/2000], Loss: 0.2574, Val Loss: 0.2753\n",
      "Epoch [1120/2000], Loss: 0.2574, Val Loss: 0.2755\n",
      "Epoch [1121/2000], Loss: 0.2573, Val Loss: 0.2752\n",
      "Epoch [1122/2000], Loss: 0.2573, Val Loss: 0.2754\n",
      "Epoch [1123/2000], Loss: 0.2572, Val Loss: 0.2752\n",
      "Epoch [1124/2000], Loss: 0.2572, Val Loss: 0.2753\n",
      "Epoch [1125/2000], Loss: 0.2572, Val Loss: 0.2755\n",
      "Epoch [1126/2000], Loss: 0.2571, Val Loss: 0.2752\n",
      "Epoch [1127/2000], Loss: 0.2571, Val Loss: 0.2756\n",
      "Epoch [1128/2000], Loss: 0.2570, Val Loss: 0.2751\n",
      "Epoch [1129/2000], Loss: 0.2570, Val Loss: 0.2754\n",
      "Epoch [1130/2000], Loss: 0.2570, Val Loss: 0.2751\n",
      "Epoch [1131/2000], Loss: 0.2569, Val Loss: 0.2752\n",
      "Epoch [1132/2000], Loss: 0.2569, Val Loss: 0.2752\n",
      "Epoch [1133/2000], Loss: 0.2568, Val Loss: 0.2752\n",
      "Epoch [1134/2000], Loss: 0.2568, Val Loss: 0.2752\n",
      "Epoch [1135/2000], Loss: 0.2568, Val Loss: 0.2752\n",
      "Epoch [1136/2000], Loss: 0.2567, Val Loss: 0.2750\n",
      "Epoch [1137/2000], Loss: 0.2567, Val Loss: 0.2752\n",
      "Epoch [1138/2000], Loss: 0.2566, Val Loss: 0.2751\n",
      "Epoch [1139/2000], Loss: 0.2566, Val Loss: 0.2753\n",
      "Epoch [1140/2000], Loss: 0.2566, Val Loss: 0.2750\n",
      "Epoch [1141/2000], Loss: 0.2565, Val Loss: 0.2752\n",
      "Epoch [1142/2000], Loss: 0.2565, Val Loss: 0.2750\n",
      "Epoch [1143/2000], Loss: 0.2564, Val Loss: 0.2750\n",
      "Epoch [1144/2000], Loss: 0.2564, Val Loss: 0.2750\n",
      "Epoch [1145/2000], Loss: 0.2564, Val Loss: 0.2750\n",
      "Epoch [1146/2000], Loss: 0.2563, Val Loss: 0.2751\n",
      "Epoch [1147/2000], Loss: 0.2563, Val Loss: 0.2749\n",
      "Epoch [1148/2000], Loss: 0.2563, Val Loss: 0.2752\n",
      "Epoch [1149/2000], Loss: 0.2562, Val Loss: 0.2746\n",
      "Epoch [1150/2000], Loss: 0.2562, Val Loss: 0.2753\n",
      "Epoch [1151/2000], Loss: 0.2562, Val Loss: 0.2747\n",
      "Epoch [1152/2000], Loss: 0.2561, Val Loss: 0.2754\n",
      "Epoch [1153/2000], Loss: 0.2561, Val Loss: 0.2745\n",
      "Epoch [1154/2000], Loss: 0.2561, Val Loss: 0.2752\n",
      "Epoch [1155/2000], Loss: 0.2560, Val Loss: 0.2746\n",
      "Epoch [1156/2000], Loss: 0.2560, Val Loss: 0.2751\n",
      "Epoch [1157/2000], Loss: 0.2559, Val Loss: 0.2746\n",
      "Epoch [1158/2000], Loss: 0.2559, Val Loss: 0.2748\n",
      "Epoch [1159/2000], Loss: 0.2558, Val Loss: 0.2747\n",
      "Epoch [1160/2000], Loss: 0.2558, Val Loss: 0.2747\n",
      "Epoch [1161/2000], Loss: 0.2558, Val Loss: 0.2749\n",
      "Epoch [1162/2000], Loss: 0.2558, Val Loss: 0.2745\n",
      "Epoch [1163/2000], Loss: 0.2557, Val Loss: 0.2748\n",
      "Epoch [1164/2000], Loss: 0.2557, Val Loss: 0.2745\n",
      "Epoch [1165/2000], Loss: 0.2557, Val Loss: 0.2746\n",
      "Epoch [1166/2000], Loss: 0.2556, Val Loss: 0.2744\n",
      "Epoch [1167/2000], Loss: 0.2556, Val Loss: 0.2745\n",
      "Epoch [1168/2000], Loss: 0.2555, Val Loss: 0.2744\n",
      "Epoch [1169/2000], Loss: 0.2554, Val Loss: 0.2743\n",
      "Epoch [1170/2000], Loss: 0.2554, Val Loss: 0.2744\n",
      "Epoch [1171/2000], Loss: 0.2554, Val Loss: 0.2742\n",
      "Epoch [1172/2000], Loss: 0.2554, Val Loss: 0.2745\n",
      "Epoch [1173/2000], Loss: 0.2554, Val Loss: 0.2741\n",
      "Epoch [1174/2000], Loss: 0.2554, Val Loss: 0.2746\n",
      "Epoch [1175/2000], Loss: 0.2554, Val Loss: 0.2739\n",
      "Epoch [1176/2000], Loss: 0.2554, Val Loss: 0.2744\n",
      "Epoch [1177/2000], Loss: 0.2554, Val Loss: 0.2741\n",
      "Epoch [1178/2000], Loss: 0.2554, Val Loss: 0.2741\n",
      "Epoch [1179/2000], Loss: 0.2552, Val Loss: 0.2743\n",
      "Epoch [1180/2000], Loss: 0.2551, Val Loss: 0.2739\n",
      "Epoch [1181/2000], Loss: 0.2550, Val Loss: 0.2742\n",
      "Epoch [1182/2000], Loss: 0.2550, Val Loss: 0.2740\n",
      "Epoch [1183/2000], Loss: 0.2551, Val Loss: 0.2741\n",
      "Epoch [1184/2000], Loss: 0.2550, Val Loss: 0.2739\n",
      "Epoch [1185/2000], Loss: 0.2550, Val Loss: 0.2741\n",
      "Epoch [1186/2000], Loss: 0.2549, Val Loss: 0.2740\n",
      "Epoch [1187/2000], Loss: 0.2548, Val Loss: 0.2739\n",
      "Epoch [1188/2000], Loss: 0.2548, Val Loss: 0.2740\n",
      "Epoch [1189/2000], Loss: 0.2548, Val Loss: 0.2738\n",
      "Epoch [1190/2000], Loss: 0.2547, Val Loss: 0.2738\n",
      "Epoch [1191/2000], Loss: 0.2547, Val Loss: 0.2739\n",
      "Epoch [1192/2000], Loss: 0.2546, Val Loss: 0.2738\n",
      "Epoch [1193/2000], Loss: 0.2546, Val Loss: 0.2737\n",
      "Epoch [1194/2000], Loss: 0.2545, Val Loss: 0.2739\n",
      "Epoch [1195/2000], Loss: 0.2545, Val Loss: 0.2736\n",
      "Epoch [1196/2000], Loss: 0.2545, Val Loss: 0.2740\n",
      "Epoch [1197/2000], Loss: 0.2545, Val Loss: 0.2738\n",
      "Epoch [1198/2000], Loss: 0.2544, Val Loss: 0.2735\n",
      "Epoch [1199/2000], Loss: 0.2543, Val Loss: 0.2739\n",
      "Epoch [1200/2000], Loss: 0.2543, Val Loss: 0.2733\n",
      "Epoch [1201/2000], Loss: 0.2543, Val Loss: 0.2738\n",
      "Epoch [1202/2000], Loss: 0.2543, Val Loss: 0.2737\n",
      "Epoch [1203/2000], Loss: 0.2542, Val Loss: 0.2735\n",
      "Epoch [1204/2000], Loss: 0.2542, Val Loss: 0.2738\n",
      "Epoch [1205/2000], Loss: 0.2541, Val Loss: 0.2733\n",
      "Epoch [1206/2000], Loss: 0.2541, Val Loss: 0.2736\n",
      "Epoch [1207/2000], Loss: 0.2541, Val Loss: 0.2734\n",
      "Epoch [1208/2000], Loss: 0.2540, Val Loss: 0.2735\n",
      "Epoch [1209/2000], Loss: 0.2540, Val Loss: 0.2734\n",
      "Epoch [1210/2000], Loss: 0.2540, Val Loss: 0.2734\n",
      "Epoch [1211/2000], Loss: 0.2539, Val Loss: 0.2733\n",
      "Epoch [1212/2000], Loss: 0.2539, Val Loss: 0.2733\n",
      "Epoch [1213/2000], Loss: 0.2539, Val Loss: 0.2733\n",
      "Epoch [1214/2000], Loss: 0.2538, Val Loss: 0.2733\n",
      "Epoch [1215/2000], Loss: 0.2538, Val Loss: 0.2735\n",
      "Epoch [1216/2000], Loss: 0.2538, Val Loss: 0.2731\n",
      "Epoch [1217/2000], Loss: 0.2537, Val Loss: 0.2733\n",
      "Epoch [1218/2000], Loss: 0.2537, Val Loss: 0.2731\n",
      "Epoch [1219/2000], Loss: 0.2537, Val Loss: 0.2731\n",
      "Epoch [1220/2000], Loss: 0.2536, Val Loss: 0.2733\n",
      "Epoch [1221/2000], Loss: 0.2536, Val Loss: 0.2730\n",
      "Epoch [1222/2000], Loss: 0.2536, Val Loss: 0.2732\n",
      "Epoch [1223/2000], Loss: 0.2535, Val Loss: 0.2730\n",
      "Epoch [1224/2000], Loss: 0.2535, Val Loss: 0.2729\n",
      "Epoch [1225/2000], Loss: 0.2534, Val Loss: 0.2731\n",
      "Epoch [1226/2000], Loss: 0.2534, Val Loss: 0.2729\n",
      "Epoch [1227/2000], Loss: 0.2533, Val Loss: 0.2730\n",
      "Epoch [1228/2000], Loss: 0.2533, Val Loss: 0.2729\n",
      "Epoch [1229/2000], Loss: 0.2533, Val Loss: 0.2728\n",
      "Epoch [1230/2000], Loss: 0.2533, Val Loss: 0.2730\n",
      "Epoch [1231/2000], Loss: 0.2532, Val Loss: 0.2727\n",
      "Epoch [1232/2000], Loss: 0.2532, Val Loss: 0.2730\n",
      "Epoch [1233/2000], Loss: 0.2532, Val Loss: 0.2726\n",
      "Epoch [1234/2000], Loss: 0.2531, Val Loss: 0.2729\n",
      "Epoch [1235/2000], Loss: 0.2531, Val Loss: 0.2723\n",
      "Epoch [1236/2000], Loss: 0.2531, Val Loss: 0.2730\n",
      "Epoch [1237/2000], Loss: 0.2530, Val Loss: 0.2723\n",
      "Epoch [1238/2000], Loss: 0.2530, Val Loss: 0.2730\n",
      "Epoch [1239/2000], Loss: 0.2530, Val Loss: 0.2722\n",
      "Epoch [1240/2000], Loss: 0.2530, Val Loss: 0.2729\n",
      "Epoch [1241/2000], Loss: 0.2530, Val Loss: 0.2724\n",
      "Epoch [1242/2000], Loss: 0.2529, Val Loss: 0.2727\n",
      "Epoch [1243/2000], Loss: 0.2529, Val Loss: 0.2725\n",
      "Epoch [1244/2000], Loss: 0.2528, Val Loss: 0.2724\n",
      "Epoch [1245/2000], Loss: 0.2527, Val Loss: 0.2724\n",
      "Epoch [1246/2000], Loss: 0.2527, Val Loss: 0.2720\n",
      "Epoch [1247/2000], Loss: 0.2527, Val Loss: 0.2726\n",
      "Epoch [1248/2000], Loss: 0.2527, Val Loss: 0.2720\n",
      "Epoch [1249/2000], Loss: 0.2526, Val Loss: 0.2726\n",
      "Epoch [1250/2000], Loss: 0.2526, Val Loss: 0.2721\n",
      "Epoch [1251/2000], Loss: 0.2526, Val Loss: 0.2722\n",
      "Epoch [1252/2000], Loss: 0.2525, Val Loss: 0.2724\n",
      "Epoch [1253/2000], Loss: 0.2525, Val Loss: 0.2720\n",
      "Epoch [1254/2000], Loss: 0.2524, Val Loss: 0.2727\n",
      "Epoch [1255/2000], Loss: 0.2524, Val Loss: 0.2718\n",
      "Epoch [1256/2000], Loss: 0.2524, Val Loss: 0.2725\n",
      "Epoch [1257/2000], Loss: 0.2523, Val Loss: 0.2715\n",
      "Epoch [1258/2000], Loss: 0.2523, Val Loss: 0.2722\n",
      "Epoch [1259/2000], Loss: 0.2523, Val Loss: 0.2717\n",
      "Epoch [1260/2000], Loss: 0.2523, Val Loss: 0.2720\n",
      "Epoch [1261/2000], Loss: 0.2522, Val Loss: 0.2719\n",
      "Epoch [1262/2000], Loss: 0.2522, Val Loss: 0.2717\n",
      "Epoch [1263/2000], Loss: 0.2522, Val Loss: 0.2719\n",
      "Epoch [1264/2000], Loss: 0.2521, Val Loss: 0.2715\n",
      "Epoch [1265/2000], Loss: 0.2521, Val Loss: 0.2719\n",
      "Epoch [1266/2000], Loss: 0.2520, Val Loss: 0.2717\n",
      "Epoch [1267/2000], Loss: 0.2520, Val Loss: 0.2716\n",
      "Epoch [1268/2000], Loss: 0.2519, Val Loss: 0.2717\n",
      "Epoch [1269/2000], Loss: 0.2519, Val Loss: 0.2713\n",
      "Epoch [1270/2000], Loss: 0.2519, Val Loss: 0.2718\n",
      "Epoch [1271/2000], Loss: 0.2518, Val Loss: 0.2712\n",
      "Epoch [1272/2000], Loss: 0.2518, Val Loss: 0.2716\n",
      "Epoch [1273/2000], Loss: 0.2518, Val Loss: 0.2712\n",
      "Epoch [1274/2000], Loss: 0.2517, Val Loss: 0.2713\n",
      "Epoch [1275/2000], Loss: 0.2517, Val Loss: 0.2714\n",
      "Epoch [1276/2000], Loss: 0.2516, Val Loss: 0.2713\n",
      "Epoch [1277/2000], Loss: 0.2516, Val Loss: 0.2714\n",
      "Epoch [1278/2000], Loss: 0.2516, Val Loss: 0.2710\n",
      "Epoch [1279/2000], Loss: 0.2516, Val Loss: 0.2713\n",
      "Epoch [1280/2000], Loss: 0.2515, Val Loss: 0.2710\n",
      "Epoch [1281/2000], Loss: 0.2515, Val Loss: 0.2714\n",
      "Epoch [1282/2000], Loss: 0.2515, Val Loss: 0.2709\n",
      "Epoch [1283/2000], Loss: 0.2514, Val Loss: 0.2714\n",
      "Epoch [1284/2000], Loss: 0.2514, Val Loss: 0.2707\n",
      "Epoch [1285/2000], Loss: 0.2514, Val Loss: 0.2712\n",
      "Epoch [1286/2000], Loss: 0.2513, Val Loss: 0.2708\n",
      "Epoch [1287/2000], Loss: 0.2513, Val Loss: 0.2710\n",
      "Epoch [1288/2000], Loss: 0.2513, Val Loss: 0.2708\n",
      "Epoch [1289/2000], Loss: 0.2513, Val Loss: 0.2708\n",
      "Epoch [1290/2000], Loss: 0.2512, Val Loss: 0.2707\n",
      "Epoch [1291/2000], Loss: 0.2512, Val Loss: 0.2708\n",
      "Epoch [1292/2000], Loss: 0.2512, Val Loss: 0.2708\n",
      "Epoch [1293/2000], Loss: 0.2512, Val Loss: 0.2709\n",
      "Epoch [1294/2000], Loss: 0.2511, Val Loss: 0.2705\n",
      "Epoch [1295/2000], Loss: 0.2511, Val Loss: 0.2710\n",
      "Epoch [1296/2000], Loss: 0.2510, Val Loss: 0.2703\n",
      "Epoch [1297/2000], Loss: 0.2510, Val Loss: 0.2708\n",
      "Epoch [1298/2000], Loss: 0.2509, Val Loss: 0.2703\n",
      "Epoch [1299/2000], Loss: 0.2509, Val Loss: 0.2706\n",
      "Epoch [1300/2000], Loss: 0.2509, Val Loss: 0.2705\n",
      "Epoch [1301/2000], Loss: 0.2508, Val Loss: 0.2706\n",
      "Epoch [1302/2000], Loss: 0.2508, Val Loss: 0.2704\n",
      "Epoch [1303/2000], Loss: 0.2508, Val Loss: 0.2706\n",
      "Epoch [1304/2000], Loss: 0.2508, Val Loss: 0.2704\n",
      "Epoch [1305/2000], Loss: 0.2508, Val Loss: 0.2704\n",
      "Epoch [1306/2000], Loss: 0.2508, Val Loss: 0.2704\n",
      "Epoch [1307/2000], Loss: 0.2507, Val Loss: 0.2703\n",
      "Epoch [1308/2000], Loss: 0.2507, Val Loss: 0.2705\n",
      "Epoch [1309/2000], Loss: 0.2506, Val Loss: 0.2702\n",
      "Epoch [1310/2000], Loss: 0.2506, Val Loss: 0.2704\n",
      "Epoch [1311/2000], Loss: 0.2505, Val Loss: 0.2700\n",
      "Epoch [1312/2000], Loss: 0.2505, Val Loss: 0.2703\n",
      "Epoch [1313/2000], Loss: 0.2504, Val Loss: 0.2702\n",
      "Epoch [1314/2000], Loss: 0.2504, Val Loss: 0.2702\n",
      "Epoch [1315/2000], Loss: 0.2504, Val Loss: 0.2703\n",
      "Epoch [1316/2000], Loss: 0.2504, Val Loss: 0.2701\n",
      "Epoch [1317/2000], Loss: 0.2503, Val Loss: 0.2704\n",
      "Epoch [1318/2000], Loss: 0.2503, Val Loss: 0.2701\n",
      "Epoch [1319/2000], Loss: 0.2502, Val Loss: 0.2702\n",
      "Epoch [1320/2000], Loss: 0.2502, Val Loss: 0.2701\n",
      "Epoch [1321/2000], Loss: 0.2501, Val Loss: 0.2698\n",
      "Epoch [1322/2000], Loss: 0.2501, Val Loss: 0.2703\n",
      "Epoch [1323/2000], Loss: 0.2501, Val Loss: 0.2694\n",
      "Epoch [1324/2000], Loss: 0.2501, Val Loss: 0.2704\n",
      "Epoch [1325/2000], Loss: 0.2501, Val Loss: 0.2693\n",
      "Epoch [1326/2000], Loss: 0.2500, Val Loss: 0.2704\n",
      "Epoch [1327/2000], Loss: 0.2500, Val Loss: 0.2696\n",
      "Epoch [1328/2000], Loss: 0.2500, Val Loss: 0.2703\n",
      "Epoch [1329/2000], Loss: 0.2500, Val Loss: 0.2698\n",
      "Epoch [1330/2000], Loss: 0.2500, Val Loss: 0.2701\n",
      "Epoch [1331/2000], Loss: 0.2500, Val Loss: 0.2697\n",
      "Epoch [1332/2000], Loss: 0.2500, Val Loss: 0.2701\n",
      "Epoch [1333/2000], Loss: 0.2500, Val Loss: 0.2698\n",
      "Epoch [1334/2000], Loss: 0.2500, Val Loss: 0.2700\n",
      "Epoch [1335/2000], Loss: 0.2499, Val Loss: 0.2698\n",
      "Epoch [1336/2000], Loss: 0.2498, Val Loss: 0.2696\n",
      "Epoch [1337/2000], Loss: 0.2497, Val Loss: 0.2701\n",
      "Epoch [1338/2000], Loss: 0.2496, Val Loss: 0.2695\n",
      "Epoch [1339/2000], Loss: 0.2496, Val Loss: 0.2702\n",
      "Epoch [1340/2000], Loss: 0.2496, Val Loss: 0.2695\n",
      "Epoch [1341/2000], Loss: 0.2496, Val Loss: 0.2699\n",
      "Epoch [1342/2000], Loss: 0.2496, Val Loss: 0.2698\n",
      "Epoch [1343/2000], Loss: 0.2495, Val Loss: 0.2697\n",
      "Epoch [1344/2000], Loss: 0.2495, Val Loss: 0.2697\n",
      "Epoch [1345/2000], Loss: 0.2494, Val Loss: 0.2698\n",
      "Epoch [1346/2000], Loss: 0.2493, Val Loss: 0.2697\n",
      "Epoch [1347/2000], Loss: 0.2493, Val Loss: 0.2698\n",
      "Epoch [1348/2000], Loss: 0.2493, Val Loss: 0.2697\n",
      "Epoch [1349/2000], Loss: 0.2493, Val Loss: 0.2696\n",
      "Epoch [1350/2000], Loss: 0.2492, Val Loss: 0.2696\n",
      "Epoch [1351/2000], Loss: 0.2492, Val Loss: 0.2695\n",
      "Epoch [1352/2000], Loss: 0.2492, Val Loss: 0.2698\n",
      "Epoch [1353/2000], Loss: 0.2491, Val Loss: 0.2696\n",
      "Epoch [1354/2000], Loss: 0.2491, Val Loss: 0.2696\n",
      "Epoch [1355/2000], Loss: 0.2491, Val Loss: 0.2696\n",
      "Epoch [1356/2000], Loss: 0.2490, Val Loss: 0.2692\n",
      "Epoch [1357/2000], Loss: 0.2490, Val Loss: 0.2700\n",
      "Epoch [1358/2000], Loss: 0.2490, Val Loss: 0.2690\n",
      "Epoch [1359/2000], Loss: 0.2490, Val Loss: 0.2701\n",
      "Epoch [1360/2000], Loss: 0.2490, Val Loss: 0.2690\n",
      "Epoch [1361/2000], Loss: 0.2489, Val Loss: 0.2698\n",
      "Epoch [1362/2000], Loss: 0.2489, Val Loss: 0.2693\n",
      "Epoch [1363/2000], Loss: 0.2488, Val Loss: 0.2697\n",
      "Epoch [1364/2000], Loss: 0.2488, Val Loss: 0.2694\n",
      "Epoch [1365/2000], Loss: 0.2488, Val Loss: 0.2695\n",
      "Epoch [1366/2000], Loss: 0.2487, Val Loss: 0.2695\n",
      "Epoch [1367/2000], Loss: 0.2487, Val Loss: 0.2693\n",
      "Epoch [1368/2000], Loss: 0.2486, Val Loss: 0.2695\n",
      "Epoch [1369/2000], Loss: 0.2486, Val Loss: 0.2693\n",
      "Epoch [1370/2000], Loss: 0.2486, Val Loss: 0.2696\n",
      "Epoch [1371/2000], Loss: 0.2486, Val Loss: 0.2692\n",
      "Epoch [1372/2000], Loss: 0.2486, Val Loss: 0.2699\n",
      "Epoch [1373/2000], Loss: 0.2485, Val Loss: 0.2689\n",
      "Epoch [1374/2000], Loss: 0.2485, Val Loss: 0.2700\n",
      "Epoch [1375/2000], Loss: 0.2485, Val Loss: 0.2689\n",
      "Epoch [1376/2000], Loss: 0.2485, Val Loss: 0.2699\n",
      "Epoch [1377/2000], Loss: 0.2484, Val Loss: 0.2690\n",
      "Epoch [1378/2000], Loss: 0.2484, Val Loss: 0.2697\n",
      "Epoch [1379/2000], Loss: 0.2484, Val Loss: 0.2692\n",
      "Epoch [1380/2000], Loss: 0.2484, Val Loss: 0.2696\n",
      "Epoch [1381/2000], Loss: 0.2483, Val Loss: 0.2693\n",
      "Epoch [1382/2000], Loss: 0.2483, Val Loss: 0.2694\n",
      "Epoch [1383/2000], Loss: 0.2483, Val Loss: 0.2696\n",
      "Epoch [1384/2000], Loss: 0.2483, Val Loss: 0.2694\n",
      "Epoch [1385/2000], Loss: 0.2482, Val Loss: 0.2698\n",
      "Epoch [1386/2000], Loss: 0.2482, Val Loss: 0.2690\n",
      "Epoch [1387/2000], Loss: 0.2482, Val Loss: 0.2695\n",
      "Epoch [1388/2000], Loss: 0.2481, Val Loss: 0.2692\n",
      "Epoch [1389/2000], Loss: 0.2481, Val Loss: 0.2695\n",
      "Epoch [1390/2000], Loss: 0.2480, Val Loss: 0.2694\n",
      "Epoch [1391/2000], Loss: 0.2480, Val Loss: 0.2694\n",
      "Epoch [1392/2000], Loss: 0.2479, Val Loss: 0.2694\n",
      "Epoch [1393/2000], Loss: 0.2479, Val Loss: 0.2693\n",
      "Epoch [1394/2000], Loss: 0.2479, Val Loss: 0.2696\n",
      "Epoch [1395/2000], Loss: 0.2478, Val Loss: 0.2692\n",
      "Epoch [1396/2000], Loss: 0.2478, Val Loss: 0.2697\n",
      "Epoch [1397/2000], Loss: 0.2478, Val Loss: 0.2692\n",
      "Epoch [1398/2000], Loss: 0.2478, Val Loss: 0.2697\n",
      "Epoch [1399/2000], Loss: 0.2477, Val Loss: 0.2694\n",
      "Epoch [1400/2000], Loss: 0.2477, Val Loss: 0.2695\n",
      "Epoch [1401/2000], Loss: 0.2476, Val Loss: 0.2696\n",
      "Epoch [1402/2000], Loss: 0.2476, Val Loss: 0.2692\n",
      "Epoch [1403/2000], Loss: 0.2476, Val Loss: 0.2700\n",
      "Epoch [1404/2000], Loss: 0.2476, Val Loss: 0.2691\n",
      "Epoch [1405/2000], Loss: 0.2476, Val Loss: 0.2702\n",
      "Epoch [1406/2000], Loss: 0.2476, Val Loss: 0.2691\n",
      "Epoch [1407/2000], Loss: 0.2476, Val Loss: 0.2703\n",
      "Epoch [1408/2000], Loss: 0.2475, Val Loss: 0.2691\n",
      "Epoch [1409/2000], Loss: 0.2475, Val Loss: 0.2701\n",
      "Epoch [1410/2000], Loss: 0.2474, Val Loss: 0.2693\n",
      "Epoch [1411/2000], Loss: 0.2474, Val Loss: 0.2697\n",
      "Epoch [1412/2000], Loss: 0.2473, Val Loss: 0.2698\n",
      "Epoch [1413/2000], Loss: 0.2473, Val Loss: 0.2695\n",
      "Epoch [1414/2000], Loss: 0.2473, Val Loss: 0.2700\n",
      "Epoch [1415/2000], Loss: 0.2473, Val Loss: 0.2695\n",
      "Epoch [1416/2000], Loss: 0.2473, Val Loss: 0.2700\n",
      "Epoch [1417/2000], Loss: 0.2473, Val Loss: 0.2698\n",
      "Epoch [1418/2000], Loss: 0.2473, Val Loss: 0.2697\n",
      "Epoch [1419/2000], Loss: 0.2472, Val Loss: 0.2700\n",
      "Epoch [1420/2000], Loss: 0.2472, Val Loss: 0.2695\n",
      "Epoch [1421/2000], Loss: 0.2471, Val Loss: 0.2701\n",
      "Epoch [1422/2000], Loss: 0.2470, Val Loss: 0.2696\n",
      "Epoch [1423/2000], Loss: 0.2470, Val Loss: 0.2700\n",
      "Epoch [1424/2000], Loss: 0.2470, Val Loss: 0.2696\n",
      "Epoch [1425/2000], Loss: 0.2469, Val Loss: 0.2698\n",
      "Epoch [1426/2000], Loss: 0.2469, Val Loss: 0.2697\n",
      "Epoch [1427/2000], Loss: 0.2469, Val Loss: 0.2697\n",
      "Epoch [1428/2000], Loss: 0.2469, Val Loss: 0.2698\n",
      "Epoch [1429/2000], Loss: 0.2469, Val Loss: 0.2695\n",
      "Epoch [1430/2000], Loss: 0.2468, Val Loss: 0.2698\n",
      "Epoch [1431/2000], Loss: 0.2468, Val Loss: 0.2694\n",
      "Epoch [1432/2000], Loss: 0.2467, Val Loss: 0.2700\n",
      "Epoch [1433/2000], Loss: 0.2467, Val Loss: 0.2694\n",
      "Epoch [1434/2000], Loss: 0.2467, Val Loss: 0.2702\n",
      "Epoch [1435/2000], Loss: 0.2467, Val Loss: 0.2693\n",
      "Epoch [1436/2000], Loss: 0.2466, Val Loss: 0.2702\n",
      "Epoch [1437/2000], Loss: 0.2466, Val Loss: 0.2694\n",
      "Epoch [1438/2000], Loss: 0.2466, Val Loss: 0.2702\n",
      "Epoch [1439/2000], Loss: 0.2466, Val Loss: 0.2696\n",
      "Epoch [1440/2000], Loss: 0.2465, Val Loss: 0.2701\n",
      "Epoch [1441/2000], Loss: 0.2465, Val Loss: 0.2697\n",
      "Epoch [1442/2000], Loss: 0.2464, Val Loss: 0.2700\n",
      "Epoch [1443/2000], Loss: 0.2464, Val Loss: 0.2698\n",
      "Epoch [1444/2000], Loss: 0.2463, Val Loss: 0.2699\n",
      "Epoch [1445/2000], Loss: 0.2463, Val Loss: 0.2697\n",
      "Epoch [1446/2000], Loss: 0.2463, Val Loss: 0.2699\n",
      "Epoch [1447/2000], Loss: 0.2463, Val Loss: 0.2699\n",
      "Epoch [1448/2000], Loss: 0.2463, Val Loss: 0.2698\n",
      "Epoch [1449/2000], Loss: 0.2463, Val Loss: 0.2700\n",
      "Epoch [1450/2000], Loss: 0.2463, Val Loss: 0.2698\n",
      "Epoch [1451/2000], Loss: 0.2463, Val Loss: 0.2702\n",
      "Epoch [1452/2000], Loss: 0.2463, Val Loss: 0.2695\n",
      "Epoch [1453/2000], Loss: 0.2463, Val Loss: 0.2702\n",
      "Epoch [1454/2000], Loss: 0.2463, Val Loss: 0.2696\n",
      "Epoch [1455/2000], Loss: 0.2462, Val Loss: 0.2700\n",
      "Epoch [1456/2000], Loss: 0.2462, Val Loss: 0.2701\n",
      "Epoch [1457/2000], Loss: 0.2462, Val Loss: 0.2697\n",
      "Epoch [1458/2000], Loss: 0.2462, Val Loss: 0.2707\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the autoencoder class\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size, encoding_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, encoding_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "\n",
    "# Create autoencoders for each feature group\n",
    "cum_autoencoder = Autoencoder(input_size=len(cum_features), encoding_dim=3)\n",
    "pm_autoencoder = Autoencoder(input_size=len(pm_features), encoding_dim=3)\n",
    "delta_autoencoder = Autoencoder(input_size=len(delta_features), encoding_dim=3)\n",
    "received_autoencoder = Autoencoder(input_size=len(received_features), encoding_dim=3)\n",
    "fight_autoencoder = Autoencoder(input_size=len(fight_features), encoding_dim=3)\n",
    "remaining_autoencoder = Autoencoder(input_size=len(remaining_features), encoding_dim=3)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizers for each autoencoder\n",
    "optim_cum = optim.Adam(cum_autoencoder.parameters(), lr=1e-3)\n",
    "optim_pm = optim.Adam(pm_autoencoder.parameters(), lr=1e-3)\n",
    "optim_delta = optim.Adam(delta_autoencoder.parameters(), lr=1e-3)\n",
    "optim_received = optim.Adam(received_autoencoder.parameters(), lr=1e-3)\n",
    "optim_fight = optim.Adam(fight_autoencoder.parameters(), lr=1e-3)\n",
    "optim_remaining = optim.Adam(remaining_autoencoder.parameters(), lr=1e-3)\n",
    "\n",
    "# Training function\n",
    "def train_autoencoder_with_early_stopping(autoencoder, optimizer, feature_data, val_data, epochs=2000, patience=10):\n",
    "    feature_tensor = torch.tensor(feature_data, dtype=torch.float)\n",
    "    val_tensor = torch.tensor(val_data, dtype=torch.float)\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=patience, min_delta=0.001)\n",
    "    for epoch in range(epochs):\n",
    "        autoencoder.train()\n",
    "        encoded, decoded = autoencoder(feature_tensor)\n",
    "        loss = criterion(decoded, feature_tensor)  # training loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validation step\n",
    "        autoencoder.eval()  # set to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            val_encoded, val_decoded = autoencoder(val_tensor)\n",
    "            val_loss = criterion(val_decoded, val_tensor)  # validation loss\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
    "        \n",
    "        # Check early stopping condition\n",
    "        early_stopping(val_loss.item())\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "# Train each autoencoder on their respective scaled feature sets from X_train_scaled\n",
    "train_autoencoder_with_early_stopping(pm_autoencoder, optim_pm, X_train_scaled[pm_features].values, abc_scaled[pm_features].values, epochs=2000, patience=100)\n",
    "train_autoencoder_with_early_stopping(delta_autoencoder, optim_delta, X_train_scaled[delta_features].values, abc_scaled[delta_features].values, epochs=2000, patience=100)\n",
    "train_autoencoder_with_early_stopping(received_autoencoder, optim_received, X_train_scaled[received_features].values, abc_scaled[received_features].values, epochs=2000, patience=100)\n",
    "train_autoencoder_with_early_stopping(fight_autoencoder, optim_fight, X_train_scaled[fight_features].values, abc_scaled[fight_features].values, epochs=2000, patience=10)\n",
    "train_autoencoder_with_early_stopping(remaining_autoencoder, optim_remaining, X_train_scaled[remaining_features].values, abc_scaled[remaining_features].values, epochs=2000, patience=100)\n",
    "\n",
    "# Encoding the scaled feature sets after training\n",
    "with torch.no_grad():\n",
    "    cum_encoded, _ = cum_autoencoder(torch.tensor(X_train_scaled[cum_features].values, dtype=torch.float))\n",
    "    pm_encoded, _ = pm_autoencoder(torch.tensor(X_train_scaled[pm_features].values, dtype=torch.float))\n",
    "    delta_encoded, _ = delta_autoencoder(torch.tensor(X_train_scaled[delta_features].values, dtype=torch.float))\n",
    "    received_encoded, _ = received_autoencoder(torch.tensor(X_train_scaled[received_features].values, dtype=torch.float))\n",
    "    fight_encoded, _ = fight_autoencoder(torch.tensor(X_train_scaled[fight_features].values, dtype=torch.float))\n",
    "    remaining_encoded, _ = remaining_autoencoder(torch.tensor(X_train_scaled[remaining_features].values, dtype=torch.float))\n",
    "\n",
    "# Concatenate all encoded features into a single tensor\n",
    "encoded_features = torch.cat([cum_encoded, pm_encoded, delta_encoded, received_encoded, fight_encoded, remaining_encoded], dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate encoded features after autoencoder training\n",
    "with torch.no_grad():\n",
    "    cum_encoded, _ = cum_autoencoder(torch.tensor(X_train_scaled[cum_features].values, dtype=torch.float))\n",
    "    pm_encoded, _ = pm_autoencoder(torch.tensor(X_train_scaled[pm_features].values, dtype=torch.float))\n",
    "    delta_encoded, _ = delta_autoencoder(torch.tensor(X_train_scaled[delta_features].values, dtype=torch.float))\n",
    "    received_encoded, _ = received_autoencoder(torch.tensor(X_train_scaled[received_features].values, dtype=torch.float))\n",
    "    fight_encoded, _ = fight_autoencoder(torch.tensor(X_train_scaled[fight_features].values, dtype=torch.float))\n",
    "    remaining_encoded, _ = remaining_autoencoder(torch.tensor(X_train_scaled[remaining_features].values, dtype=torch.float))\n",
    "\n",
    "    # Concatenate all encoded feature tensors\n",
    "    encoded_train_features = torch.cat([cum_encoded, pm_encoded, delta_encoded, received_encoded, fight_encoded, remaining_encoded], dim=1)\n",
    "\n",
    "# Do the same for the validation set\n",
    "with torch.no_grad():\n",
    "    cum_encoded_val, _ = cum_autoencoder(torch.tensor(X_val_scaled[cum_features].values, dtype=torch.float))\n",
    "    pm_encoded_val, _ = pm_autoencoder(torch.tensor(X_val_scaled[pm_features].values, dtype=torch.float))\n",
    "    delta_encoded_val, _ = delta_autoencoder(torch.tensor(X_val_scaled[delta_features].values, dtype=torch.float))\n",
    "    received_encoded_val, _ = received_autoencoder(torch.tensor(X_val_scaled[received_features].values, dtype=torch.float))\n",
    "    fight_encoded_val, _ = fight_autoencoder(torch.tensor(X_val_scaled[fight_features].values, dtype=torch.float))\n",
    "    remaining_encoded_val, _ = remaining_autoencoder(torch.tensor(X_val_scaled[remaining_features].values, dtype=torch.float))\n",
    "\n",
    "    # Concatenate all encoded feature tensors for validation\n",
    "    encoded_val_features = torch.cat([cum_encoded_val, pm_encoded_val, delta_encoded_val, received_encoded_val, fight_encoded_val, remaining_encoded_val], dim=1)\n",
    "\n",
    "# Do the same for the test set\n",
    "with torch.no_grad():\n",
    "    cum_encoded_test, _ = cum_autoencoder(torch.tensor(X_test_scaled[cum_features].values, dtype=torch.float))\n",
    "    pm_encoded_test, _ = pm_autoencoder(torch.tensor(X_test_scaled[pm_features].values, dtype=torch.float))\n",
    "    delta_encoded_test, _ = delta_autoencoder(torch.tensor(X_test_scaled[delta_features].values, dtype=torch.float))\n",
    "    received_encoded_test, _ = received_autoencoder(torch.tensor(X_test_scaled[received_features].values, dtype=torch.float))\n",
    "    fight_encoded_test, _ = fight_autoencoder(torch.tensor(X_test_scaled[fight_features].values, dtype=torch.float))\n",
    "    remaining_encoded_test, _ = remaining_autoencoder(torch.tensor(X_test_scaled[remaining_features].values, dtype=torch.float))\n",
    "\n",
    "    # Concatenate all encoded feature tensors for test\n",
    "    encoded_test_features = torch.cat([cum_encoded_test, pm_encoded_test, delta_encoded_test, received_encoded_test, fight_encoded_test, remaining_encoded_test], dim=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Autoencode the features in abc_scaled for each feature group\n",
    "    cum_encoded_abc, _ = cum_autoencoder(torch.tensor(abc_scaled[cum_features].values, dtype=torch.float))\n",
    "    pm_encoded_abc, _ = pm_autoencoder(torch.tensor(abc_scaled[pm_features].values, dtype=torch.float))\n",
    "    delta_encoded_abc, _ = delta_autoencoder(torch.tensor(abc_scaled[delta_features].values, dtype=torch.float))\n",
    "    received_encoded_abc, _ = received_autoencoder(torch.tensor(abc_scaled[received_features].values, dtype=torch.float))\n",
    "    fight_encoded_abc, _ = fight_autoencoder(torch.tensor(abc_scaled[fight_features].values, dtype=torch.float))\n",
    "    remaining_encoded_abc, _ = remaining_autoencoder(torch.tensor(abc_scaled[remaining_features].values, dtype=torch.float))\n",
    "\n",
    "    # Concatenate all encoded feature tensors for abc_scaled\n",
    "    encoded_abc_features = torch.cat([cum_encoded_abc, pm_encoded_abc, delta_encoded_abc, received_encoded_abc, fight_encoded_abc, remaining_encoded_abc], dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1397,  0.0487,  0.1281,  ...,  1.1624,  0.6920, -1.2212],\n",
       "        [ 0.1587,  0.0219,  0.0122,  ..., -0.4199,  0.6297,  0.1459],\n",
       "        [ 0.1335, -0.0168,  0.0246,  ..., -0.3717,  0.3906,  0.1171],\n",
       "        ...,\n",
       "        [ 0.0582,  0.0116,  0.0631,  ..., -0.7518, -0.7134,  0.2909],\n",
       "        [-0.0047,  0.9855,  0.3002,  ...,  4.0577, -1.0446,  1.0318],\n",
       "        [-0.0231, -0.1708,  0.1131,  ...,  1.1970, -3.5174, -2.6160]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.nn import PyroModule, PyroSample\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "pyro.clear_param_store()\n",
    "\n",
    "class SimpleBayesianNN(PyroModule):\n",
    "    def __init__(self, input_size, hidden_size, output_size, means, stds):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden1 = PyroModule[torch.nn.Linear](input_size, hidden_size)\n",
    "        self.hidden1.weight = PyroSample(dist.Normal(means, stds).expand([hidden_size, input_size]).to_event(2))\n",
    "        self.hidden1.bias = PyroSample(dist.Normal(means.mean(), stds.mean()).expand([hidden_size]).to_event(1))\n",
    "\n",
    "        # Residual layer to match dimensions between x and hidden1\n",
    "        self.residual_layer = PyroModule[torch.nn.Linear](input_size, hidden_size)\n",
    "\n",
    "        self.output = PyroModule[torch.nn.Linear](hidden_size, output_size)\n",
    "        self.output.weight = PyroSample(dist.Normal(0., .1).expand([output_size, hidden_size]).to_event(2))\n",
    "        self.output.bias = PyroSample(dist.Normal(0., .1).expand([output_size]).to_event(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden1 = F.leaky_relu(self.hidden1(x))\n",
    "        hidden1 = F.dropout(hidden1, p=0.5, training=self.training)\n",
    "        \n",
    "        residual = self.residual_layer(x)  # Match dimensions for residual connection\n",
    "        output = self.output(hidden1 + residual)\n",
    "        return output\n",
    "    \n",
    "\n",
    "def model(x_data, y_data=None):\n",
    "    logits = bnn(x_data).squeeze(-1)\n",
    "    probabilities = torch.sigmoid(logits)  \n",
    "    with pyro.plate(\"data\", x_data.shape[0]):\n",
    "        pyro.sample(\"obs\", dist.Bernoulli(probs=probabilities), obs=y_data)\n",
    "    return logits\n",
    "\n",
    "\n",
    "means = encoded_abc_features.mean(dim=0)\n",
    "stds = encoded_abc_features.std(dim=0)\n",
    "\n",
    "guide = pyro.infer.autoguide.AutoDiagonalNormal(model)  \n",
    "\n",
    "bnn = SimpleBayesianNN(input_size=encoded_train_features.shape[1], hidden_size=200, output_size=1, means=means, stds=stds)\n",
    "\n",
    "\n",
    "# Use encoded training, validation, and test data in the BNN\n",
    "x_tensor = encoded_train_features  # Encoded training features\n",
    "y_tensor = torch.tensor(y_train.values, dtype=torch.float)\n",
    "\n",
    "x_tensor_val = encoded_val_features  # Encoded validation features\n",
    "y_tensor_val = torch.tensor(y_val.values, dtype=torch.float)\n",
    "\n",
    "x_tensor_test = encoded_test_features  # Encoded test features\n",
    "y_tensor_test = torch.tensor(y_test.values, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1397,  0.0487,  0.1281,  ...,  1.1624,  0.6920, -1.2212],\n",
       "        [ 0.1587,  0.0219,  0.0122,  ..., -0.4199,  0.6297,  0.1459],\n",
       "        [ 0.1335, -0.0168,  0.0246,  ..., -0.3717,  0.3906,  0.1171],\n",
       "        ...,\n",
       "        [ 0.0582,  0.0116,  0.0631,  ..., -0.7518, -0.7134,  0.2909],\n",
       "        [-0.0047,  0.9855,  0.3002,  ...,  4.0577, -1.0446,  1.0318],\n",
       "        [-0.0231, -0.1708,  0.1131,  ...,  1.1970, -3.5174, -2.6160]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=500, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "\n",
    "def save_checkpoint(model, pyro_optimizer, scheduler, epoch, loss, path=\"checkpoint.pth\"):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': pyro_optimizer.get_state(),  \n",
    "        'scheduler_state_dict': scheduler.scheduler.state_dict(),  \n",
    "        'loss': loss\n",
    "    }, path)\n",
    "    print(f\"Checkpoint saved at epoch {epoch}.\")\n",
    "\n",
    "def load_checkpoint(model, pyro_optimizer, scheduler, path=\"checkpoint.pth\"):\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    pyro_optimizer.set_state(checkpoint['optimizer_state_dict'])  \n",
    "    scheduler.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])  \n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    print(f\"Checkpoint loaded. Resuming from epoch {epoch} with loss {loss:.4f}\")\n",
    "    return epoch, loss\n",
    "\n",
    "def load_model_only(model, path=\"checkpoint.pth\"):\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "\n",
    "def save_best_model(model, val_loss, best_val_loss, model_save_path=\"best_model.pth\"):\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"Best model saved with val_loss: {val_loss:.4f}\")\n",
    "    return best_val_loss\n",
    "\n",
    "def load_best_model(model, model_save_path=\"best_model.pth\"):\n",
    "    model.load_state_dict(torch.load(model_save_path))\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    print(\"Best model loaded.\")\n",
    "\n",
    "\n",
    "early_stopper = EarlyStopper(patience=2000, min_delta=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at epoch 0 with validation loss 7.925364017486572\n",
      "Epoch 0: Train Loss = 33443.0229, Val Loss = 7.9254, Best is = 7.9254\n",
      "Model saved at epoch 2 with validation loss 7.426735877990723\n",
      "Model saved at epoch 3 with validation loss 6.118226528167725\n",
      "Model saved at epoch 7 with validation loss 5.821559429168701\n",
      "Model saved at epoch 9 with validation loss 5.725302696228027\n",
      "Model saved at epoch 10 with validation loss 5.664477348327637\n",
      "Model saved at epoch 32 with validation loss 5.159912109375\n",
      "Epoch 100: Train Loss = 35233.7414, Val Loss = 5.5330, Best is = 5.1599\n",
      "Epoch 200: Train Loss = 41798.2016, Val Loss = 7.3881, Best is = 5.1599\n",
      "Model saved at epoch 237 with validation loss 4.951593399047852\n",
      "Model saved at epoch 266 with validation loss 4.870832443237305\n",
      "Epoch 300: Train Loss = 29699.3356, Val Loss = 10.0882, Best is = 4.8708\n",
      "Model saved at epoch 341 with validation loss 4.727814674377441\n",
      "Epoch 400: Train Loss = 29781.3361, Val Loss = 7.1684, Best is = 4.7278\n",
      "Epoch 500: Train Loss = 30963.4916, Val Loss = 5.6459, Best is = 4.7278\n",
      "Epoch 600: Train Loss = 34663.0318, Val Loss = 6.5031, Best is = 4.7278\n",
      "Model saved at epoch 626 with validation loss 4.648128509521484\n",
      "Model saved at epoch 634 with validation loss 4.5338850021362305\n",
      "Epoch 700: Train Loss = 30521.4571, Val Loss = 8.4756, Best is = 4.5339\n",
      "Model saved at epoch 765 with validation loss 4.407779216766357\n",
      "Epoch 800: Train Loss = 29124.6560, Val Loss = 7.4697, Best is = 4.4078\n",
      "Epoch 900: Train Loss = 27582.5010, Val Loss = 6.9299, Best is = 4.4078\n",
      "Epoch 1000: Train Loss = 25197.0220, Val Loss = 5.8908, Best is = 4.4078\n",
      "Epoch 1100: Train Loss = 24449.7697, Val Loss = 8.0315, Best is = 4.4078\n",
      "Epoch 1200: Train Loss = 28036.0229, Val Loss = 7.5205, Best is = 4.4078\n",
      "Epoch 1300: Train Loss = 23214.2507, Val Loss = 11.0916, Best is = 4.4078\n",
      "Epoch 1400: Train Loss = 31158.6342, Val Loss = 7.6713, Best is = 4.4078\n",
      "Epoch 1500: Train Loss = 34382.9034, Val Loss = 6.0818, Best is = 4.4078\n",
      "Epoch 1600: Train Loss = 23694.1865, Val Loss = 7.8964, Best is = 4.4078\n",
      "Epoch 1700: Train Loss = 22485.0881, Val Loss = 6.8538, Best is = 4.4078\n",
      "Model saved at epoch 1724 with validation loss 4.2043681144714355\n",
      "Epoch 1800: Train Loss = 23087.0090, Val Loss = 6.0895, Best is = 4.2044\n",
      "Epoch 1900: Train Loss = 22300.7476, Val Loss = 5.4234, Best is = 4.2044\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.5501)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyro.optim import PyroOptim\n",
    "from pyro.optim.lr_scheduler import PyroLRScheduler\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = PyroOptim(optim.Adam, {\"lr\": 1e-3, \"weight_decay\": 1e-3})\n",
    "scheduler = pyro.optim.ReduceLROnPlateau({\n",
    "    'optimizer': optimizer,\n",
    "    'optim_args': {'lr': 0.01},  # Learning rate arguments\n",
    "    'factor': 1,               # Learning rate reduction factor\n",
    "    'patience': 50,              # Number of epochs with no improvement before reducing LR\n",
    "    'verbose': True,             # Enable logging of LR updates\n",
    "    'mode': 'min',               # Reduce LR when a monitored metric has stopped decreasing\n",
    "    'gamma': 0.1                 # The same gamma factor\n",
    "})\n",
    "\n",
    "\n",
    "svi = SVI(model, guide, optimizer, loss=Trace_ELBO())\n",
    "\n",
    "def focal_loss(logits, targets, alpha=0.25, gamma=2):\n",
    "    BCE_loss = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')\n",
    "    pt = torch.exp(-BCE_loss)\n",
    "    F_loss = alpha * (1-pt)**gamma * BCE_loss\n",
    "    return F_loss.mean()\n",
    "\n",
    "def compute_validation_loss(x_test, y_test):\n",
    "    with torch.no_grad():\n",
    "        logits = bnn(x_test).squeeze(-1)\n",
    "        val_loss = focal_loss(logits, y_test)\n",
    "    return val_loss.item()\n",
    "\n",
    "def compute_validation_loss_no_foc(x_test, y_test):\n",
    "    with torch.no_grad():\n",
    "        logits = bnn(x_test).squeeze(-1)\n",
    "        val_loss = F.binary_cross_entropy_with_logits(logits, y_test)\n",
    "    return val_loss.item()\n",
    "\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "num_epochs = 2000\n",
    "checkpoint_path = 'best_model_AE.pth'\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = svi.step(x_tensor, y_tensor)\n",
    "    val_loss = compute_validation_loss_no_foc(x_tensor_test, y_tensor_test)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(bnn.state_dict(), \"best_model_AE.pth\")  \n",
    "        print(f\"Model saved at epoch {epoch} with validation loss {val_loss}\")\n",
    "\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Best is = {best_val_loss:.4f}\")\n",
    "\n",
    "bnn.load_state_dict(torch.load(\"best_model_AE.pth\"))\n",
    "bnn.eval()\n",
    "\n",
    "\n",
    "predictive = pyro.infer.Predictive(model, guide=guide, num_samples=5000, return_sites=[\"_RETURN\", 'obs'])\n",
    "predictions = predictive(x_tensor, None)\n",
    "logits_samples = predictions[\"_RETURN\"]\n",
    "probability_samples = torch.sigmoid(logits_samples)\n",
    "mean_probabilities = torch.mean(probability_samples, dim=0)\n",
    "predicted_labels = (mean_probabilities > 0.5).float()\n",
    "accuracy = (predicted_labels == y_tensor).float().mean()\n",
    "accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bound = torch.quantile(probability_samples, 0.025, dim=0)  \n",
    "upper_bound = torch.quantile(probability_samples, 0.975, dim=0)\n",
    "std = torch.std(probability_samples, dim=0)\n",
    "skew = torch.mean(((probability_samples - mean_probabilities) ** 3), dim=0) / (std ** 3)\n",
    "kurtosis = torch.mean(((probability_samples - mean_probabilities) ** 4), dim=0) / (std ** 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>proba</th>\n",
       "      <th>lower</th>\n",
       "      <th>upper</th>\n",
       "      <th>std</th>\n",
       "      <th>skew</th>\n",
       "      <th>kurtosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.297021</td>\n",
       "      <td>5.337259e-02</td>\n",
       "      <td>0.704958</td>\n",
       "      <td>0.173565</td>\n",
       "      <td>0.746071</td>\n",
       "      <td>3.021590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.728791</td>\n",
       "      <td>3.467245e-04</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.381522</td>\n",
       "      <td>-1.026297</td>\n",
       "      <td>2.307832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.752906</td>\n",
       "      <td>6.821339e-03</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.340500</td>\n",
       "      <td>-1.160038</td>\n",
       "      <td>2.773231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.760682</td>\n",
       "      <td>2.911244e-11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.407884</td>\n",
       "      <td>-1.221747</td>\n",
       "      <td>2.573428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.788893</td>\n",
       "      <td>3.637199e-08</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.383072</td>\n",
       "      <td>-1.411269</td>\n",
       "      <td>3.120392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9015</th>\n",
       "      <td>0.503126</td>\n",
       "      <td>3.525460e-01</td>\n",
       "      <td>0.654097</td>\n",
       "      <td>0.077337</td>\n",
       "      <td>0.028352</td>\n",
       "      <td>2.876169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9016</th>\n",
       "      <td>0.763365</td>\n",
       "      <td>2.576610e-12</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.408574</td>\n",
       "      <td>-1.238908</td>\n",
       "      <td>2.607782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9017</th>\n",
       "      <td>0.490445</td>\n",
       "      <td>3.691326e-01</td>\n",
       "      <td>0.617458</td>\n",
       "      <td>0.064624</td>\n",
       "      <td>0.013716</td>\n",
       "      <td>2.781718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9018</th>\n",
       "      <td>0.459864</td>\n",
       "      <td>2.023696e-01</td>\n",
       "      <td>0.736926</td>\n",
       "      <td>0.140523</td>\n",
       "      <td>0.113887</td>\n",
       "      <td>2.501379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9019</th>\n",
       "      <td>0.486781</td>\n",
       "      <td>2.365324e-01</td>\n",
       "      <td>0.739696</td>\n",
       "      <td>0.130815</td>\n",
       "      <td>0.030833</td>\n",
       "      <td>2.646132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9020 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         proba         lower     upper       std      skew  kurtosis\n",
       "0     0.297021  5.337259e-02  0.704958  0.173565  0.746071  3.021590\n",
       "1     0.728791  3.467245e-04  1.000000  0.381522 -1.026297  2.307832\n",
       "2     0.752906  6.821339e-03  0.999990  0.340500 -1.160038  2.773231\n",
       "3     0.760682  2.911244e-11  1.000000  0.407884 -1.221747  2.573428\n",
       "4     0.788893  3.637199e-08  1.000000  0.383072 -1.411269  3.120392\n",
       "...        ...           ...       ...       ...       ...       ...\n",
       "9015  0.503126  3.525460e-01  0.654097  0.077337  0.028352  2.876169\n",
       "9016  0.763365  2.576610e-12  1.000000  0.408574 -1.238908  2.607782\n",
       "9017  0.490445  3.691326e-01  0.617458  0.064624  0.013716  2.781718\n",
       "9018  0.459864  2.023696e-01  0.736926  0.140523  0.113887  2.501379\n",
       "9019  0.486781  2.365324e-01  0.739696  0.130815  0.030833  2.646132\n",
       "\n",
       "[9020 rows x 6 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.column_stack((mean_probabilities.numpy(), lower_bound.numpy(), upper_bound.numpy(), std.numpy(), skew.numpy(), kurtosis.numpy()))\n",
    "datafr = pd.DataFrame(data, columns = ['proba', 'lower', 'upper', 'std', 'skew', 'kurtosis'])\n",
    "datafr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictive(x_tensor_val, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5473)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_samples = predictions[\"_RETURN\"]\n",
    "probability_samples = torch.sigmoid(logits_samples)\n",
    "mean_probabilities = torch.mean(probability_samples, dim=0)\n",
    "predicted_labels = (mean_probabilities > 0.5).float()\n",
    "\n",
    "accuracy = (predicted_labels == y_tensor_val).float().mean()\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bound = torch.quantile(probability_samples, 0.025, dim=0)  \n",
    "upper_bound = torch.quantile(probability_samples, 0.975, dim=0)\n",
    "std = torch.std(probability_samples, dim=0)\n",
    "skew = torch.mean(((probability_samples - mean_probabilities) ** 3), dim=0) / (std ** 3)\n",
    "kurtosis = torch.mean(((probability_samples - mean_probabilities) ** 4), dim=0) / (std ** 4)\n",
    "median = torch.median(probability_samples, dim=0)\n",
    "\n",
    "\n",
    "datafr = {\n",
    "    'proba': mean_probabilities.numpy(),\n",
    "    'median': median.values.numpy(),\n",
    "    'lower': lower_bound.numpy(),\n",
    "    'upper': upper_bound.numpy(),\n",
    "    'std': std.numpy(),\n",
    "    'skew': skew.numpy(),\n",
    "    'kurtosis': kurtosis.numpy(),\n",
    "    'trace': [probability_samples[i] for i in range(mean_probabilities.shape[0])] \n",
    "}\n",
    "\n",
    "datafr = pd.DataFrame(datafr)\n",
    "\n",
    "datafr = datafr.reset_index(drop =  True)\n",
    "\n",
    "extra_val = df[(df['DATE'] >= '2024')]\n",
    "extra_val = extra_val.reset_index(drop = True)\n",
    "\n",
    "xtra_feats =['EVENT',\n",
    " 'BOUT',\n",
    " 'OUTCOME',\n",
    " 'WEIGHTCLASS',\n",
    " 'METHOD',\n",
    " 'ROUND',\n",
    " 'TIME',\n",
    " 'TIME FORMAT',\n",
    " 'REFEREE',\n",
    " 'DETAILS',\n",
    " 'URL',\n",
    " 'FIGHTER',\n",
    " 'RESULT',\n",
    " 'TIME_IN_SECONDS',\n",
    " 'DATE',\n",
    " 'ELO_FIGHTER',]\n",
    "\n",
    "extra_val = pd.concat([extra_val[xtra_feats], datafr], axis = 1)\n",
    "extra_val.to_csv(r'ProbaPlus_AE.csv')\n",
    "extra_val['Estimate'] = np.where(extra_val['proba'] > .5,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EVENT</th>\n",
       "      <th>BOUT</th>\n",
       "      <th>OUTCOME</th>\n",
       "      <th>WEIGHTCLASS</th>\n",
       "      <th>METHOD</th>\n",
       "      <th>ROUND</th>\n",
       "      <th>TIME</th>\n",
       "      <th>TIME FORMAT</th>\n",
       "      <th>REFEREE</th>\n",
       "      <th>DETAILS</th>\n",
       "      <th>...</th>\n",
       "      <th>ELO_FIGHTER</th>\n",
       "      <th>proba</th>\n",
       "      <th>median</th>\n",
       "      <th>lower</th>\n",
       "      <th>upper</th>\n",
       "      <th>std</th>\n",
       "      <th>skew</th>\n",
       "      <th>kurtosis</th>\n",
       "      <th>trace</th>\n",
       "      <th>Estimate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UFCFightNight:Ankalaevvs.Walker2</td>\n",
       "      <td>MagomedAnkalaevvs.JohnnyWalker</td>\n",
       "      <td>W/L</td>\n",
       "      <td>Light Heavyweight Bout</td>\n",
       "      <td>KO/TKO</td>\n",
       "      <td>2</td>\n",
       "      <td>2:42</td>\n",
       "      <td>5 Rnd (5-5-5-5-5)</td>\n",
       "      <td>Marc Goddard</td>\n",
       "      <td>Punch to Head At Distance</td>\n",
       "      <td>...</td>\n",
       "      <td>1726.390024</td>\n",
       "      <td>0.669927</td>\n",
       "      <td>0.759190</td>\n",
       "      <td>0.064609</td>\n",
       "      <td>0.993731</td>\n",
       "      <td>0.288368</td>\n",
       "      <td>-0.690849</td>\n",
       "      <td>2.210196</td>\n",
       "      <td>[tensor(0.5757), tensor(0.2977), tensor(0.4158...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UFCFightNight:Ankalaevvs.Walker2</td>\n",
       "      <td>JimMillervs.GabrielBenitez</td>\n",
       "      <td>W/L</td>\n",
       "      <td>Lightweight Bout</td>\n",
       "      <td>Submission</td>\n",
       "      <td>3</td>\n",
       "      <td>3:25</td>\n",
       "      <td>3 Rnd (5-5-5)</td>\n",
       "      <td>Dan Miragliotta</td>\n",
       "      <td>Neck Crank From Back Control</td>\n",
       "      <td>...</td>\n",
       "      <td>1509.527752</td>\n",
       "      <td>0.487733</td>\n",
       "      <td>0.483612</td>\n",
       "      <td>0.130990</td>\n",
       "      <td>0.867594</td>\n",
       "      <td>0.202510</td>\n",
       "      <td>0.071405</td>\n",
       "      <td>2.165578</td>\n",
       "      <td>[tensor(0.5206), tensor(0.3033), tensor(0.5628...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UFCFightNight:Ankalaevvs.Walker2</td>\n",
       "      <td>JimMillervs.GabrielBenitez</td>\n",
       "      <td>W/L</td>\n",
       "      <td>Lightweight Bout</td>\n",
       "      <td>Submission</td>\n",
       "      <td>3</td>\n",
       "      <td>3:25</td>\n",
       "      <td>3 Rnd (5-5-5)</td>\n",
       "      <td>Dan Miragliotta</td>\n",
       "      <td>Neck Crank From Back Control</td>\n",
       "      <td>...</td>\n",
       "      <td>1606.672231</td>\n",
       "      <td>0.313769</td>\n",
       "      <td>0.274243</td>\n",
       "      <td>0.038304</td>\n",
       "      <td>0.770339</td>\n",
       "      <td>0.201599</td>\n",
       "      <td>0.708433</td>\n",
       "      <td>2.773551</td>\n",
       "      <td>[tensor(0.9892), tensor(0.8510), tensor(0.2175...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UFCFightNight:Ankalaevvs.Walker2</td>\n",
       "      <td>MagomedAnkalaevvs.JohnnyWalker</td>\n",
       "      <td>W/L</td>\n",
       "      <td>Light Heavyweight Bout</td>\n",
       "      <td>KO/TKO</td>\n",
       "      <td>2</td>\n",
       "      <td>2:42</td>\n",
       "      <td>5 Rnd (5-5-5-5-5)</td>\n",
       "      <td>Marc Goddard</td>\n",
       "      <td>Punch to Head At Distance</td>\n",
       "      <td>...</td>\n",
       "      <td>1622.106542</td>\n",
       "      <td>0.575931</td>\n",
       "      <td>0.582008</td>\n",
       "      <td>0.323868</td>\n",
       "      <td>0.799449</td>\n",
       "      <td>0.124302</td>\n",
       "      <td>-0.196757</td>\n",
       "      <td>2.651780</td>\n",
       "      <td>[tensor(0.8763), tensor(0.5415), tensor(0.1044...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UFCFightNight:Ankalaevvs.Walker2</td>\n",
       "      <td>MatthewSemelsbergervs.PrestonParsons</td>\n",
       "      <td>L/W</td>\n",
       "      <td>Welterweight Bout</td>\n",
       "      <td>Decision - Unanimous</td>\n",
       "      <td>3</td>\n",
       "      <td>5:00</td>\n",
       "      <td>3 Rnd (5-5-5)</td>\n",
       "      <td>Dan Miragliotta</td>\n",
       "      <td>Mike Bell 27 - 30.Junichiro Kamijo 27 - 30.Ron...</td>\n",
       "      <td>...</td>\n",
       "      <td>1482.314337</td>\n",
       "      <td>0.811297</td>\n",
       "      <td>0.986142</td>\n",
       "      <td>0.014689</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.311524</td>\n",
       "      <td>-1.578840</td>\n",
       "      <td>3.980455</td>\n",
       "      <td>[tensor(0.8357), tensor(0.6041), tensor(0.2870...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>UFCFightNight:Sandhagenvs.Nurmagomedov</td>\n",
       "      <td>MackenzieDernvs.LoopyGodinez</td>\n",
       "      <td>W/L</td>\n",
       "      <td>Women's Strawweight Bout</td>\n",
       "      <td>Decision - Unanimous</td>\n",
       "      <td>3</td>\n",
       "      <td>5:00</td>\n",
       "      <td>3 Rnd (5-5-5)</td>\n",
       "      <td>Greg Kleynjans</td>\n",
       "      <td>Hadi Mohamed Ali 28 - 29.Howard Hughes 28 - 29...</td>\n",
       "      <td>...</td>\n",
       "      <td>1555.735723</td>\n",
       "      <td>0.372387</td>\n",
       "      <td>0.355724</td>\n",
       "      <td>0.121861</td>\n",
       "      <td>0.701482</td>\n",
       "      <td>0.153146</td>\n",
       "      <td>0.409076</td>\n",
       "      <td>2.603126</td>\n",
       "      <td>[tensor(0.7148), tensor(0.4332), tensor(0.6254...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>UFCFightNight:Sandhagenvs.Nurmagomedov</td>\n",
       "      <td>TonyFergusonvs.MichaelChiesa</td>\n",
       "      <td>L/W</td>\n",
       "      <td>Welterweight Bout</td>\n",
       "      <td>Submission</td>\n",
       "      <td>1</td>\n",
       "      <td>3:44</td>\n",
       "      <td>3 Rnd (5-5-5)</td>\n",
       "      <td>Marc Goddard</td>\n",
       "      <td>Rear Naked Choke</td>\n",
       "      <td>...</td>\n",
       "      <td>1578.089046</td>\n",
       "      <td>0.330777</td>\n",
       "      <td>0.315402</td>\n",
       "      <td>0.125038</td>\n",
       "      <td>0.619703</td>\n",
       "      <td>0.128477</td>\n",
       "      <td>0.550786</td>\n",
       "      <td>2.947959</td>\n",
       "      <td>[tensor(0.7867), tensor(0.6063), tensor(0.4453...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>UFCFightNight:Sandhagenvs.Nurmagomedov</td>\n",
       "      <td>TonyFergusonvs.MichaelChiesa</td>\n",
       "      <td>L/W</td>\n",
       "      <td>Welterweight Bout</td>\n",
       "      <td>Submission</td>\n",
       "      <td>1</td>\n",
       "      <td>3:44</td>\n",
       "      <td>3 Rnd (5-5-5)</td>\n",
       "      <td>Marc Goddard</td>\n",
       "      <td>Rear Naked Choke</td>\n",
       "      <td>...</td>\n",
       "      <td>1622.714126</td>\n",
       "      <td>0.647541</td>\n",
       "      <td>0.739231</td>\n",
       "      <td>0.039999</td>\n",
       "      <td>0.995035</td>\n",
       "      <td>0.307114</td>\n",
       "      <td>-0.618858</td>\n",
       "      <td>2.037477</td>\n",
       "      <td>[tensor(0.5761), tensor(0.9245), tensor(0.2913...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>UFCFightNight:Sandhagenvs.Nurmagomedov</td>\n",
       "      <td>MarlonVeravs.DeivesonFigueiredo</td>\n",
       "      <td>L/W</td>\n",
       "      <td>Bantamweight Bout</td>\n",
       "      <td>Decision - Unanimous</td>\n",
       "      <td>3</td>\n",
       "      <td>5:00</td>\n",
       "      <td>3 Rnd (5-5-5)</td>\n",
       "      <td>Keith Peterson</td>\n",
       "      <td>Ben Cartlidge 28 - 29.Hadi Mohamed Ali 28 - 29...</td>\n",
       "      <td>...</td>\n",
       "      <td>1684.684440</td>\n",
       "      <td>0.476529</td>\n",
       "      <td>0.467581</td>\n",
       "      <td>0.115009</td>\n",
       "      <td>0.867519</td>\n",
       "      <td>0.203825</td>\n",
       "      <td>0.113106</td>\n",
       "      <td>2.238836</td>\n",
       "      <td>[tensor(0.2567), tensor(0.6581), tensor(0.4311...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>UFCFightNight:Sandhagenvs.Nurmagomedov</td>\n",
       "      <td>MackenzieDernvs.LoopyGodinez</td>\n",
       "      <td>W/L</td>\n",
       "      <td>Women's Strawweight Bout</td>\n",
       "      <td>Decision - Unanimous</td>\n",
       "      <td>3</td>\n",
       "      <td>5:00</td>\n",
       "      <td>3 Rnd (5-5-5)</td>\n",
       "      <td>Greg Kleynjans</td>\n",
       "      <td>Hadi Mohamed Ali 28 - 29.Howard Hughes 28 - 29...</td>\n",
       "      <td>...</td>\n",
       "      <td>1582.243272</td>\n",
       "      <td>0.526487</td>\n",
       "      <td>0.526828</td>\n",
       "      <td>0.331778</td>\n",
       "      <td>0.717604</td>\n",
       "      <td>0.099837</td>\n",
       "      <td>-0.060892</td>\n",
       "      <td>2.661362</td>\n",
       "      <td>[tensor(0.3509), tensor(0.8979), tensor(0.2155...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>296 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      EVENT  \\\n",
       "0          UFCFightNight:Ankalaevvs.Walker2   \n",
       "1          UFCFightNight:Ankalaevvs.Walker2   \n",
       "2          UFCFightNight:Ankalaevvs.Walker2   \n",
       "3          UFCFightNight:Ankalaevvs.Walker2   \n",
       "4          UFCFightNight:Ankalaevvs.Walker2   \n",
       "..                                      ...   \n",
       "291  UFCFightNight:Sandhagenvs.Nurmagomedov   \n",
       "292  UFCFightNight:Sandhagenvs.Nurmagomedov   \n",
       "293  UFCFightNight:Sandhagenvs.Nurmagomedov   \n",
       "294  UFCFightNight:Sandhagenvs.Nurmagomedov   \n",
       "295  UFCFightNight:Sandhagenvs.Nurmagomedov   \n",
       "\n",
       "                                     BOUT OUTCOME               WEIGHTCLASS  \\\n",
       "0          MagomedAnkalaevvs.JohnnyWalker     W/L    Light Heavyweight Bout   \n",
       "1              JimMillervs.GabrielBenitez     W/L          Lightweight Bout   \n",
       "2              JimMillervs.GabrielBenitez     W/L          Lightweight Bout   \n",
       "3          MagomedAnkalaevvs.JohnnyWalker     W/L    Light Heavyweight Bout   \n",
       "4    MatthewSemelsbergervs.PrestonParsons     L/W         Welterweight Bout   \n",
       "..                                    ...     ...                       ...   \n",
       "291          MackenzieDernvs.LoopyGodinez     W/L  Women's Strawweight Bout   \n",
       "292          TonyFergusonvs.MichaelChiesa     L/W         Welterweight Bout   \n",
       "293          TonyFergusonvs.MichaelChiesa     L/W         Welterweight Bout   \n",
       "294       MarlonVeravs.DeivesonFigueiredo     L/W         Bantamweight Bout   \n",
       "295          MackenzieDernvs.LoopyGodinez     W/L  Women's Strawweight Bout   \n",
       "\n",
       "                    METHOD  ROUND  TIME        TIME FORMAT          REFEREE  \\\n",
       "0                  KO/TKO       2  2:42  5 Rnd (5-5-5-5-5)     Marc Goddard   \n",
       "1              Submission       3  3:25      3 Rnd (5-5-5)  Dan Miragliotta   \n",
       "2              Submission       3  3:25      3 Rnd (5-5-5)  Dan Miragliotta   \n",
       "3                  KO/TKO       2  2:42  5 Rnd (5-5-5-5-5)     Marc Goddard   \n",
       "4    Decision - Unanimous       3  5:00      3 Rnd (5-5-5)  Dan Miragliotta   \n",
       "..                     ...    ...   ...                ...              ...   \n",
       "291  Decision - Unanimous       3  5:00      3 Rnd (5-5-5)   Greg Kleynjans   \n",
       "292            Submission       1  3:44      3 Rnd (5-5-5)     Marc Goddard   \n",
       "293            Submission       1  3:44      3 Rnd (5-5-5)     Marc Goddard   \n",
       "294  Decision - Unanimous       3  5:00      3 Rnd (5-5-5)   Keith Peterson   \n",
       "295  Decision - Unanimous       3  5:00      3 Rnd (5-5-5)   Greg Kleynjans   \n",
       "\n",
       "                                               DETAILS  ...  ELO_FIGHTER  \\\n",
       "0                           Punch to Head At Distance   ...  1726.390024   \n",
       "1                        Neck Crank From Back Control   ...  1509.527752   \n",
       "2                        Neck Crank From Back Control   ...  1606.672231   \n",
       "3                           Punch to Head At Distance   ...  1622.106542   \n",
       "4    Mike Bell 27 - 30.Junichiro Kamijo 27 - 30.Ron...  ...  1482.314337   \n",
       "..                                                 ...  ...          ...   \n",
       "291  Hadi Mohamed Ali 28 - 29.Howard Hughes 28 - 29...  ...  1555.735723   \n",
       "292                                  Rear Naked Choke   ...  1578.089046   \n",
       "293                                  Rear Naked Choke   ...  1622.714126   \n",
       "294  Ben Cartlidge 28 - 29.Hadi Mohamed Ali 28 - 29...  ...  1684.684440   \n",
       "295  Hadi Mohamed Ali 28 - 29.Howard Hughes 28 - 29...  ...  1582.243272   \n",
       "\n",
       "        proba    median     lower     upper       std      skew  kurtosis  \\\n",
       "0    0.669927  0.759190  0.064609  0.993731  0.288368 -0.690849  2.210196   \n",
       "1    0.487733  0.483612  0.130990  0.867594  0.202510  0.071405  2.165578   \n",
       "2    0.313769  0.274243  0.038304  0.770339  0.201599  0.708433  2.773551   \n",
       "3    0.575931  0.582008  0.323868  0.799449  0.124302 -0.196757  2.651780   \n",
       "4    0.811297  0.986142  0.014689  0.999997  0.311524 -1.578840  3.980455   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "291  0.372387  0.355724  0.121861  0.701482  0.153146  0.409076  2.603126   \n",
       "292  0.330777  0.315402  0.125038  0.619703  0.128477  0.550786  2.947959   \n",
       "293  0.647541  0.739231  0.039999  0.995035  0.307114 -0.618858  2.037477   \n",
       "294  0.476529  0.467581  0.115009  0.867519  0.203825  0.113106  2.238836   \n",
       "295  0.526487  0.526828  0.331778  0.717604  0.099837 -0.060892  2.661362   \n",
       "\n",
       "                                                 trace  Estimate  \n",
       "0    [tensor(0.5757), tensor(0.2977), tensor(0.4158...         1  \n",
       "1    [tensor(0.5206), tensor(0.3033), tensor(0.5628...         0  \n",
       "2    [tensor(0.9892), tensor(0.8510), tensor(0.2175...         0  \n",
       "3    [tensor(0.8763), tensor(0.5415), tensor(0.1044...         1  \n",
       "4    [tensor(0.8357), tensor(0.6041), tensor(0.2870...         1  \n",
       "..                                                 ...       ...  \n",
       "291  [tensor(0.7148), tensor(0.4332), tensor(0.6254...         0  \n",
       "292  [tensor(0.7867), tensor(0.6063), tensor(0.4453...         0  \n",
       "293  [tensor(0.5761), tensor(0.9245), tensor(0.2913...         1  \n",
       "294  [tensor(0.2567), tensor(0.6581), tensor(0.4311...         0  \n",
       "295  [tensor(0.3509), tensor(0.8979), tensor(0.2155...         1  \n",
       "\n",
       "[296 rows x 25 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "from scipy.stats import bernoulli, norm\n",
    "\n",
    "def calculate_uncertainty(p_win, num_simulations):\n",
    "    \"\"\"\n",
    "    Calculate uncertainty statistics for a Bernoulli outcome.\n",
    "    \n",
    "    Parameters:\n",
    "        p_win: Probability of the fighter winning.\n",
    "        num_simulations: Number of simulations run.\n",
    "    \n",
    "    Returns:\n",
    "        variance, std_dev, and Wilson score confidence intervals (lower, upper).\n",
    "    \"\"\"\n",
    "    variance = p_win * (1 - p_win)\n",
    "    std_dev = np.sqrt(variance)\n",
    "    \n",
    "    # 95% confidence interval using the Wilson score interval\n",
    "    z = norm.ppf(0.975)  # 1.96 for 95% CI\n",
    "    denominator = 1 + (z**2 / num_simulations)\n",
    "    center_adjusted_probability = p_win + (z**2 / (2 * num_simulations))\n",
    "    adjusted_std_dev = np.sqrt((p_win * (1 - p_win) + (z**2 / (4 * num_simulations))) / num_simulations)\n",
    "    \n",
    "    lower_bound = (center_adjusted_probability - z * adjusted_std_dev) / denominator\n",
    "    upper_bound = (center_adjusted_probability + z * adjusted_std_dev) / denominator\n",
    "    \n",
    "    return variance, std_dev, lower_bound, upper_bound\n",
    "\n",
    "def simulate_fights_by_bout_with_uncertainty(df, bout_col, trace_col, fighter_col, num_simulations=10000):\n",
    "    \"\"\"\n",
    "    Simulate future fights for each bout by comparing tensor traces of both fighters,\n",
    "    and use Bernoulli distribution to model the fight outcome, along with uncertainty statistics.\n",
    "    \n",
    "    Parameters:\n",
    "        df: DataFrame containing fight data.\n",
    "        bout_col: Column name for BOUT (to group by).\n",
    "        trace_col: Column name for fighter's trace (tensor).\n",
    "        fighter_col: Column name for the fighter's name.\n",
    "        num_simulations: Number of simulations to run.\n",
    "    \n",
    "    Returns:\n",
    "        A DataFrame containing the win probability, Bernoulli outcome, and uncertainty statistics.\n",
    "    \"\"\"\n",
    "    bout_groups = df.groupby(bout_col)\n",
    "    \n",
    "    probabilities = []\n",
    "\n",
    "    for bout, group in bout_groups:\n",
    "        if len(group) == 2:  # Ensure there are two fighters in the bout\n",
    "            fighter_A = group.iloc[0]  # First fighter\n",
    "            fighter_B = group.iloc[1]  # Second fighter\n",
    "            \n",
    "            # Access the trace as a tensor and convert to NumPy\n",
    "            trace_A = fighter_A[trace_col].numpy()\n",
    "            trace_B = fighter_B[trace_col].numpy()\n",
    "            \n",
    "            wins_A, wins_B = 0, 0\n",
    "            \n",
    "            # Monte Carlo simulation to compute win probabilities\n",
    "            for _ in range(num_simulations):\n",
    "                sample_A = np.random.choice(trace_A)  # Random sample from Fighter A's trace\n",
    "                sample_B = np.random.choice(trace_B)  # Random sample from Fighter B's trace\n",
    "                \n",
    "                if sample_A > sample_B:\n",
    "                    wins_A += 1\n",
    "                else:\n",
    "                    wins_B += 1\n",
    "            \n",
    "            prob_A_win = wins_A / num_simulations\n",
    "            prob_B_win = wins_B / num_simulations\n",
    "            \n",
    "            # Simulate fight outcome using Bernoulli distribution\n",
    "            fight_outcome_A = bernoulli.rvs(prob_A_win)  # 1 if Fighter A wins, 0 otherwise\n",
    "            fight_outcome_B = 1 - fight_outcome_A  # 1 if Fighter B wins, 0 otherwise\n",
    "            \n",
    "            # Calculate uncertainty statistics for Fighter A and Fighter B\n",
    "            variance_A, std_dev_A, lower_A, upper_A = calculate_uncertainty(prob_A_win, num_simulations)\n",
    "            variance_B, std_dev_B, lower_B, upper_B = calculate_uncertainty(prob_B_win, num_simulations)\n",
    "            \n",
    "            # Assign probability, Bernoulli outcome, and uncertainty stats for each fighter in the bout\n",
    "            for index, fighter in group.iterrows():\n",
    "                if fighter[fighter_col] == fighter_A[fighter_col]:\n",
    "                    probabilities.append({\n",
    "                        'index': index,\n",
    "                        'win_probability': prob_A_win,\n",
    "                        'bernoulli_outcome': fight_outcome_A,  # 1 if Fighter A wins\n",
    "                        'variance': variance_A,\n",
    "                        'std_dev': std_dev_A,\n",
    "                        'lower_CI': lower_A,\n",
    "                        'upper_CI': upper_A\n",
    "                    })\n",
    "                elif fighter[fighter_col] == fighter_B[fighter_col]:\n",
    "                    probabilities.append({\n",
    "                        'index': index,\n",
    "                        'win_probability': prob_B_win,\n",
    "                        'bernoulli_outcome': fight_outcome_B,  # 1 if Fighter B wins\n",
    "                        'variance': variance_B,\n",
    "                        'std_dev': std_dev_B,\n",
    "                        'lower_CI': lower_B,\n",
    "                        'upper_CI': upper_B\n",
    "                    })\n",
    "    \n",
    "    # Convert probabilities into a DataFrame\n",
    "    prob_df = pd.DataFrame(probabilities).set_index('index')\n",
    "    \n",
    "    return prob_df\n",
    "\n",
    "\n",
    "simulation_results = simulate_fights_by_bout_with_uncertainty(extra_val, 'BOUT', 'trace', 'FIGHTER')\n",
    "df_merged = extra_val.join(simulation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:718: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sk = 2*(b-a)*np.sqrt(a + b + 1) / (a + b + 2) / np.sqrt(a*b)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:718: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sk = 2*(b-a)*np.sqrt(a + b + 1) / (a + b + 2) / np.sqrt(a*b)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:718: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sk = 2*(b-a)*np.sqrt(a + b + 1) / (a + b + 2) / np.sqrt(a*b)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:718: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sk = 2*(b-a)*np.sqrt(a + b + 1) / (a + b + 2) / np.sqrt(a*b)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:723: RuntimeWarning: The iteration is not making good progress, as measured by the \n",
      "  improvement from the last ten iterations.\n",
      "  a, b = optimize.fsolve(func, (1.0, 1.0))\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:718: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sk = 2*(b-a)*np.sqrt(a + b + 1) / (a + b + 2) / np.sqrt(a*b)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:718: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sk = 2*(b-a)*np.sqrt(a + b + 1) / (a + b + 2) / np.sqrt(a*b)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:723: RuntimeWarning: The iteration is not making good progress, as measured by the \n",
      "  improvement from the last ten iterations.\n",
      "  a, b = optimize.fsolve(func, (1.0, 1.0))\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:718: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sk = 2*(b-a)*np.sqrt(a + b + 1) / (a + b + 2) / np.sqrt(a*b)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:723: RuntimeWarning: The iteration is not making good progress, as measured by the \n",
      "  improvement from the last ten iterations.\n",
      "  a, b = optimize.fsolve(func, (1.0, 1.0))\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:718: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sk = 2*(b-a)*np.sqrt(a + b + 1) / (a + b + 2) / np.sqrt(a*b)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:718: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sk = 2*(b-a)*np.sqrt(a + b + 1) / (a + b + 2) / np.sqrt(a*b)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:718: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sk = 2*(b-a)*np.sqrt(a + b + 1) / (a + b + 2) / np.sqrt(a*b)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:718: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sk = 2*(b-a)*np.sqrt(a + b + 1) / (a + b + 2) / np.sqrt(a*b)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6323: RuntimeWarning: divide by zero encountered in power\n",
      "  return cd2*x**(c-1)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:723: RuntimeWarning: The iteration is not making good progress, as measured by the \n",
      "  improvement from the last ten iterations.\n",
      "  a, b = optimize.fsolve(func, (1.0, 1.0))\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:718: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sk = 2*(b-a)*np.sqrt(a + b + 1) / (a + b + 2) / np.sqrt(a*b)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:718: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sk = 2*(b-a)*np.sqrt(a + b + 1) / (a + b + 2) / np.sqrt(a*b)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:718: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sk = 2*(b-a)*np.sqrt(a + b + 1) / (a + b + 2) / np.sqrt(a*b)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:723: RuntimeWarning: The iteration is not making good progress, as measured by the \n",
      "  improvement from the last ten iterations.\n",
      "  a, b = optimize.fsolve(func, (1.0, 1.0))\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:718: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sk = 2*(b-a)*np.sqrt(a + b + 1) / (a + b + 2) / np.sqrt(a*b)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:723: RuntimeWarning: The iteration is not making good progress, as measured by the \n",
      "  improvement from the last ten iterations.\n",
      "  a, b = optimize.fsolve(func, (1.0, 1.0))\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:718: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sk = 2*(b-a)*np.sqrt(a + b + 1) / (a + b + 2) / np.sqrt(a*b)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:723: RuntimeWarning: The iteration is not making good progress, as measured by the \n",
      "  improvement from the last five Jacobian evaluations.\n",
      "  a, b = optimize.fsolve(func, (1.0, 1.0))\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:723: RuntimeWarning: The iteration is not making good progress, as measured by the \n",
      "  improvement from the last ten iterations.\n",
      "  a, b = optimize.fsolve(func, (1.0, 1.0))\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:723: RuntimeWarning: The iteration is not making good progress, as measured by the \n",
      "  improvement from the last ten iterations.\n",
      "  a, b = optimize.fsolve(func, (1.0, 1.0))\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:718: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sk = 2*(b-a)*np.sqrt(a + b + 1) / (a + b + 2) / np.sqrt(a*b)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:723: RuntimeWarning: The iteration is not making good progress, as measured by the \n",
      "  improvement from the last ten iterations.\n",
      "  a, b = optimize.fsolve(func, (1.0, 1.0))\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:718: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sk = 2*(b-a)*np.sqrt(a + b + 1) / (a + b + 2) / np.sqrt(a*b)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:723: RuntimeWarning: The iteration is not making good progress, as measured by the \n",
      "  improvement from the last ten iterations.\n",
      "  a, b = optimize.fsolve(func, (1.0, 1.0))\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:718: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sk = 2*(b-a)*np.sqrt(a + b + 1) / (a + b + 2) / np.sqrt(a*b)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:723: RuntimeWarning: The iteration is not making good progress, as measured by the \n",
      "  improvement from the last ten iterations.\n",
      "  a, b = optimize.fsolve(func, (1.0, 1.0))\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:718: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sk = 2*(b-a)*np.sqrt(a + b + 1) / (a + b + 2) / np.sqrt(a*b)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:723: RuntimeWarning: The iteration is not making good progress, as measured by the \n",
      "  improvement from the last ten iterations.\n",
      "  a, b = optimize.fsolve(func, (1.0, 1.0))\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:718: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sk = 2*(b-a)*np.sqrt(a + b + 1) / (a + b + 2) / np.sqrt(a*b)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:718: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sk = 2*(b-a)*np.sqrt(a + b + 1) / (a + b + 2) / np.sqrt(a*b)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:718: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sk = 2*(b-a)*np.sqrt(a + b + 1) / (a + b + 2) / np.sqrt(a*b)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:718: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sk = 2*(b-a)*np.sqrt(a + b + 1) / (a + b + 2) / np.sqrt(a*b)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:718: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sk = 2*(b-a)*np.sqrt(a + b + 1) / (a + b + 2) / np.sqrt(a*b)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:723: RuntimeWarning: The iteration is not making good progress, as measured by the \n",
      "  improvement from the last ten iterations.\n",
      "  a, b = optimize.fsolve(func, (1.0, 1.0))\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:718: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sk = 2*(b-a)*np.sqrt(a + b + 1) / (a + b + 2) / np.sqrt(a*b)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:718: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sk = 2*(b-a)*np.sqrt(a + b + 1) / (a + b + 2) / np.sqrt(a*b)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:718: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sk = 2*(b-a)*np.sqrt(a + b + 1) / (a + b + 2) / np.sqrt(a*b)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:718: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sk = 2*(b-a)*np.sqrt(a + b + 1) / (a + b + 2) / np.sqrt(a*b)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:718: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sk = 2*(b-a)*np.sqrt(a + b + 1) / (a + b + 2) / np.sqrt(a*b)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:718: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sk = 2*(b-a)*np.sqrt(a + b + 1) / (a + b + 2) / np.sqrt(a*b)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:723: RuntimeWarning: The iteration is not making good progress, as measured by the \n",
      "  improvement from the last ten iterations.\n",
      "  a, b = optimize.fsolve(func, (1.0, 1.0))\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:718: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sk = 2*(b-a)*np.sqrt(a + b + 1) / (a + b + 2) / np.sqrt(a*b)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:723: RuntimeWarning: The iteration is not making good progress, as measured by the \n",
      "  improvement from the last ten iterations.\n",
      "  a, b = optimize.fsolve(func, (1.0, 1.0))\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:718: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sk = 2*(b-a)*np.sqrt(a + b + 1) / (a + b + 2) / np.sqrt(a*b)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:718: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sk = 2*(b-a)*np.sqrt(a + b + 1) / (a + b + 2) / np.sqrt(a*b)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:723: RuntimeWarning: The iteration is not making good progress, as measured by the \n",
      "  improvement from the last ten iterations.\n",
      "  a, b = optimize.fsolve(func, (1.0, 1.0))\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:718: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sk = 2*(b-a)*np.sqrt(a + b + 1) / (a + b + 2) / np.sqrt(a*b)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:723: RuntimeWarning: The iteration is not making good progress, as measured by the \n",
      "  improvement from the last ten iterations.\n",
      "  a, b = optimize.fsolve(func, (1.0, 1.0))\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:723: RuntimeWarning: The iteration is not making good progress, as measured by the \n",
      "  improvement from the last ten iterations.\n",
      "  a, b = optimize.fsolve(func, (1.0, 1.0))\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:718: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sk = 2*(b-a)*np.sqrt(a + b + 1) / (a + b + 2) / np.sqrt(a*b)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:718: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sk = 2*(b-a)*np.sqrt(a + b + 1) / (a + b + 2) / np.sqrt(a*b)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:723: RuntimeWarning: The iteration is not making good progress, as measured by the \n",
      "  improvement from the last ten iterations.\n",
      "  a, b = optimize.fsolve(func, (1.0, 1.0))\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:718: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sk = 2*(b-a)*np.sqrt(a + b + 1) / (a + b + 2) / np.sqrt(a*b)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:723: RuntimeWarning: The iteration is not making good progress, as measured by the \n",
      "  improvement from the last ten iterations.\n",
      "  a, b = optimize.fsolve(func, (1.0, 1.0))\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:723: RuntimeWarning: The iteration is not making good progress, as measured by the \n",
      "  improvement from the last ten iterations.\n",
      "  a, b = optimize.fsolve(func, (1.0, 1.0))\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:718: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sk = 2*(b-a)*np.sqrt(a + b + 1) / (a + b + 2) / np.sqrt(a*b)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:718: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sk = 2*(b-a)*np.sqrt(a + b + 1) / (a + b + 2) / np.sqrt(a*b)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:723: RuntimeWarning: The iteration is not making good progress, as measured by the \n",
      "  improvement from the last ten iterations.\n",
      "  a, b = optimize.fsolve(func, (1.0, 1.0))\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:718: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sk = 2*(b-a)*np.sqrt(a + b + 1) / (a + b + 2) / np.sqrt(a*b)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:718: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sk = 2*(b-a)*np.sqrt(a + b + 1) / (a + b + 2) / np.sqrt(a*b)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:718: RuntimeWarning: invalid value encountered in sqrt\n",
      "  sk = 2*(b-a)*np.sqrt(a + b + 1) / (a + b + 2) / np.sqrt(a*b)\n",
      "/Users/jamesjirsa/miniforge3/envs/mma/lib/python3.11/site-packages/scipy/stats/_continuous_distns.py:6338: RuntimeWarning: divide by zero encountered in divide\n",
      "  return c**2 / (c**2 - n**2)\n"
     ]
    }
   ],
   "source": [
    "from copulas.multivariate import GaussianMultivariate\n",
    "from copulas.univariate import GaussianKDE\n",
    "from copulas.multivariate import GaussianMultivariate\n",
    "from copulas.univariate import GaussianUnivariate\n",
    "\n",
    "def simulate_fights_with_copulas(df, bout_col, trace_col, fighter_col, num_simulations=10000):\n",
    "    \"\"\"\n",
    "    Simulate future fights using copulas by modeling dependency between fighter performances.\n",
    "    \n",
    "    Parameters:\n",
    "        df: DataFrame containing fight data.\n",
    "        bout_col: Column name for BOUT (to group by).\n",
    "        trace_col: Column name for fighter's trace (tensor).\n",
    "        fighter_col: Column name for the fighter's name.\n",
    "        num_simulations: Number of simulations to run.\n",
    "    \n",
    "    Returns:\n",
    "        A DataFrame containing the win probability for the fighter in the row.\n",
    "    \"\"\"\n",
    "    bout_groups = df.groupby(bout_col)\n",
    "    \n",
    "    probabilities = []\n",
    "    \n",
    "    for bout, group in bout_groups:\n",
    "        if len(group) == 2:  # Ensure there are two fighters in the bout\n",
    "            fighter_A = group.iloc[0]  # First fighter\n",
    "            fighter_B = group.iloc[1]  # Second fighter\n",
    "            \n",
    "            # Extract traces and convert to NumPy\n",
    "            trace_A = fighter_A[trace_col].numpy()\n",
    "            trace_B = fighter_B[trace_col].numpy()\n",
    "            \n",
    "            # Use simple Gaussian marginals instead of KDE\n",
    "            marginal_A = GaussianUnivariate()\n",
    "            marginal_B = GaussianUnivariate()\n",
    "            \n",
    "            try:\n",
    "                marginal_A.fit(trace_A)\n",
    "                marginal_B.fit(trace_B)\n",
    "            except Exception as e:\n",
    "                print(f\"Error fitting marginals for bout {bout}: {e}\")\n",
    "                continue  # Skip this bout if there's a fitting issue\n",
    "            \n",
    "            # Combine the traces into a joint dataset\n",
    "            joint_data = np.column_stack([trace_A, trace_B])\n",
    "            \n",
    "            # Fit a Gaussian Copula to the joint distribution\n",
    "            copula = GaussianMultivariate()\n",
    "            try:\n",
    "                copula.fit(joint_data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error fitting copula for bout {bout}: {e}\")\n",
    "                continue  # Skip this bout if there's a fitting issue\n",
    "            \n",
    "            # Simulate joint outcomes from the copula\n",
    "            simulated_outcomes = copula.sample(num_simulations)\n",
    "            simulated_outcomes = np.array(simulated_outcomes)  # Ensure it's a NumPy array\n",
    "            \n",
    "            if simulated_outcomes.shape[1] != 2:\n",
    "                print(f\"Unexpected shape for simulated outcomes: {simulated_outcomes.shape}\")\n",
    "                continue  # Skip if the simulation results aren't correct\n",
    "            \n",
    "            # Extract the simulated marginals for Fighter A and Fighter B\n",
    "            simulated_A = simulated_outcomes[:, 0]\n",
    "            simulated_B = simulated_outcomes[:, 1]\n",
    "            \n",
    "            # Calculate probabilities of Fighter A winning over Fighter B\n",
    "            wins_A = np.sum(simulated_A > simulated_B)\n",
    "            prob_A_win = wins_A / num_simulations\n",
    "            prob_B_win = 1 - prob_A_win\n",
    "            \n",
    "            # Assign probability for the fighter in the current row\n",
    "            for index, fighter in group.iterrows():\n",
    "                if fighter[fighter_col] == fighter_A[fighter_col]:\n",
    "                    probabilities.append({\n",
    "                        'index': index,\n",
    "                        'copula_win_probability': prob_A_win\n",
    "                    })\n",
    "                elif fighter[fighter_col] == fighter_B[fighter_col]:\n",
    "                    probabilities.append({\n",
    "                        'index': index,\n",
    "                        'copula_win_probability': prob_B_win\n",
    "                    })\n",
    "    \n",
    "    # Convert probabilities into a DataFrame\n",
    "    prob_df = pd.DataFrame(probabilities).set_index('index')\n",
    "    \n",
    "    return prob_df\n",
    "\n",
    "probabilities_df = simulate_fights_with_copulas(df_merged, 'BOUT', 'trace', 'FIGHTER')\n",
    "\n",
    "# Merge the probabilities back to the original DataFrame based on the index\n",
    "df_merged = df_merged.join(probabilities_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EVENT</th>\n",
       "      <th>BOUT</th>\n",
       "      <th>OUTCOME</th>\n",
       "      <th>WEIGHTCLASS</th>\n",
       "      <th>METHOD</th>\n",
       "      <th>ROUND</th>\n",
       "      <th>TIME</th>\n",
       "      <th>TIME FORMAT</th>\n",
       "      <th>REFEREE</th>\n",
       "      <th>DETAILS</th>\n",
       "      <th>...</th>\n",
       "      <th>kurtosis</th>\n",
       "      <th>trace</th>\n",
       "      <th>Estimate</th>\n",
       "      <th>win_probability</th>\n",
       "      <th>bernoulli_outcome</th>\n",
       "      <th>variance</th>\n",
       "      <th>std_dev</th>\n",
       "      <th>lower_CI</th>\n",
       "      <th>upper_CI</th>\n",
       "      <th>copula_win_probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UFCFightNight:Ankalaevvs.Walker2</td>\n",
       "      <td>MagomedAnkalaevvs.JohnnyWalker</td>\n",
       "      <td>W/L</td>\n",
       "      <td>Light Heavyweight Bout</td>\n",
       "      <td>KO/TKO</td>\n",
       "      <td>2</td>\n",
       "      <td>2:42</td>\n",
       "      <td>5 Rnd (5-5-5-5-5)</td>\n",
       "      <td>Marc Goddard</td>\n",
       "      <td>Punch to Head At Distance</td>\n",
       "      <td>...</td>\n",
       "      <td>2.210196</td>\n",
       "      <td>[tensor(0.5757), tensor(0.2977), tensor(0.4158...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4867</td>\n",
       "      <td>0</td>\n",
       "      <td>0.249823</td>\n",
       "      <td>0.499823</td>\n",
       "      <td>0.476911</td>\n",
       "      <td>0.496500</td>\n",
       "      <td>0.4823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UFCFightNight:Ankalaevvs.Walker2</td>\n",
       "      <td>JimMillervs.GabrielBenitez</td>\n",
       "      <td>W/L</td>\n",
       "      <td>Lightweight Bout</td>\n",
       "      <td>Submission</td>\n",
       "      <td>3</td>\n",
       "      <td>3:25</td>\n",
       "      <td>3 Rnd (5-5-5)</td>\n",
       "      <td>Dan Miragliotta</td>\n",
       "      <td>Neck Crank From Back Control</td>\n",
       "      <td>...</td>\n",
       "      <td>2.165578</td>\n",
       "      <td>[tensor(0.5206), tensor(0.3033), tensor(0.5628...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4128</td>\n",
       "      <td>1</td>\n",
       "      <td>0.242396</td>\n",
       "      <td>0.492337</td>\n",
       "      <td>0.403186</td>\n",
       "      <td>0.422481</td>\n",
       "      <td>0.4040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UFCFightNight:Ankalaevvs.Walker2</td>\n",
       "      <td>JimMillervs.GabrielBenitez</td>\n",
       "      <td>W/L</td>\n",
       "      <td>Lightweight Bout</td>\n",
       "      <td>Submission</td>\n",
       "      <td>3</td>\n",
       "      <td>3:25</td>\n",
       "      <td>3 Rnd (5-5-5)</td>\n",
       "      <td>Dan Miragliotta</td>\n",
       "      <td>Neck Crank From Back Control</td>\n",
       "      <td>...</td>\n",
       "      <td>2.773551</td>\n",
       "      <td>[tensor(0.9892), tensor(0.8510), tensor(0.2175...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5872</td>\n",
       "      <td>0</td>\n",
       "      <td>0.242396</td>\n",
       "      <td>0.492337</td>\n",
       "      <td>0.577519</td>\n",
       "      <td>0.596814</td>\n",
       "      <td>0.5960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UFCFightNight:Ankalaevvs.Walker2</td>\n",
       "      <td>MagomedAnkalaevvs.JohnnyWalker</td>\n",
       "      <td>W/L</td>\n",
       "      <td>Light Heavyweight Bout</td>\n",
       "      <td>KO/TKO</td>\n",
       "      <td>2</td>\n",
       "      <td>2:42</td>\n",
       "      <td>5 Rnd (5-5-5-5-5)</td>\n",
       "      <td>Marc Goddard</td>\n",
       "      <td>Punch to Head At Distance</td>\n",
       "      <td>...</td>\n",
       "      <td>2.651780</td>\n",
       "      <td>[tensor(0.8763), tensor(0.5415), tensor(0.1044...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5133</td>\n",
       "      <td>1</td>\n",
       "      <td>0.249823</td>\n",
       "      <td>0.499823</td>\n",
       "      <td>0.503500</td>\n",
       "      <td>0.523089</td>\n",
       "      <td>0.5177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UFCFightNight:Ankalaevvs.Walker2</td>\n",
       "      <td>MatthewSemelsbergervs.PrestonParsons</td>\n",
       "      <td>L/W</td>\n",
       "      <td>Welterweight Bout</td>\n",
       "      <td>Decision - Unanimous</td>\n",
       "      <td>3</td>\n",
       "      <td>5:00</td>\n",
       "      <td>3 Rnd (5-5-5)</td>\n",
       "      <td>Dan Miragliotta</td>\n",
       "      <td>Mike Bell 27 - 30.Junichiro Kamijo 27 - 30.Ron...</td>\n",
       "      <td>...</td>\n",
       "      <td>3.980455</td>\n",
       "      <td>[tensor(0.8357), tensor(0.6041), tensor(0.2870...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5127</td>\n",
       "      <td>0</td>\n",
       "      <td>0.249839</td>\n",
       "      <td>0.499839</td>\n",
       "      <td>0.502900</td>\n",
       "      <td>0.522490</td>\n",
       "      <td>0.5164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>UFCFightNight:Sandhagenvs.Nurmagomedov</td>\n",
       "      <td>MackenzieDernvs.LoopyGodinez</td>\n",
       "      <td>W/L</td>\n",
       "      <td>Women's Strawweight Bout</td>\n",
       "      <td>Decision - Unanimous</td>\n",
       "      <td>3</td>\n",
       "      <td>5:00</td>\n",
       "      <td>3 Rnd (5-5-5)</td>\n",
       "      <td>Greg Kleynjans</td>\n",
       "      <td>Hadi Mohamed Ali 28 - 29.Howard Hughes 28 - 29...</td>\n",
       "      <td>...</td>\n",
       "      <td>2.603126</td>\n",
       "      <td>[tensor(0.7148), tensor(0.4332), tensor(0.6254...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6089</td>\n",
       "      <td>1</td>\n",
       "      <td>0.238141</td>\n",
       "      <td>0.487997</td>\n",
       "      <td>0.599295</td>\n",
       "      <td>0.618421</td>\n",
       "      <td>0.6012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>UFCFightNight:Sandhagenvs.Nurmagomedov</td>\n",
       "      <td>TonyFergusonvs.MichaelChiesa</td>\n",
       "      <td>L/W</td>\n",
       "      <td>Welterweight Bout</td>\n",
       "      <td>Submission</td>\n",
       "      <td>1</td>\n",
       "      <td>3:44</td>\n",
       "      <td>3 Rnd (5-5-5)</td>\n",
       "      <td>Marc Goddard</td>\n",
       "      <td>Rear Naked Choke</td>\n",
       "      <td>...</td>\n",
       "      <td>2.947959</td>\n",
       "      <td>[tensor(0.7867), tensor(0.6063), tensor(0.4453...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4513</td>\n",
       "      <td>0</td>\n",
       "      <td>0.247628</td>\n",
       "      <td>0.497623</td>\n",
       "      <td>0.441567</td>\n",
       "      <td>0.461070</td>\n",
       "      <td>0.4470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>UFCFightNight:Sandhagenvs.Nurmagomedov</td>\n",
       "      <td>TonyFergusonvs.MichaelChiesa</td>\n",
       "      <td>L/W</td>\n",
       "      <td>Welterweight Bout</td>\n",
       "      <td>Submission</td>\n",
       "      <td>1</td>\n",
       "      <td>3:44</td>\n",
       "      <td>3 Rnd (5-5-5)</td>\n",
       "      <td>Marc Goddard</td>\n",
       "      <td>Rear Naked Choke</td>\n",
       "      <td>...</td>\n",
       "      <td>2.037477</td>\n",
       "      <td>[tensor(0.5761), tensor(0.9245), tensor(0.2913...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5487</td>\n",
       "      <td>1</td>\n",
       "      <td>0.247628</td>\n",
       "      <td>0.497623</td>\n",
       "      <td>0.538930</td>\n",
       "      <td>0.558433</td>\n",
       "      <td>0.5530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>UFCFightNight:Sandhagenvs.Nurmagomedov</td>\n",
       "      <td>MarlonVeravs.DeivesonFigueiredo</td>\n",
       "      <td>L/W</td>\n",
       "      <td>Bantamweight Bout</td>\n",
       "      <td>Decision - Unanimous</td>\n",
       "      <td>3</td>\n",
       "      <td>5:00</td>\n",
       "      <td>3 Rnd (5-5-5)</td>\n",
       "      <td>Keith Peterson</td>\n",
       "      <td>Ben Cartlidge 28 - 29.Hadi Mohamed Ali 28 - 29...</td>\n",
       "      <td>...</td>\n",
       "      <td>2.238836</td>\n",
       "      <td>[tensor(0.2567), tensor(0.6581), tensor(0.4311...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3623</td>\n",
       "      <td>1</td>\n",
       "      <td>0.231039</td>\n",
       "      <td>0.480665</td>\n",
       "      <td>0.352934</td>\n",
       "      <td>0.371772</td>\n",
       "      <td>0.3764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>UFCFightNight:Sandhagenvs.Nurmagomedov</td>\n",
       "      <td>MackenzieDernvs.LoopyGodinez</td>\n",
       "      <td>W/L</td>\n",
       "      <td>Women's Strawweight Bout</td>\n",
       "      <td>Decision - Unanimous</td>\n",
       "      <td>3</td>\n",
       "      <td>5:00</td>\n",
       "      <td>3 Rnd (5-5-5)</td>\n",
       "      <td>Greg Kleynjans</td>\n",
       "      <td>Hadi Mohamed Ali 28 - 29.Howard Hughes 28 - 29...</td>\n",
       "      <td>...</td>\n",
       "      <td>2.661362</td>\n",
       "      <td>[tensor(0.3509), tensor(0.8979), tensor(0.2155...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3911</td>\n",
       "      <td>0</td>\n",
       "      <td>0.238141</td>\n",
       "      <td>0.487997</td>\n",
       "      <td>0.381579</td>\n",
       "      <td>0.400705</td>\n",
       "      <td>0.3988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>296 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      EVENT  \\\n",
       "0          UFCFightNight:Ankalaevvs.Walker2   \n",
       "1          UFCFightNight:Ankalaevvs.Walker2   \n",
       "2          UFCFightNight:Ankalaevvs.Walker2   \n",
       "3          UFCFightNight:Ankalaevvs.Walker2   \n",
       "4          UFCFightNight:Ankalaevvs.Walker2   \n",
       "..                                      ...   \n",
       "291  UFCFightNight:Sandhagenvs.Nurmagomedov   \n",
       "292  UFCFightNight:Sandhagenvs.Nurmagomedov   \n",
       "293  UFCFightNight:Sandhagenvs.Nurmagomedov   \n",
       "294  UFCFightNight:Sandhagenvs.Nurmagomedov   \n",
       "295  UFCFightNight:Sandhagenvs.Nurmagomedov   \n",
       "\n",
       "                                     BOUT OUTCOME               WEIGHTCLASS  \\\n",
       "0          MagomedAnkalaevvs.JohnnyWalker     W/L    Light Heavyweight Bout   \n",
       "1              JimMillervs.GabrielBenitez     W/L          Lightweight Bout   \n",
       "2              JimMillervs.GabrielBenitez     W/L          Lightweight Bout   \n",
       "3          MagomedAnkalaevvs.JohnnyWalker     W/L    Light Heavyweight Bout   \n",
       "4    MatthewSemelsbergervs.PrestonParsons     L/W         Welterweight Bout   \n",
       "..                                    ...     ...                       ...   \n",
       "291          MackenzieDernvs.LoopyGodinez     W/L  Women's Strawweight Bout   \n",
       "292          TonyFergusonvs.MichaelChiesa     L/W         Welterweight Bout   \n",
       "293          TonyFergusonvs.MichaelChiesa     L/W         Welterweight Bout   \n",
       "294       MarlonVeravs.DeivesonFigueiredo     L/W         Bantamweight Bout   \n",
       "295          MackenzieDernvs.LoopyGodinez     W/L  Women's Strawweight Bout   \n",
       "\n",
       "                    METHOD  ROUND  TIME        TIME FORMAT          REFEREE  \\\n",
       "0                  KO/TKO       2  2:42  5 Rnd (5-5-5-5-5)     Marc Goddard   \n",
       "1              Submission       3  3:25      3 Rnd (5-5-5)  Dan Miragliotta   \n",
       "2              Submission       3  3:25      3 Rnd (5-5-5)  Dan Miragliotta   \n",
       "3                  KO/TKO       2  2:42  5 Rnd (5-5-5-5-5)     Marc Goddard   \n",
       "4    Decision - Unanimous       3  5:00      3 Rnd (5-5-5)  Dan Miragliotta   \n",
       "..                     ...    ...   ...                ...              ...   \n",
       "291  Decision - Unanimous       3  5:00      3 Rnd (5-5-5)   Greg Kleynjans   \n",
       "292            Submission       1  3:44      3 Rnd (5-5-5)     Marc Goddard   \n",
       "293            Submission       1  3:44      3 Rnd (5-5-5)     Marc Goddard   \n",
       "294  Decision - Unanimous       3  5:00      3 Rnd (5-5-5)   Keith Peterson   \n",
       "295  Decision - Unanimous       3  5:00      3 Rnd (5-5-5)   Greg Kleynjans   \n",
       "\n",
       "                                               DETAILS  ...  kurtosis  \\\n",
       "0                           Punch to Head At Distance   ...  2.210196   \n",
       "1                        Neck Crank From Back Control   ...  2.165578   \n",
       "2                        Neck Crank From Back Control   ...  2.773551   \n",
       "3                           Punch to Head At Distance   ...  2.651780   \n",
       "4    Mike Bell 27 - 30.Junichiro Kamijo 27 - 30.Ron...  ...  3.980455   \n",
       "..                                                 ...  ...       ...   \n",
       "291  Hadi Mohamed Ali 28 - 29.Howard Hughes 28 - 29...  ...  2.603126   \n",
       "292                                  Rear Naked Choke   ...  2.947959   \n",
       "293                                  Rear Naked Choke   ...  2.037477   \n",
       "294  Ben Cartlidge 28 - 29.Hadi Mohamed Ali 28 - 29...  ...  2.238836   \n",
       "295  Hadi Mohamed Ali 28 - 29.Howard Hughes 28 - 29...  ...  2.661362   \n",
       "\n",
       "                                                 trace  Estimate  \\\n",
       "0    [tensor(0.5757), tensor(0.2977), tensor(0.4158...         1   \n",
       "1    [tensor(0.5206), tensor(0.3033), tensor(0.5628...         0   \n",
       "2    [tensor(0.9892), tensor(0.8510), tensor(0.2175...         0   \n",
       "3    [tensor(0.8763), tensor(0.5415), tensor(0.1044...         1   \n",
       "4    [tensor(0.8357), tensor(0.6041), tensor(0.2870...         1   \n",
       "..                                                 ...       ...   \n",
       "291  [tensor(0.7148), tensor(0.4332), tensor(0.6254...         0   \n",
       "292  [tensor(0.7867), tensor(0.6063), tensor(0.4453...         0   \n",
       "293  [tensor(0.5761), tensor(0.9245), tensor(0.2913...         1   \n",
       "294  [tensor(0.2567), tensor(0.6581), tensor(0.4311...         0   \n",
       "295  [tensor(0.3509), tensor(0.8979), tensor(0.2155...         1   \n",
       "\n",
       "     win_probability bernoulli_outcome  variance   std_dev  lower_CI  \\\n",
       "0             0.4867                 0  0.249823  0.499823  0.476911   \n",
       "1             0.4128                 1  0.242396  0.492337  0.403186   \n",
       "2             0.5872                 0  0.242396  0.492337  0.577519   \n",
       "3             0.5133                 1  0.249823  0.499823  0.503500   \n",
       "4             0.5127                 0  0.249839  0.499839  0.502900   \n",
       "..               ...               ...       ...       ...       ...   \n",
       "291           0.6089                 1  0.238141  0.487997  0.599295   \n",
       "292           0.4513                 0  0.247628  0.497623  0.441567   \n",
       "293           0.5487                 1  0.247628  0.497623  0.538930   \n",
       "294           0.3623                 1  0.231039  0.480665  0.352934   \n",
       "295           0.3911                 0  0.238141  0.487997  0.381579   \n",
       "\n",
       "     upper_CI  copula_win_probability  \n",
       "0    0.496500                  0.4823  \n",
       "1    0.422481                  0.4040  \n",
       "2    0.596814                  0.5960  \n",
       "3    0.523089                  0.5177  \n",
       "4    0.522490                  0.5164  \n",
       "..        ...                     ...  \n",
       "291  0.618421                  0.6012  \n",
       "292  0.461070                  0.4470  \n",
       "293  0.558433                  0.5530  \n",
       "294  0.371772                  0.3764  \n",
       "295  0.400705                  0.3988  \n",
       "\n",
       "[296 rows x 32 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.to_csv('MergedWJointProba_AE.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      EVENT  \\\n",
      "0          UFCFightNight:Ankalaevvs.Walker2   \n",
      "1          UFCFightNight:Ankalaevvs.Walker2   \n",
      "2          UFCFightNight:Ankalaevvs.Walker2   \n",
      "3          UFCFightNight:Ankalaevvs.Walker2   \n",
      "4          UFCFightNight:Ankalaevvs.Walker2   \n",
      "..                                      ...   \n",
      "291  UFCFightNight:Sandhagenvs.Nurmagomedov   \n",
      "292  UFCFightNight:Sandhagenvs.Nurmagomedov   \n",
      "293  UFCFightNight:Sandhagenvs.Nurmagomedov   \n",
      "294  UFCFightNight:Sandhagenvs.Nurmagomedov   \n",
      "295  UFCFightNight:Sandhagenvs.Nurmagomedov   \n",
      "\n",
      "                                     BOUT OUTCOME               WEIGHTCLASS  \\\n",
      "0          MagomedAnkalaevvs.JohnnyWalker     W/L    Light Heavyweight Bout   \n",
      "1              JimMillervs.GabrielBenitez     W/L          Lightweight Bout   \n",
      "2              JimMillervs.GabrielBenitez     W/L          Lightweight Bout   \n",
      "3          MagomedAnkalaevvs.JohnnyWalker     W/L    Light Heavyweight Bout   \n",
      "4    MatthewSemelsbergervs.PrestonParsons     L/W         Welterweight Bout   \n",
      "..                                    ...     ...                       ...   \n",
      "291          MackenzieDernvs.LoopyGodinez     W/L  Women's Strawweight Bout   \n",
      "292          TonyFergusonvs.MichaelChiesa     L/W         Welterweight Bout   \n",
      "293          TonyFergusonvs.MichaelChiesa     L/W         Welterweight Bout   \n",
      "294       MarlonVeravs.DeivesonFigueiredo     L/W         Bantamweight Bout   \n",
      "295          MackenzieDernvs.LoopyGodinez     W/L  Women's Strawweight Bout   \n",
      "\n",
      "                    METHOD  ROUND  TIME        TIME FORMAT          REFEREE  \\\n",
      "0                  KO/TKO       2  2:42  5 Rnd (5-5-5-5-5)     Marc Goddard   \n",
      "1              Submission       3  3:25      3 Rnd (5-5-5)  Dan Miragliotta   \n",
      "2              Submission       3  3:25      3 Rnd (5-5-5)  Dan Miragliotta   \n",
      "3                  KO/TKO       2  2:42  5 Rnd (5-5-5-5-5)     Marc Goddard   \n",
      "4    Decision - Unanimous       3  5:00      3 Rnd (5-5-5)  Dan Miragliotta   \n",
      "..                     ...    ...   ...                ...              ...   \n",
      "291  Decision - Unanimous       3  5:00      3 Rnd (5-5-5)   Greg Kleynjans   \n",
      "292            Submission       1  3:44      3 Rnd (5-5-5)     Marc Goddard   \n",
      "293            Submission       1  3:44      3 Rnd (5-5-5)     Marc Goddard   \n",
      "294  Decision - Unanimous       3  5:00      3 Rnd (5-5-5)   Keith Peterson   \n",
      "295  Decision - Unanimous       3  5:00      3 Rnd (5-5-5)   Greg Kleynjans   \n",
      "\n",
      "                                               DETAILS  ...  ELO_FIGHTER  \\\n",
      "0                           Punch to Head At Distance   ...  1726.390024   \n",
      "1                        Neck Crank From Back Control   ...  1509.527752   \n",
      "2                        Neck Crank From Back Control   ...  1606.672231   \n",
      "3                           Punch to Head At Distance   ...  1622.106542   \n",
      "4    Mike Bell 27 - 30.Junichiro Kamijo 27 - 30.Ron...  ...  1482.314337   \n",
      "..                                                 ...  ...          ...   \n",
      "291  Hadi Mohamed Ali 28 - 29.Howard Hughes 28 - 29...  ...  1555.735723   \n",
      "292                                  Rear Naked Choke   ...  1578.089046   \n",
      "293                                  Rear Naked Choke   ...  1622.714126   \n",
      "294  Ben Cartlidge 28 - 29.Hadi Mohamed Ali 28 - 29...  ...  1684.684440   \n",
      "295  Hadi Mohamed Ali 28 - 29.Howard Hughes 28 - 29...  ...  1582.243272   \n",
      "\n",
      "        proba    median     lower     upper       std      skew  kurtosis  \\\n",
      "0    0.669927  0.759190  0.064609  0.993731  0.288368 -0.690849  2.210196   \n",
      "1    0.487733  0.483612  0.130990  0.867594  0.202510  0.071405  2.165578   \n",
      "2    0.313769  0.274243  0.038304  0.770339  0.201599  0.708433  2.773551   \n",
      "3    0.575931  0.582008  0.323868  0.799449  0.124302 -0.196757  2.651780   \n",
      "4    0.811297  0.986142  0.014689  0.999997  0.311524 -1.578840  3.980455   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "291  0.372387  0.355724  0.121861  0.701482  0.153146  0.409076  2.603126   \n",
      "292  0.330777  0.315402  0.125038  0.619703  0.128477  0.550786  2.947959   \n",
      "293  0.647541  0.739231  0.039999  0.995035  0.307114 -0.618858  2.037477   \n",
      "294  0.476529  0.467581  0.115009  0.867519  0.203825  0.113106  2.238836   \n",
      "295  0.526487  0.526828  0.331778  0.717604  0.099837 -0.060892  2.661362   \n",
      "\n",
      "                                                 trace  Estimate  \n",
      "0    [tensor(0.5757), tensor(0.2977), tensor(0.4158...         1  \n",
      "1    [tensor(0.5206), tensor(0.3033), tensor(0.5628...         0  \n",
      "2    [tensor(0.9892), tensor(0.8510), tensor(0.2175...         0  \n",
      "3    [tensor(0.8763), tensor(0.5415), tensor(0.1044...         1  \n",
      "4    [tensor(0.8357), tensor(0.6041), tensor(0.2870...         1  \n",
      "..                                                 ...       ...  \n",
      "291  [tensor(0.7148), tensor(0.4332), tensor(0.6254...         0  \n",
      "292  [tensor(0.7867), tensor(0.6063), tensor(0.4453...         0  \n",
      "293  [tensor(0.5761), tensor(0.9245), tensor(0.2913...         1  \n",
      "294  [tensor(0.2567), tensor(0.6581), tensor(0.4311...         0  \n",
      "295  [tensor(0.3509), tensor(0.8979), tensor(0.2155...         1  \n",
      "\n",
      "[296 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "print(extra_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6699274 , 0.48773324, 0.31376907, 0.57593113, 0.8112971 ,\n",
       "       0.5373616 , 0.57997674, 0.43945357, 0.43517256, 0.55845594,\n",
       "       0.6136654 , 0.49048722, 0.42557114, 0.4546615 , 0.33998466,\n",
       "       0.49982616, 0.7368689 , 0.68928957, 0.41386324, 0.47904328,\n",
       "       0.6117593 , 0.5453888 , 0.35135522, 0.41235963, 0.53032887,\n",
       "       0.5524815 , 0.85125536, 0.505069  , 0.38537726, 0.6046647 ,\n",
       "       0.55143183, 0.38505596, 0.58296853, 0.5525833 , 0.44703373,\n",
       "       0.32672492, 0.4176081 , 0.6231255 , 0.6089352 , 0.41760707,\n",
       "       0.38029486, 0.64103466, 0.43141142, 0.5233549 , 0.49390668,\n",
       "       0.8023379 , 0.5775018 , 0.6294845 , 0.6002789 , 0.4033495 ,\n",
       "       0.64799815, 0.45472682, 0.85415334, 0.5074542 , 0.39174864,\n",
       "       0.6690643 , 0.8073959 , 0.7351551 , 0.49931964, 0.3826582 ,\n",
       "       0.44524702, 0.48661685, 0.5490535 , 0.36660203, 0.32905108,\n",
       "       0.5336979 , 0.49845272, 0.57608974, 0.44237882, 0.78635603,\n",
       "       0.39617494, 0.31596476, 0.4323557 , 0.7384062 , 0.70688933,\n",
       "       0.75102884, 0.31326625, 0.39158887, 0.6370276 , 0.49353877,\n",
       "       0.8004222 , 0.8297534 , 0.68865925, 0.52590567, 0.3912221 ,\n",
       "       0.56260955, 0.5031461 , 0.35830092, 0.44863608, 0.38901117,\n",
       "       0.43038994, 0.53603435, 0.6019329 , 0.4466412 , 0.43368682,\n",
       "       0.38171217, 0.51986647, 0.65539134, 0.49945933, 0.60780305,\n",
       "       0.5389689 , 0.4554208 , 0.6567361 , 0.25378728, 0.47537145,\n",
       "       0.523534  , 0.5824332 , 0.5556293 , 0.58884996, 0.49878204,\n",
       "       0.48891923, 0.4100145 , 0.4643956 , 0.44849893, 0.53214884,\n",
       "       0.61754924, 0.37134817, 0.7547952 , 0.78143096, 0.54967743,\n",
       "       0.8360074 , 0.50673634, 0.62116134, 0.43054542, 0.41157237,\n",
       "       0.48662743, 0.43769962, 0.44855702, 0.59341925, 0.6143843 ,\n",
       "       0.4056626 , 0.48591495, 0.5057226 , 0.43518558, 0.42880517,\n",
       "       0.44710624, 0.69908965, 0.5297224 , 0.45364845, 0.43704838,\n",
       "       0.5048171 , 0.5665601 , 0.54496795, 0.73698294, 0.4863305 ,\n",
       "       0.62693167, 0.4494457 , 0.35075778, 0.49497896, 0.48714146,\n",
       "       0.6279166 , 0.51102376, 0.48764628, 0.7094338 , 0.46716133,\n",
       "       0.6326256 , 0.41769502, 0.58762026, 0.31625506, 0.48698178,\n",
       "       0.7846424 , 0.36958537, 0.40942302, 0.44562557, 0.54221433,\n",
       "       0.5439405 , 0.44638085, 0.41863883, 0.8322731 , 0.48856133,\n",
       "       0.38359132, 0.38740048, 0.4832999 , 0.39351973, 0.6815582 ,\n",
       "       0.5242923 , 0.67658913, 0.6700106 , 0.59564376, 0.39984024,\n",
       "       0.6852038 , 0.34525633, 0.5388377 , 0.72710603, 0.5947761 ,\n",
       "       0.64223623, 0.39945343, 0.28699103, 0.79195184, 0.63277996,\n",
       "       0.55875117, 0.44187665, 0.7367082 , 0.6244107 , 0.39640498,\n",
       "       0.6091056 , 0.33703828, 0.2977306 , 0.5334471 , 0.71607244,\n",
       "       0.75825363, 0.6128161 , 0.5164564 , 0.8141186 , 0.41790387,\n",
       "       0.33586708, 0.6354382 , 0.75231826, 0.4622428 , 0.8112839 ,\n",
       "       0.40640342, 0.7428613 , 0.44802782, 0.50922227, 0.50039667,\n",
       "       0.48032963, 0.5260665 , 0.45623475, 0.43342182, 0.5547532 ,\n",
       "       0.4157682 , 0.4437565 , 0.531716  , 0.51041126, 0.60954523,\n",
       "       0.53965586, 0.4828538 , 0.47882876, 0.46977627, 0.4363358 ,\n",
       "       0.5395107 , 0.4780714 , 0.51162076, 0.2980585 , 0.49964815,\n",
       "       0.6998319 , 0.54569614, 0.40113625, 0.5078312 , 0.57248396,\n",
       "       0.3123245 , 0.49869376, 0.40626723, 0.75564724, 0.46128365,\n",
       "       0.54916704, 0.55398524, 0.42139536, 0.3756304 , 0.3248024 ,\n",
       "       0.40832576, 0.5696651 , 0.49738687, 0.3410498 , 0.53906053,\n",
       "       0.51201177, 0.32520962, 0.44924682, 0.5014437 , 0.73706824,\n",
       "       0.51841277, 0.46256608, 0.49074692, 0.48594332, 0.5266368 ,\n",
       "       0.6600994 , 0.43041074, 0.47427177, 0.7765331 , 0.4048268 ,\n",
       "       0.43005216, 0.40348527, 0.51407903, 0.7683931 , 0.57248163,\n",
       "       0.7222276 , 0.31829292, 0.6383671 , 0.47818232, 0.56190675,\n",
       "       0.6813483 , 0.6670871 , 0.5542388 , 0.47349042, 0.56181455,\n",
       "       0.41972578, 0.5501745 , 0.8069691 , 0.5230058 , 0.4909205 ,\n",
       "       0.4342418 , 0.37238687, 0.3307769 , 0.6475408 , 0.47652945,\n",
       "       0.5264874 ], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_probabilities.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.53      0.54       148\n",
      "           1       0.55      0.56      0.55       148\n",
      "\n",
      "    accuracy                           0.55       296\n",
      "   macro avg       0.55      0.55      0.55       296\n",
      "weighted avg       0.55      0.55      0.55       296\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_tensor_val.int(), predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4510, 4510])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_counts = torch.bincount(torch.tensor(y_train.values, dtype=torch.long))\n",
    "class_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.667163\n",
      "         Iterations 4\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 RESULT   No. Observations:                 9020\n",
      "Model:                          Logit   Df Residuals:                     9016\n",
      "Method:                           MLE   Df Model:                            3\n",
      "Date:                Thu, 05 Sep 2024   Pseudo R-squ.:                 0.03749\n",
      "Time:                        23:05:31   Log-Likelihood:                -6017.8\n",
      "converged:                       True   LL-Null:                       -6252.2\n",
      "Covariance Type:            nonrobust   LLR p-value:                2.812e-101\n",
      "=====================================================================================\n",
      "                        coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "const             -1.147e-17      0.022  -5.31e-16      1.000      -0.042       0.042\n",
      "delta_ELO_FIGHTER     0.0047      0.000     13.397      0.000       0.004       0.005\n",
      "delta_AGE            -0.0682      0.004    -15.718      0.000      -0.077      -0.060\n",
      "delta_REACH           0.0246      0.007      3.672      0.000       0.011       0.038\n",
      "=====================================================================================\n",
      "Accuracy: 0.5735\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "feats = ['delta_ELO_FIGHTER', 'delta_AGE', 'delta_REACH']\n",
    "\n",
    "X = X_train[feats]\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "y = y_train\n",
    "\n",
    "logit_model = sm.Logit(y, X)\n",
    "result = logit_model.fit()\n",
    "\n",
    "print(result.summary())\n",
    "\n",
    "X_test['predicted_prob'] = result.predict(sm.add_constant(X_test[feats]))\n",
    "X_test['predicted_class'] = (X_test['predicted_prob'] > 0.5).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_test, X_test['predicted_class'])\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.57      0.57       272\n",
      "           1       0.57      0.57      0.57       272\n",
      "\n",
      "    accuracy                           0.57       544\n",
      "   macro avg       0.57      0.57      0.57       544\n",
      "weighted avg       0.57      0.57      0.57       544\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(X_test['predicted_class'], y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
